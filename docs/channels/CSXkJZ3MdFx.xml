<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Dev News</title><link>https://konrad.website/liveboat-github-runner/</link><description></description><item><title>I was bored, so I created a Reddit CLI client (read-only). You cannot upvote or comment, but it’s better than nothing—for sure, it’s my go-to choice for a quick peek at my favorite subreddit to check what’s new or news about tariffs, haha.</title><link>https://www.reddit.com/r/linux/comments/1k4yjf3/i_was_bored_so_i_created_a_reddit_cli_client/</link><author>/u/internal-pagal</author><category>dev</category><category>reddit</category><pubDate>Tue, 22 Apr 2025 05:15:15 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[   submitted by    /u/internal-pagal ]]></content:encoded></item><item><title>Multizone cluster cost optimization</title><link>https://www.reddit.com/r/kubernetes/comments/1k4y3v4/multizone_cluster_cost_optimization/</link><author>/u/elephantum</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Tue, 22 Apr 2025 04:48:20 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[So, I recently realized, that at least 30% of my GKE bill is traffic between zones "Network Inter Zone Data Transfer" SKU. This project is very heavy on internal traffic, so I can see how monthly data exchange between services can be in terms of hundreds of terabytesMy cluster was setup by default with nodes scattered across all zones in the region (default setup if I'm not mistaken)At this moment I decided to force all nodes into a single zone, which brought cost down, but it goes against all the recommendations about availabilitySo it got me thinking, if I want to achieve both goals at once: - have multi AZ cluster for availability - keep intra AZ traffic at minimum I know how to do it by hand: deploy separate app stack for each AZ and loadbalance traffic between them, but it seems like an overcomplicationIs there a less explicit way to prefer local communication between services in k8s?]]></content:encoded></item><item><title>Steam Linux Support - Valve will abandon support of the Steam client on Linux distributions without glibc 2.31 or newer as of 8/15/25</title><link>https://help.steampowered.com/en/faqs/view/107F-BB20-FB5A-1CE4</link><author>/u/wickedplayer494</author><category>dev</category><category>reddit</category><pubDate>Tue, 22 Apr 2025 04:25:08 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>What are Webhooks?</title><link>https://blog.algomaster.io/p/what-are-webhooks</link><author>Ashish Pratap Singh</author><category>dev</category><enclosure url="https://substack-post-media.s3.amazonaws.com/public/images/f2c9c574-39f0-426c-b540-6d7ecf838504_1468x1016.png" length="" type=""/><pubDate>Tue, 22 Apr 2025 04:00:37 +0000</pubDate><source url="https://blog.algomaster.io/">Dev - Algomaster</source><content:encoded><![CDATA[Imagine you're building an e-commerce platform and using an external payment processor like Stripe to collect payments from users.Once a user completes a payment, your system needs to:Notify the warehouse to start packingBut here's the challenge:Stripe operates on its own infrastructure. Your system doesn’t control it. So how do you know  when a payment goes through?A naive solution would be to keep asking Stripe every few seconds:Now imagine doing this for  on a site like Amazon. It just doesn’t scale and wastes server resources.Instead of your app repeatedly asking, what if Stripe could just tell you when the payment succeeds?In this article, we will explore:What does a Webhook request look like?How to setup a Webhook receiver?How to design a scalable Webhook infrastructure?A webhook is a simple way for one system (provider) to notify another system (receiver) in real time when an event happens using an HTTP request.Instead of one system asking repeatedly if something happened, the other system simply pushes the data as soon as the event is triggered.Let’s say you go to a busy restaurant.The host says: “There’s a 30-minute wait. Please leave your number, and we’ll text you when your table is ready.”You don’t need to stand at the counter asking every 2 minutes: “Is my table ready yet?”Instead, you walk away, and when your turn comes, they notify you automatically.That’s the idea behind webhooks.At a high level, webhooks work through registration, triggering, and delivery. Let’s walk through a real example to see what actually happens under the hood.Example: GitHub Webhook to Your AppLet’s say you’ve built a system that needs to react when someone opens a pull request (PR) in a GitHub repository — maybe to kick off a CI/CD pipeline, send a Slack notification, or run automated tests.Here’s how the webhook flow works step-by-step:Step 1: You Register a WebhookYou go to GitHub → Settings → WebhooksYou provide a  (e.g., https://myapp.com/github-webhook)You choose the  you care about, like , , or At this point, GitHub knows where to send data whenever those events occur.Step 2: GitHub Monitors EventsGitHub monitors its internal event stream for your repositoryWhen someone opens a new pull request, the event gets logged internallyStep 3: GitHub Sends a POST RequestGitHub prepares a  with details about the event: who opened the PR, which branch, repo metadata, etc.It makes a  request to your webhook URL:Step 4: Your Server Processes the EventYour app receives the requestYou verify the signature (to ensure it really came from GitHub)You process the payload — maybe enqueue it in a job queue, log it, or trigger business logicYou respond with an HTTP  to acknowledge receiptTo integrate with webhooks successfully, you need to understand what’s actually being sent to your server when the event occurs.Most webhook providers use the  to send event data to your server.Why POST? Because it allows the payload to be sent in the request body, which can include structured data (usually JSON) describing what happened.Rarely, some services allow , , or even , but  is the de facto standard for webhook delivery.The headers in a webhook request often include  and security-related information. Usually application/json (or sometimes application/x-www-form-urlencoded) Identifies the sender (e.g., Stripe/1.0, GitHub-Hookshot) Describes what type of event occurred (payment_intent.succeeded, pull_request, etc.)X-Signature / X-Hub-Signature: A hash of the payload using a secret key (HMAC) used to verify the request authenticity Unique ID for the webhook event (useful for logging/debugging)The body of a webhook request typically contains  in . This data includes what happened, when it happened, and which user/resource it’s related to.{
  "action": "opened",
  "number": 42,
  "pull_request": {
    "id": 11223344,
    "title": "Add login validation",
    "user": {
      "login": "octocat"
    },
    "created_at": "2025-04-21T12:00:00Z"
  },
  "repository": {
    "name": "awesome-project",
    "full_name": "octocat/awesome-project"
  },
  "sender": {
    "login": "octocat"
  }
}Example: Stripe Payment Intent Succeeded{
  "id": "evt_1PoJkD2eZvKYlo2CmOJbvwD9",
  "object": "event",
  "api_version": "2024-08-01",
  "created": 1713681701,
  "data": {
    "object": {
      "id": "pi_3JhdNe2eZvKYlo2C1IqojYg9",
      "object": "payment_intent",
      "amount": 2000,
      "currency": "usd",
      "status": "succeeded"
    }
  },
  "livemode": false,
  "type": "payment_intent.succeeded"
}Once you’ve registered your webhook URL with a provider like GitHub or Stripe, the next step is to build a robust endpoint on your server that can receive and process these events reliably.This sounds simple — just receive a POST request, right?But in production, things get tricky:Webhooks might arrive orYour server might be , or briefly  might try to spoof requestsLet’s break down how to setup your webhook receiver the  way.Create a Dedicated EndpointStart by exposing a simple HTTP endpoint like:Accept only POST requestsAccept only JSON (via )Webhook events can be , , or even  by providers. Your handler must ensure that processing the same event  has no side effects.Every webhook event includes a  (e.g.,  in Stripe,  in GitHub)Store a record of processed IDs in a database or cacheBefore processing, check if the event ID has already been handledif (eventAlreadyProcessed(eventId)) {
    return 200 OK;
}This ensures , even if you receive the same event more than once.Handle HTTP Status Codes ProperlyReturn  after successful handlingReturn  if the payload is invalid or malformedAvoid 5xx errors unless something truly failed because most providers  on 5xx4.2 Security ConsiderationsWebhooks are public endpoints. That means anyone on the internet can POST to them. You must validate incoming requests.Verify the Signature (HMAC)Most providers include a  of the payload, using a shared secret. This ensures the payload came from the real source.You configure a webhook secret: Stripe signs the payload using HMAC-SHA256You verify the signature on your end:String expectedSignature = hmacSha256(secret, payload);

if (!expectedSignature.equals(signatureFromHeader)) {
    return 403 Forbidden;
}This prevents spoofed or forged webhooks.Whitelist Source IPs (Optional)Some providers publish static IP ranges from which webhooks are sent. You can optionally block all other IPs to tighten access.Downside: This adds DevOps complexity and can break things if IPs change.Don’t Expose Sensitive DataDon’t log full webhook payloads in plaintextDon’t return stack traces or internal errors in responsesDon’t include secrets or tokens in the response bodyAs your application grows and starts receiving thousands or even millions of webhook events daily, a simple synchronous handler won’t be enough.To make your webhook system reliable, fault-tolerant, and scalable, you need a design that’s asynchronous, decoupled, and observable.Let’s walk through how to build a production-grade webhook processing pipeline.5.1 Queue Incoming RequestsDon’t do heavy processing inside your webhook endpoint.Parse and validate the requestVerify its authenticity (signature, event ID, etc.)Immediately enqueue the event into a  like: (for high-throughput event pipelines) (lightweight and flexible) (serverless and easy to manage)This way, your server can return a fast , while actual processing happens in the background.5.2 Store Events for Audit & ReplayTo ensure traceability and recovery, store every incoming event in a database:Source info (e.g., IP address, headers)Status (, , )Use a durable data store like PostgreSQL, MongoDB, or DynamoDB.This gives you the ability to  and .5.3 Process with Asynchronous WorkersBackground workers (or worker pools) pull events from the queue and perform actual business logic:Deduplicate the event using eventIDUpdate internal databases or call downstream servicesTrigger notifications or workflowsYou can scale them horizontallyYou gain control over load and processing rateYou isolate failures from the webhook endpointSometimes, event processing fails temporarily due to a database issue, timeout, or an unavailable downstream API.Your workers should  those events automatically, using a : (recommended): retry with increasing delays (1s, 2s, 4s…): retry every N seconds (e.g., retry every 30s): add randomness to avoid all retries happening at onceAlways limit retries and log failures after the final attempt.5.5 Use a Dead Letter Queue (DLQ)If a webhook consistently fails after multiple retries, don’t keep retrying forever.Instead, send it to a , a special holding queue for problematic events.Investigate the root causeRetry manually after fixing the issueThis prevents bad events from clogging your pipeline and ensures no event is lost silently.5.6 Add Observability: Logs, Metrics & AlertsYou can’t fix what you can’t see. Add observability at every stage of the pipeline.Total webhook events received per hourRetry count and DLQ event countSudden drop in incoming eventsGrowing queue sizes (indicating backlog)Popular observability tools: (open-source),  (cloud-based) (Elasticsearch + Logstash + Kibana)With proper observability, you’ll catch issues early before they impact your users.If you found it valuable, hit a like ❤️ and consider subscribing for more such content every week.This post is public so feel free to share it. If you’re enjoying this newsletter and want to get even more value, consider becoming a .I hope you have a lovely day!]]></content:encoded></item><item><title>Cheating the Reaper in Go · mcyoung</title><link>https://mcyoung.xyz/2025/04/21/go-arenas/</link><author>/u/FoxInTheRedBox</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Tue, 22 Apr 2025 03:36:52 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Even though I am a C++ programmer at heart, Go fascinates me for none of the reasons you think. Go has made several interesting design decisions:It has virtually no Undefined Behavior.It has very simple GC semantics that they’re mostly stuck with due to design decisions in the surface language.These things mean that despite Go having a GC, it’s possible to do manual memory management in pure Go and in cooperation with the GC (although without any help from the  package). To demonstrate this, we will be building an untyped, garbage-collected arena abstraction in Go which relies on several GC implementation details.I would never play this kind of game in Rust or C++, because LLVM is extremely intelligent and able to find all kinds of ways to break you over the course of frequent compiler upgrades. On the other hand, although Go does not promise any compatibility across versions for code that imports , in practice, two forces work against Go doing this:Go prioritizes not breaking the ecosystem; this allows to assume that Hyrum’s Law will protect certain observable behaviors of the runtime, from which we may infer what can or cannot break easily.This is in contrast to a high-performance native compiler like LLVM, which has a carefully defined boundary around all UB, allowing them to arbitrarily break programs that cross it (mostly) without fear of breaking the ecosystem.So, let’s dive in and cheat death.Our goal is to build an , which is a data structure for efficient allocation of memory that has the same lifetime. This reduces pressure on the general-purpose allocator by only requesting memory in large chunks and then freeing it all at once.For a comparison in Go, consider the following program:This program will print successive powers of 2: this is because  is implemented approximately like so:For appending small pieces,  is only called  times, a big improvement over calling it for every call to . Virtually every programming language’s dynamic array abstraction makes this optimization.An arena generalizes this concept, but instead of resizing exponentially, it allocates  blocks and vends pointers into them. The interface we want to conform to is as follows:In go a size and and an alignment, out comes a pointer fresh memory with that layout. Go does not have user-visible uninitialized memory, so we additionally require that the returned region be zeroed. We also require that  be a power of two.We can give this a type-safe interface by writing a generic  function:This all feels very fine and dandy to anyone used to hurting themselves with  or  in C++, but there is a small problem. What happens when we allocate pointer-typed memory into this allocator? takes a size and an alignment, which is sufficient to describe the  of any type. For example, on 64-bit systems,  and  have the same layout: 8 bytes of size, and 8 bytes of alignment.However, the Go GC (and all garbage collectors, generally) require one additional piece of information, which is somewhere between the layout of a value (how it is placed in memory) and the type of a value (rich information on its structure). To understand this, we need a brief overview on what a GC does.For a complete overview on how to build a simple GC, take a look at a toy GC I designed some time ago: The Alkyne GC.A garbage collector’s responsibility is to maintain a memory allocator and an accounting of:What memory has been allocated.Whether that memory is still in use.Memory that is not in use can be reclaimed and marked as unallocated, for re-use.The most popular way to accomplish this is via a “mark and sweep” architecture. The GC will periodically walk the entire object graph of the program from certain pre-determined ; anything it finds is “marked” as alive. After a mark is complete, all other memory is “swept”, which means to mark it is unallocated for future re-use, or to return it to the OS, in the case of significant surplus.The roots are typically entities that are actively being manipulated by the program. In the case of Go, this is anything currently on the stack of some G, or anything in a global (of which there is a compile-time-known set).The marking phase begins with , which looks at the stack of each G and locates any pointers contained therein. The Go compiler generates metadata for each function that specifies which stack slots in a function’s frame contain pointers. All of these pointers are live by definition.These pointers are placed into a queue, and each pointer is traced to its allocation on the heap. If the GC does not know anything about a particular address, it is discarded as foreign memory that does not need to be marked. If it does, each pointer in that allocation is pushed onto the queue if it has not already been marked as alive. The process continues until the queue is empty.The critical step here is to take the address of some allocation, and convert it into all of the pointer values within. Go has precise garbage collection, which means that it only treats things declared as pointers in the surface language as pointers: an integer that happens to look like an address will not result in sweeping. This results in more efficient memory usage, but trades off some more complexity in the GC.For example, the types , , ,  all contain at least one pointer, while , , struct {X bool; F uintptr} do not. The latter are called  types.Go enhances the layout of a type into a  by adding a bitset that specifies which pointer-aligned, pointer-sized words of the type’s memory region contain a pointer. These are called the . For example, here are the shapes of a few Go types on a 64-bit system.In the Go GC, each allocation is tagged with its shape (this is done in a variety of ways in the GC, either through an explicit header on the allocation, itself (a “malloc header”), a runtime type stored in the allocation’s , or another mechanism). When scanning a value, it uses this information to determine where the pointers to scan through are.The most obvious problem with our  type is that it does not discriminate shapes, so it cannot allocate memory that contains pointers: the GC will not be able to find the pointers, and will free them prematurely!In our example where we allocated an  in our custom allocator, we wind up with a  on the stack. You would think that Go would simply trace through the first  to find an  and mark it as being alive, but that is not what happens! Go instead finds a pointer into some chunk that the custom allocator grabbed from the heap, which is missing the pointer bits of its shape!Why does go not look at the type of the pointer it steps through? Two reasons.All pointers in Go are untyped from the runtime’s perspective; every  gets erased into an . This allows much of the Go runtime to be “generic” without using actual generics.Pointee metadata can be aggregated, so that each pointer to an object does not have to remember its type at runtime.The end result for us is that we can’t put pointers on the arena. This makes our  API unsafe, especially since Go does not provide a standard constraint for marking generic parameters as pointer-free: unsurprisingly, the don’t expect most users to care about such a detail.It  possible to deduce the pointer bits of a type using reflection, but that’s very slow, and the whole point of using arenas is to go fast. As we design our arena, though, it will become clear that there is a safe way to have pointers on it.Now that we have a pretty good understanding about what the Go GC is doing, we can go about designing a fast arena structure.The ideal case is that a call to  is very fast: just offsetting a pointer in the common case. One assumption we can make off the bat is that all memory can be forced to have maximum alignment: most objects are a pointer or larger, and Go does have a maximum alignment for ordinary user types, so we can just ignore the  parameter and always align to say, 8 bytes. This means that the pointer to the next unallocated chunk will always be well-aligned. Thus, we might come up with a structure like this one:How fast is this really? Here’s a simple benchmark for it.The focus of this benchmark is to measure the cost of allocating many objects of the same size. The number of times the  loop will execute is unknown, and determined by the benchmarking framework to try to reduce statistical anomaly. This means that if we instead just benchmark a single allocation, the result will be  sensitive to the number of runs.We also use  to get a throughput measurement on the benchmark. This is a bit easier to interpret than the gross , the benchmark would otherwise produce. It tells us how much memory each allocator can allocate per unit time.We want to compare against , but just writing  will get optimized out, since the resulting pointer does not escape. Writing it to a global is sufficient to convince Go that it escapes.Here’s the results, abbreviated to show only the bytes per second. All benchmarks were performed on my AMD Ryzen Threadripper 3960X. Larger is better.This is quite nice, and certainly worth pursuing! The performance increase seems to scale up with the amount of memory allocated, for a 2x-4x improvement across different cases.Now we need to contend with the fact that our implementation is completely broken if we want to have pointers in it.In , when we assign a freshly-allocated chunk, we overwrite , which means the GC can reclaim it. But this is fine: as long as pointers into that arena chunk are alive, the GC will not free it, independent of the arena. So it seems like we don’t need to worry about it?However, the whole point of an arena is to allocate lots of memory that has the same lifetime. This is common for graph data structures, such as an AST or a compiler IR, which performs a lot of work that allocates a lot and then throws the result away.We are not allowed to put pointers in the arena, because they would disappear from the view of the GC and become freed too soon. But, if a pointer wants to go on an arena, it necessarily outlive the whole arena, since it outlives part of the arena, and the arena is meant to have the same lifetime.In particular, if we could make it so that holding any pointer returned by  prevents the  from being swept by the GC, the arena can safely contain pointers into itself! Consider this:We have a pointer . It is allocated on some arena .The GC sees our pointer (as a type-erased ) and marks its allocation as live.Somehow, the GC also marks  as alive as a consequence.Somehow, the GC then marks every chunk  has allocated as alive.Therefore he chunk that  points to is also alive, so  does not need to be marked directly, and will not be freed early.The step (3) is crucial. By forcing the whole arena to be marked, any pointers stored in the arena into itself will be kept alive automatically, without the GC needing to know how to scan for them.So, even though  is still going to result in a use-after-free, *New[*int](a) = New[int](a) would not! This small improvement does not make arenas themselves safe, but a data structure with an internal arena can be completely safe, so long as the only pointers that go into the arena are from the arena itself.How can we make this work? The easy part is (4), which we can implement by adding a  to the arena, and sticking every pointer we allocate into it.The cost of the  is amortized: to allocate  bytes, we wind up allocating an additional  times. But what does this do to our benchmarks?Seems pretty much the same, which is a good sign.Now that the arena does not discard any allocated memory, we can focus on condition (3): making it so that if any pointer returned by  is alive, then so is the whole arena.Here we can make use of an important property of how Go’s GC works: any pointer into an allocation will keep it alive, as well as anything reachable from that pointer. But the chunks we’re allocating are s, which will not be scanned. If there could  be a single pointer in this slice that was scanned, we would be able to stick the pointer  there, and so when anything that  returns is scanned, it would cause  to be marked as alive.So far, we have been allocating  using , but we would actually like to allocate struct { A [N]uintptr; P unsafe.Pointer }, where  is some dynamic value.In its infintie wisdom, the Go standard library actually gives us a dedicated mechanism to do this: . This can be used to construct arbitrary anonymous  types at runtime, which we can then allocate on the heap.So, instead of calling , we might call this function:This appears to have a minor but noticeable effect on performance.Looking back at , the end of this function has a branch:This is the absolute hottest part of allocation, since it is executed every time we call this function. The branch is a bit unfortunate, but it’s necessary, as noted by the comment.In C++, if we have an array of  with  elements in it, and  is a pointer to the start of the array,  is a valid pointer, even though it can’t be dereferenced; it points “one past the end” of the array. This is a useful construction, since, for example, you can use it to eliminate a loop induction variable:Go, however, gets very upset if you do this, because it confuses the garbage collector. The GC can’t tell the difference between a one-past-the-end pointer for allocation A, and for the start of allocation B immediately after it. At best this causes memory to stay alive for longer, and at worst it triggers safety interlocks in the GC. The GC will panic if it happens to scan a pointer for an address that it knows has been freed.But in our code above, every chunk now has an extra element at the very end that is not used for allocation, so we  have a pointer that  one-past-the-end of the  that we are vending memory from.The updated allocation function would look like this:Notably, we do not replace  with an end pointer, because of the  comparison. We can’t actually avoid the subtraction  because we would have to do it to make this comparison work if we got rid of .So how much better is this?Remarkably, not very! This is an improvement on the order of magnitude of one or two percentage points. This is because the branch we deleted is extremely predictable.Turns out there’s a bigger improvement we can make.Here’s the assembly Go generated for this function, heavily abridged, and annotated with the corresponding Go source code.There’s a lot going on in this function, but most of it is a mix of Go not being great at register allocation, and lots of .A write barrier is a mechanism for synchronizing ordinary user code with the GC. Go generates code for one any time a non-pointer-free type is stored. For example, writing to a , , or  requires a write barrier.Write barriers are implemented as follows: is checked, which determines whether the write barrier is necessary, which is only when the GC is in the mark phase. Otherwise the branch is taken to skip the write barrier.A call to one of the  functions happens.  is the number of pointers that the GC needs to be informed of.This function calls , which returns a buffer onto which pointers the GC needs to now trace through should be written to.The actual store happens.A write barrier is required for a case like the following. Consider the following code.This function will call  to allocate eight bytes of memory. The resulting pointer will be returned in . This function then stores  into  and returns. If we Godbolt this function, we’ll find that it does, in fact, generate a write barrier:Note that two pointers get written: the pointer returned by , and the old value of . This ensures that regardless of where in this function the GC happens to be scanning through , it sees both values during the mark phase.Now, this isn’t necessary if the relevant pointers are already reachable in some other way… which is exactly the case in our arena (thanks to the  slice). So the write barrier in the fast path is redundant.But, how do we get rid of it? There is a , but that’s not allowed outside of a list of packages allowlisted in the compiler. It also doens’t disable write barriers; it simply generates a diagnostic if any are emitted.But remember, write barriers only occur when storing pointer-typed memory… so we can just replace  with . hates this, because it doesn’t know that we’re smarter than it is. Does This make the code faster? To make it a little bit more realistic, I’ve written a separate variant of the benchmarks that hammers the GC really hard in a separate G:The result indicates that this is a worthwhile optimization for churn-heavy contexts. Performance is much worse overall, but that’s because the GC is pre-empting everyone. The improvement seems to be on the order of 20% for very small allocations.Before
After
Go does not offer an easy mechanism to “reallocate” an allocation, as with  in C. This is because it has no mechanism for freeing pointers explicitly, which is necessary for a reallocation abstraction.But we already don’t care about safety, so we can offer reallocation on our arena. Now, the reallocation we can offer is quite primitive: if a chunk happens to be the most recent one allocated, we can grow it. Otherwise we just allocate a new chunk and don’t free the old one.This makes it possible to implement “arena slices” that can be constructed by appending, which will not trigger reallocation on slice growth as long as nothing else gets put on the arena. would look something like this:Then, whenever we append to our arena slice, we can call  to grow it. However, this does not work if the slice’s base pointer is not the original address returned by  or . It is an exercise for the reader to:Implement a  type that uses an arena for allocation.Make this work for any value of  within the most recent allocation, not just the base offset. This requires extra book-keeping.Here is the entirety of the code that we have developed, not including the reallocation function above.There are other optimizations that we could make here that I haven’t discussed. For example, arenas could be re-used; once an arena is done, it could be “reset” and placed into a . This arena would not need to go into the GC to request new chunks, re-using the ones previously allocated (and potentially saving on the cost of zeroing memory over and over again).I did say that this relies very heavily on Go’s internal implementation details. Whats the odds that they get broken in the future? Well, the requirement that allocations know their shape is forced by the existence of , and the requirement that a pointer into any part of an allocation keeps the whole thing alive essentially comes from slices being both sliceable and mutable; once a slice escapes to the heap (and thus multiple goroutines) coordinating copies for shrinking a slice would require much more complexity than the current write barrier implementation.And in my opinion, it’s pretty safe to say that Hyrum’s Law has us covered here. ;)]]></content:encoded></item><item><title>Single method interfaces vs functions</title><link>https://www.reddit.com/r/golang/comments/1k4u69x/single_method_interfaces_vs_functions/</link><author>/u/RomanaOswin</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Tue, 22 Apr 2025 01:17:26 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I know this has been asked before and it's fairly subjective, but single method interfaces vs functions. Which would you choose when, and why? Both seemingly accomplish the exact same thing with minor tradeoffs.In this case, I'm looking at this specifically in defining the capabilities provided in a domain-driven design. For example:go type SesssionCreator interface { CreateSession(Session) error } type SessionReader interface { ReadSession(id string) (Session, error) } go type ( CreateSessionFunc(Session) error ReadSessionFunc(id string) (Session, error) ) And, then in some consumer, e.g., an HTTP handler:```go func PostSession(store identity.SessionCreator) HttpHandlerFunc { return func(req Request) { store.CreateSession(s) } }func PostSession(createSession identity.CreateSessionFunc) HttpHandlerFunc { return func(req Request) { createSession(s) } } ```I think in simple examples like this, functions seem simpler than interfaces, the test will be shorter and easier to read, and so on. It gets more ambiguous when the consumer function performs multiple actions, e.g.:```go func PostSomething(store interface{ identity.SessionReader catalog.ItemReader execution.JobCreator }) HttpHandlerFunc { return func(req Request) { // Use store } }func PostSomething( readSession identity.ReadSessionFunc, readItem catalog.ReadItemFunc, createJob execution.CreateJobFunc, ) HttpHandlerFunc { return func(req Request) { // use individual functions } } ```And, on the initiating side of this, assuming these are implemented by some aggregate "store" repository:go router.Post("/things", PostSomething(store)) // vs router.Post("/things", PostSomething(store.ReadSession, store.ReadItem, store.CreateJob) I'm sure there are lots of edge cases and reasons for one approach over the other. Idiomatic naming for a lot of small, purposeful interfaces in Go with  can get a bit wonky sometimes. What else? Which approach would you take, and why? Or something else entirely?]]></content:encoded></item><item><title>Prolog Adventure Game</title><link>https://github.com/stefanrodrigues2/Prolog-Adventure-game</link><author>shakna</author><category>dev</category><category>hn</category><pubDate>Tue, 22 Apr 2025 00:25:12 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Kubernetes Multicontainer Pods: An Overview</title><link>https://kubernetes.io/blog/2025/04/22/multi-container-pods-overview/</link><author></author><category>dev</category><category>k8s</category><pubDate>Tue, 22 Apr 2025 00:00:00 +0000</pubDate><source url="https://kubernetes.io/">Dev - Kubernetes Blog</source><content:encoded><![CDATA[As cloud-native architectures continue to evolve, Kubernetes has become the go-to platform for deploying complex, distributed systems. One of the most powerful yet nuanced design patterns in this ecosystem is the sidecar pattern—a technique that allows developers to extend application functionality without diving deep into source code.The origins of the sidecar patternThink of a sidecar like a trusty companion motorcycle attachment. Historically, IT infrastructures have always used auxiliary services to handle critical tasks. Before containers, we relied on background processes and helper daemons to manage logging, monitoring, and networking. The microservices revolution transformed this approach, making sidecars a structured and intentional architectural choice.
With the rise of microservices, the sidecar pattern became more clearly defined, allowing developers to offload specific responsibilities from the main service without altering its code. Service meshes like Istio and Linkerd have popularized sidecar proxies, demonstrating how these companion containers can elegantly handle observability, security, and traffic management in distributed systems.Kubernetes implementationIn Kubernetes, sidecar containers operate within
the same Pod as the main application, enabling communication and resource sharing.
Does this sound just like defining multiple containers along each other inside the Pod? It actually does, and
this is how sidecar containers had to be implemented before Kubernetes v1.29.0, which introduced
native support for sidecars.
Sidecar containers can now be defined within a Pod manifest using the  field. What makes
it a sidecar container is that you specify it with . You can see an example of this below, which is a partial snippet of the full Kubernetes manifest:That field name,  may sound confusing. How come when you want to define a sidecar container, you have to put an entry in the  array?  are run to completion just before main application starts, so they’re one-off, whereas sidecars often run in parallel to the main app container. It’s the  with  which differs classic init containers from Kubernetes-native sidecar containers and ensures they are always up.When to embrace (or avoid) sidecarsWhile the sidecar pattern can be useful in many cases, it is generally not the preferred approach unless the use case justifies it. Adding a sidecar increases complexity, resource consumption, and potential network latency. Instead, simpler alternatives such as built-in libraries or shared infrastructure should be considered first.You need to extend application functionality without touching the original codeImplementing cross-cutting concerns like logging, monitoring or securityWorking with legacy applications requiring modern networking capabilitiesDesigning microservices that demand independent scaling and updatesResource efficiency is your primary concernMinimal network latency is criticalSimpler alternatives existYou want to minimize troubleshooting complexityFour essential multi-container patternsThe  pattern is used to execute (often critical) setup tasks before the main application container starts. Unlike regular containers, init containers run to completion and then terminate, ensuring that preconditions for the main application are met.Verifying dependency availabilityRunning database migrationsThe init container ensures your application starts in a predictable, controlled environment without code modifications.An ambassador container provides Pod-local helper services that expose a simple way to access a network service. Commonly, ambassador containers send network requests on behalf of a an application container and
take care of challenges such as service discovery, peer identity verification, or encryption in transit.Perfect when you need to:Offload client connectivity concernsImplement language-agnostic networking featuresAdd security layers like TLSCreate robust circuit breakers and retry mechanismsA  sidecar provides configuration updates to an application dynamically, ensuring it always has access to the latest settings without disrupting the service. Often the helper needs to provide an initial
configuration before the application would be able to start successfully.Fetching environment variables and secretsPolling configuration changesDecoupling configuration management from application logicAn  (or sometimes ) container enables interoperability between the main application container and external services. It does this by translating data formats, protocols, or APIs.Transforming legacy data formatsBridging communication protocolsFacilitating integration between mismatched servicesWhile sidecar patterns offer tremendous flexibility, they're not a silver bullet. Each added sidecar introduces complexity, consumes resources, and potentially increases operational overhead. Always evaluate simpler alternatives first.
The key is strategic implementation: use sidecars as precision tools to solve specific architectural challenges, not as a default approach. When used correctly, they can improve security, networking, and configuration management in containerized environments.
Choose wisely, implement carefully, and let your sidecars elevate your container ecosystem.]]></content:encoded></item><item><title>EKS Multicluster service mesh</title><link>https://www.reddit.com/r/kubernetes/comments/1k4rwnv/eks_multicluster_service_mesh/</link><author>/u/IllustriousStorage28</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Mon, 21 Apr 2025 23:27:32 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[I work for an enterprise company with 2 clusters for production running same set of applications and being load balanced by aws alb. We are looking to introduce service mesh in our environment, while evaluating multiple meshes we came across istio and kuma both being a good fit for multi-cluster environment. On one hand kuma looks to be very easy to setup and built with multi-cluster architecture. Though docs are lacking a lot of information and don’t see much community support either. On the other hand istio has been battle tested in multiple production environments and has a great community support and documentations. Though multi-cluster setup is more sort of extension than built in capability. Also, various tools required to manage configs and visualise metrics. We would want capabilities to control traffic effectively and ability to load balance between multiple cluster not being connected directly ( separate vpc with peering and non-peering connections). And ability to be able add a new cluster as we want. Is there anyone here who has used istio or kuma multi-cluster. Also, please do share your experience with either of them in managing, debugging and upgrading them. ]]></content:encoded></item><item><title>SouthEast LinuxFest 2025 Registration and CFP is open</title><link>https://southeastlinuxfest.org/2025/04/self-2025-update/</link><author>/u/q5sys</author><category>dev</category><category>reddit</category><pubDate>Mon, 21 Apr 2025 22:18:58 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[This year’s theme: Retro computing.Registration is NOW OPEN.Call for speakers closes very soon on 4/27!Do you have some cool retro computing stuff you’d like to bring to show or swap? We’ll be having a Swapfest at SELF.We are looking for sponsors for this year’s event.Get ahead of the rush and register for the ham radio exam (FREE).We are also looking for volunteers for this year’s event. Volunteers are eligible for the volunteer raffle, which features everything from servers and networking equipment to books about open source autographed by the author.]]></content:encoded></item><item><title>Evertop: E-ink IBM XT clone with 100+ hours of battery life</title><link>https://github.com/ericjenott/Evertop</link><author>harryvederci</author><category>dev</category><category>hn</category><pubDate>Mon, 21 Apr 2025 22:07:49 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Cheating the Reaper in Go</title><link>https://mcyoung.xyz/2025/04/21/go-arenas/</link><author>ingve</author><category>dev</category><category>hn</category><pubDate>Mon, 21 Apr 2025 21:46:32 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Even though I am a C++ programmer at heart, Go fascinates me for none of the reasons you think. Go has made several interesting design decisions:It has virtually no Undefined Behavior.It has very simple GC semantics that they’re mostly stuck with due to design decisions in the surface language.These things mean that despite Go having a GC, it’s possible to do manual memory management in pure Go and in cooperation with the GC (although without any help from the  package). To demonstrate this, we will be building an untyped, garbage-collected arena abstraction in Go which relies on several GC implementation details.I would never play this kind of game in Rust or C++, because LLVM is extremely intelligent and able to find all kinds of ways to break you over the course of frequent compiler upgrades. On the other hand, although Go does not promise any compatibility across versions for code that imports , in practice, two forces work against Go doing this:Go prioritizes not breaking the ecosystem; this allows to assume that Hyrum’s Law will protect certain observable behaviors of the runtime, from which we may infer what can or cannot break easily.This is in contrast to a high-performance native compiler like LLVM, which has a carefully defined boundary around all UB, allowing them to arbitrarily break programs that cross it (mostly) without fear of breaking the ecosystem.So, let’s dive in and cheat death.Our goal is to build an , which is a data structure for efficient allocation of memory that has the same lifetime. This reduces pressure on the general-purpose allocator by only requesting memory in large chunks and then freeing it all at once.For a comparison in Go, consider the following program:This program will print successive powers of 2: this is because  is implemented approximately like so:For appending small pieces,  is only called  times, a big improvement over calling it for every call to . Virtually every programming language’s dynamic array abstraction makes this optimization.An arena generalizes this concept, but instead of resizing exponentially, it allocates  blocks and vends pointers into them. The interface we want to conform to is as follows:In go a size and and an alignment, out comes a pointer fresh memory with that layout. Go does not have user-visible uninitialized memory, so we additionally require that the returned region be zeroed. We also require that  be a power of two.We can give this a type-safe interface by writing a generic  function:This all feels very fine and dandy to anyone used to hurting themselves with  or  in C++, but there is a small problem. What happens when we allocate pointer-typed memory into this allocator? takes a size and an alignment, which is sufficient to describe the  of any type. For example, on 64-bit systems,  and  have the same layout: 8 bytes of size, and 8 bytes of alignment.However, the Go GC (and all garbage collectors, generally) require one additional piece of information, which is somewhere between the layout of a value (how it is placed in memory) and the type of a value (rich information on its structure). To understand this, we need a brief overview on what a GC does.For a complete overview on how to build a simple GC, take a look at a toy GC I designed some time ago: The Alkyne GC.A garbage collector’s responsibility is to maintain a memory allocator and an accounting of:What memory has been allocated.Whether that memory is still in use.Memory that is not in use can be reclaimed and marked as unallocated, for re-use.The most popular way to accomplish this is via a “mark and sweep” architecture. The GC will periodically walk the entire object graph of the program from certain pre-determined ; anything it finds is “marked” as alive. After a mark is complete, all other memory is “swept”, which means to mark it is unallocated for future re-use, or to return it to the OS, in the case of significant surplus.The roots are typically entities that are actively being manipulated by the program. In the case of Go, this is anything currently on the stack of some G, or anything in a global (of which there is a compile-time-known set).The marking phase begins with , which looks at the stack of each G and locates any pointers contained therein. The Go compiler generates metadata for each function that specifies which stack slots in a function’s frame contain pointers. All of these pointers are live by definition.These pointers are placed into a queue, and each pointer is traced to its allocation on the heap. If the GC does not know anything about a particular address, it is discarded as foreign memory that does not need to be marked. If it does, each pointer in that allocation is pushed onto the queue if it has not already been marked as alive. The process continues until the queue is empty.The critical step here is to take the address of some allocation, and convert it into all of the pointer values within. Go has precise garbage collection, which means that it only treats things declared as pointers in the surface language as pointers: an integer that happens to look like an address will not result in sweeping. This results in more efficient memory usage, but trades off some more complexity in the GC.For example, the types , , ,  all contain at least one pointer, while , , struct {X bool; F uintptr} do not. The latter are called  types.Go enhances the layout of a type into a  by adding a bitset that specifies which pointer-aligned, pointer-sized words of the type’s memory region contain a pointer. These are called the . For example, here are the shapes of a few Go types on a 64-bit system.In the Go GC, each allocation is tagged with its shape (this is done in a variety of ways in the GC, either through an explicit header on the allocation, itself (a “malloc header”), a runtime type stored in the allocation’s , or another mechanism). When scanning a value, it uses this information to determine where the pointers to scan through are.The most obvious problem with our  type is that it does not discriminate shapes, so it cannot allocate memory that contains pointers: the GC will not be able to find the pointers, and will free them prematurely!In our example where we allocated an  in our custom allocator, we wind up with a  on the stack. You would think that Go would simply trace through the first  to find an  and mark it as being alive, but that is not what happens! Go instead finds a pointer into some chunk that the custom allocator grabbed from the heap, which is missing the pointer bits of its shape!Why does go not look at the type of the pointer it steps through? Two reasons.All pointers in Go are untyped from the runtime’s perspective; every  gets erased into an . This allows much of the Go runtime to be “generic” without using actual generics.Pointee metadata can be aggregated, so that each pointer to an object does not have to remember its type at runtime.The end result for us is that we can’t put pointers on the arena. This makes our  API unsafe, especially since Go does not provide a standard constraint for marking generic parameters as pointer-free: unsurprisingly, the don’t expect most users to care about such a detail.It  possible to deduce the pointer bits of a type using reflection, but that’s very slow, and the whole point of using arenas is to go fast. As we design our arena, though, it will become clear that there is a safe way to have pointers on it.Now that we have a pretty good understanding about what the Go GC is doing, we can go about designing a fast arena structure.The ideal case is that a call to  is very fast: just offsetting a pointer in the common case. One assumption we can make off the bat is that all memory can be forced to have maximum alignment: most objects are a pointer or larger, and Go does have a maximum alignment for ordinary user types, so we can just ignore the  parameter and always align to say, 8 bytes. This means that the pointer to the next unallocated chunk will always be well-aligned. Thus, we might come up with a structure like this one:How fast is this really? Here’s a simple benchmark for it.The focus of this benchmark is to measure the cost of allocating many objects of the same size. The number of times the  loop will execute is unknown, and determined by the benchmarking framework to try to reduce statistical anomaly. This means that if we instead just benchmark a single allocation, the result will be  sensitive to the number of runs.We also use  to get a throughput measurement on the benchmark. This is a bit easier to interpret than the gross , the benchmark would otherwise produce. It tells us how much memory each allocator can allocate per unit time.We want to compare against , but just writing  will get optimized out, since the resulting pointer does not escape. Writing it to a global is sufficient to convince Go that it escapes.Here’s the results, abbreviated to show only the bytes per second. All benchmarks were performed on my AMD Ryzen Threadripper 3960X. Larger is better.This is quite nice, and certainly worth pursuing! The performance increase seems to scale up with the amount of memory allocated, for a 2x-4x improvement across different cases.Now we need to contend with the fact that our implementation is completely broken if we want to have pointers in it.In , when we assign a freshly-allocated chunk, we overwrite , which means the GC can reclaim it. But this is fine: as long as pointers into that arena chunk are alive, the GC will not free it, independent of the arena. So it seems like we don’t need to worry about it?However, the whole point of an arena is to allocate lots of memory that has the same lifetime. This is common for graph data structures, such as an AST or a compiler IR, which performs a lot of work that allocates a lot and then throws the result away.We are not allowed to put pointers in the arena, because they would disappear from the view of the GC and become freed too soon. But, if a pointer wants to go on an arena, it necessarily outlive the whole arena, since it outlives part of the arena, and the arena is meant to have the same lifetime.In particular, if we could make it so that holding any pointer returned by  prevents the  from being swept by the GC, the arena can safely contain pointers into itself! Consider this:We have a pointer . It is allocated on some arena .The GC sees our pointer (as a type-erased ) and marks its allocation as live.Somehow, the GC also marks  as alive as a consequence.Somehow, the GC then marks every chunk  has allocated as alive.Therefore he chunk that  points to is also alive, so  does not need to be marked directly, and will not be freed early.The step (3) is crucial. By forcing the whole arena to be marked, any pointers stored in the arena into itself will be kept alive automatically, without the GC needing to know how to scan for them.So, even though  is still going to result in a use-after-free, *New[*int](a) = New[int](a) would not! This small improvement does not make arenas themselves safe, but a data structure with an internal arena can be completely safe, so long as the only pointers that go into the arena are from the arena itself.How can we make this work? The easy part is (4), which we can implement by adding a  to the arena, and sticking every pointer we allocate into it.The cost of the  is amortized: to allocate  bytes, we wind up allocating an additional  times. But what does this do to our benchmarks?Seems pretty much the same, which is a good sign.Now that the arena does not discard any allocated memory, we can focus on condition (3): making it so that if any pointer returned by  is alive, then so is the whole arena.Here we can make use of an important property of how Go’s GC works: any pointer into an allocation will keep it alive, as well as anything reachable from that pointer. But the chunks we’re allocating are s, which will not be scanned. If there could  be a single pointer in this slice that was scanned, we would be able to stick the pointer  there, and so when anything that  returns is scanned, it would cause  to be marked as alive.So far, we have been allocating  using , but we would actually like to allocate struct { A [N]uintptr; P unsafe.Pointer }, where  is some dynamic value.In its infintie wisdom, the Go standard library actually gives us a dedicated mechanism to do this: . This can be used to construct arbitrary anonymous  types at runtime, which we can then allocate on the heap.So, instead of calling , we might call this function:This appears to have a minor but noticeable effect on performance.Looking back at , the end of this function has a branch:This is the absolute hottest part of allocation, since it is executed every time we call this function. The branch is a bit unfortunate, but it’s necessary, as noted by the comment.In C++, if we have an array of  with  elements in it, and  is a pointer to the start of the array,  is a valid pointer, even though it can’t be dereferenced; it points “one past the end” of the array. This is a useful construction, since, for example, you can use it to eliminate a loop induction variable:Go, however, gets very upset if you do this, because it confuses the garbage collector. The GC can’t tell the difference between a one-past-the-end pointer for allocation A, and for the start of allocation B immediately after it. At best this causes memory to stay alive for longer, and at worst it triggers safety interlocks in the GC. The GC will panic if it happens to scan a pointer for an address that it knows has been freed.But in our code above, every chunk now has an extra element at the very end that is not used for allocation, so we  have a pointer that  one-past-the-end of the  that we are vending memory from.The updated allocation function would look like this:Notably, we do not replace  with an end pointer, because of the  comparison. We can’t actually avoid the subtraction  because we would have to do it to make this comparison work if we got rid of .So how much better is this?Remarkably, not very! This is an improvement on the order of magnitude of one or two percentage points. This is because the branch we deleted is extremely predictable.Turns out there’s a bigger improvement we can make.Here’s the assembly Go generated for this function, heavily abridged, and annotated with the corresponding Go source code.There’s a lot going on in this function, but most of it is a mix of Go not being great at register allocation, and lots of .A write barrier is a mechanism for synchronizing ordinary user code with the GC. Go generates code for one any time a non-pointer-free type is stored. For example, writing to a , , or  requires a write barrier.Write barriers are implemented as follows: is checked, which determines whether the write barrier is necessary, which is only when the GC is in the mark phase. Otherwise the branch is taken to skip the write barrier.A call to one of the  functions happens.  is the number of pointers that the GC needs to be informed of.This function calls , which returns a buffer onto which pointers the GC needs to now trace through should be written to.The actual store happens.A write barrier is required for a case like the following. Consider the following code.This function will call  to allocate eight bytes of memory. The resulting pointer will be returned in . This function then stores  into  and returns. If we Godbolt this function, we’ll find that it does, in fact, generate a write barrier:Note that two pointers get written: the pointer returned by , and the old value of . This ensures that regardless of where in this function the GC happens to be scanning through , it sees both values during the mark phase.Now, this isn’t necessary if the relevant pointers are already reachable in some other way… which is exactly the case in our arena (thanks to the  slice). So the write barrier in the fast path is redundant.But, how do we get rid of it? There is a , but that’s not allowed outside of a list of packages allowlisted in the compiler. It also doens’t disable write barriers; it simply generates a diagnostic if any are emitted.But remember, write barriers only occur when storing pointer-typed memory… so we can just replace  with . hates this, because it doesn’t know that we’re smarter than it is. Does This make the code faster? To make it a little bit more realistic, I’ve written a separate variant of the benchmarks that hammers the GC really hard in a separate G:The result indicates that this is a worthwhile optimization for churn-heavy contexts. Performance is much worse overall, but that’s because the GC is pre-empting everyone. The improvement seems to be on the order of 20% for very small allocations.Before
After
Go does not offer an easy mechanism to “reallocate” an allocation, as with  in C. This is because it has no mechanism for freeing pointers explicitly, which is necessary for a reallocation abstraction.But we already don’t care about safety, so we can offer reallocation on our arena. Now, the reallocation we can offer is quite primitive: if a chunk happens to be the most recent one allocated, we can grow it. Otherwise we just allocate a new chunk and don’t free the old one.This makes it possible to implement “arena slices” that can be constructed by appending, which will not trigger reallocation on slice growth as long as nothing else gets put on the arena. would look something like this:Then, whenever we append to our arena slice, we can call  to grow it. However, this does not work if the slice’s base pointer is not the original address returned by  or . It is an exercise for the reader to:Implement a  type that uses an arena for allocation.Make this work for any value of  within the most recent allocation, not just the base offset. This requires extra book-keeping.Here is the entirety of the code that we have developed, not including the reallocation function above.There are other optimizations that we could make here that I haven’t discussed. For example, arenas could be re-used; once an arena is done, it could be “reset” and placed into a . This arena would not need to go into the GC to request new chunks, re-using the ones previously allocated (and potentially saving on the cost of zeroing memory over and over again).I did say that this relies very heavily on Go’s internal implementation details. Whats the odds that they get broken in the future? Well, the requirement that allocations know their shape is forced by the existence of , and the requirement that a pointer into any part of an allocation keeps the whole thing alive essentially comes from slices being both sliceable and mutable; once a slice escapes to the heap (and thus multiple goroutines) coordinating copies for shrinking a slice would require much more complexity than the current write barrier implementation.And in my opinion, it’s pretty safe to say that Hyrum’s Law has us covered here. ;)]]></content:encoded></item><item><title>Made a library with common 3D operations that is agnostic over the vector type</title><link>https://www.reddit.com/r/rust/comments/1k4pj9s/made_a_library_with_common_3d_operations_that_is/</link><author>/u/camilo16</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Mon, 21 Apr 2025 21:42:30 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[I made euclidean, a collection of functions for 3D euclidean geometry such as:Point to plane projection.Triangle box intersection.Segment-segment intersection.Shortest points between two lines.The main Point of the library is that it uses another crate of mine  to abstract over the underlying linear algebra type. It works directly with nalgebra, but it should work (with no need of additional work on the user end) with many other vector types, provided they implement sane traits, like indexing, iterating over the values, supporting addition and scalar multiplication...I hope this will be useful to some people.]]></content:encoded></item><item><title>Thoughts on Upwind alternative to Wiz?</title><link>https://www.reddit.com/r/kubernetes/comments/1k4p81b/thoughts_on_upwind_alternative_to_wiz/</link><author>/u/pxrage</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Mon, 21 Apr 2025 21:29:24 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[I'm contracting as a fCTO for enterprise health tech, wrapping up a project focused on optimizing their k8s monitoring costs. We are nearly done implementing and rolling out a new eBPF based solution to further cut cost.In the same time I'm tackling their security tooling related costs. They're currently heavily invested in AWS-native tools, and we're exploring alternatives that might offer better value. Potentially integrating more smoothly with our BYOC infra.I've already begun PoV using Upwind. Finished initial deep dive exploring their run-time powered cloud security stack and seems like it's the right fit for us. While not completely validated, I am impressed by the claim of reducing noise by up to 95% and the speed improvement up root cause analysis (via client case studies). Their use of eBPF for agentless sensors also resonates with our goal of maintaining efficiency.Before we dive deeper, I wanted to tap into the community's collective wisdom:"Runtime-powered" reality check: For those who have experience, how well does the "runtime-powered" aspect deliver in practice? Does it truly leverage runtime context effectively to prioritize real threats and reduce alert fatigue compared to more traditional CNAPP solutions or native cloud provider tools? How seamless is the integration of its CSPM, CWPP, Vulnerability Management, etc., under this runtime umbrella?eBPF monitoring and security in one: we've already invested in building out an eBPF-based o11y stack. Has anyone successfully leveraged eBPF for both monitoring/observability and security within the same k8s environment? Are there tangible synergies (performance benefits, reduced overhead, unified data plane) or is it more practical to keep these stacks separate, even if both utilize eBPF? Does using eBPF security stack alongside an existing eBPF monitoring solution create conflicts or complexities?Lastly, we're still early in the discovery phase that I'm allowed to look beyond one single security provider. Are there other runtime-focused security platforms (especially those leveraging eBPF) that you've found particularly effective in complex K8s environments, specifically when cost optimization and reducing tool sprawl are key drivers?Appreciate any insights, thanks!]]></content:encoded></item><item><title>Wine 10.6 Released</title><link>https://tech.slashdot.org/story/25/04/21/2027253/wine-106-released?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>dev</category><category>slashdot</category><pubDate>Mon, 21 Apr 2025 20:57:00 +0000</pubDate><source url="https://linux.slashdot.org/">Dev - Slashdot - Linux</source><content:encoded><![CDATA[Wine 10.6 has been released, featuring a new lexer within its Command Processor (CMD), support for the PBKDF2 algorithm to its Bcrypt implementation, and improved metadata handling in WindowsCodecs. According to Phoronix, the update also includes 27 known bug fixes that address issues with Unity games, Alan Wake, GDI+, and various other games and applications.
 
You can see all the changes and download the relesae via WineHQ.org GitLab.]]></content:encoded></item><item><title>Visiting Us</title><link>https://www.epic.com/visiting/</link><author>tobr</author><category>dev</category><category>hn</category><pubDate>Mon, 21 Apr 2025 20:49:20 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Sat, Apr 26—Fri, May 09, 2025Fri, May 16—Sun, May 18, 2025Sat, Aug 09—Fri, Aug 22, 2025Wed, Dec 24—Thu, Dec 25, 2025]]></content:encoded></item><item><title>Supercharge Your Go Tests Using Fake HTTP Services</title><link>https://tutorialedge.net/golang/testing-with-fake-http-services-in-go/</link><author>/u/elliotforbes</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Mon, 21 Apr 2025 20:48:32 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Why Testing With Fakes Is ImportantTesting is a critical part of building reliable and maintainable Go applications. When your code interacts with external HTTP services, relying on real HTTP requests during tests can lead to flaky and inconsistent results. Network issues, rate limits, or changes in external APIs can all cause your tests to fail unpredictably. By using fake HTTP services, you can simulate real-world scenarios in a controlled environment, ensuring your tests are fast, reliable, and repeatable. In this article, we’ll explore how to set up fake HTTP services in Go to improve the confidence you have in your systems and streamline your testing process.Configurable HTTP Clients with Environment VariablesIn order for this approach to work however, you do need to ensure that you build your systems in such a way that it’s easy to specify where the clients are ultimately sending your HTTP requests to. One effective approach is to use environment variables to configure the base URLs your clients interact with. This ensures that your application can seamlessly switch between production, staging, and testing environments without requiring code changes.Another approach is to have a sensible default base URL specified as a constant within the client code. You could then leverage something like the 
  functional options parameter pattern to allow for the overriding of this value.The important thing is that you have some level of control over this value within your client. This allows you to leverage the concept of fakes far easier in your test fixtures.Here’s an example of how you can achieve this:Benefits of This ApproachEnvironment-Specific Configuration: By using environment variables, you can easily configure your application to use different base URLs for production, staging, or local testing environments.: During testing, you can set the  to point to a fake HTTP service or mock server, allowing you to simulate various scenarios without relying on external services.: Sensitive URLs or credentials can be managed securely using environment variables, reducing the risk of hardcoding them into your source code.When writing tests, you can override the  environment variable to point to your fake HTTP service:By designing your HTTP clients with configurability in mind, you can ensure that your application is both flexible and testable, leading to more robust and maintainable code.Writing HTTP Tests Using Now, traditionally, a Go developer would lean on the 
  net/http/httptest package for creating and managing test HTTP servers. These servers allow you to simulate HTTP responses and test how your application interacts with external services in a controlled environment.Example: How to test Go Clients with httptestSuppose you have an HTTP client that fetches data from an external API. You can use  to simulate the API’s behavior and test your client’s functionality.Whilst this approach works, I think it could be improved upon to provide a better experience for developers maintaining and extending these tests.Over the past year, I’ve been working through improving the developer experience of writing a suite of fairly comprehensive and complex acceptance tests that exercise the integrations.From my totally un-biased perspective, this strikes me as a far easier setup of a fake that allows me to really focus my attention on the logic contained within my clients as opposed to worry too much about the fake setup itself.Let’s throw this into our earlier example to see how this plays out:This sets up a series of sensible defaults that you’d typically expect of an API, like setting the  to . It also does some basic assertions that these endpoints defined on the fake have actually been called at least once.For those of you that require a stronger suite of assertions on both how your client methods handle the responses, as well as guaranteeing that you’ve sent the correct request shape to the downstream API. You can take advantage of the  field when registering an  struct on the downstream API.As a general rule, I’d highly recommend fleshing out assertions that validate you are indeed sending the right shape of data that a downstream API expects. Perhaps validating the headers being sent with the request may be a good idea.Simulating Errors in your FakesIt’d be lovely if we only ever had to worry about happy-path testing and could disregard any potential error cases or sad paths. Our software systems would be much simpler and take far less time to build. However, as we live in the real world, we know that this isn’t the case and our jobs as engineers is to ensure we’re able to handle the inevitable failures.It’s important that regardless of what testing approach you leverage, you should be writing tests that ensure you are indeed exercising these sad-path cases.The fakes package enables us to simulate failure cases within our tests with ease:Now, when our test goes to hit our downstream API, it’ll be greeted with a lovely 404 status code and an error message that will allow us to exercise our code’s ability to handle failure gracefully.In this article, we’ve explored how to set up tests for Go applications using fake HTTP services, leveraging both the  package and the  library. By adopting these techniques, you can create robust, reliable, and maintainable tests that simulate real-world scenarios while avoiding the pitfalls of flaky external dependencies.The  library, in particular, aims to simplify the process of creating and managing fake HTTP services, providing a more developer-friendly experience. Whether you’re testing happy paths or simulating error cases, this library can help streamline your testing workflow and improve the overall quality of your codebase.If you found this article helpful or have feedback on the  library, I’d love to hear from you! Feel free to reach out or contribute to the project on 
  GitHub. Your input is invaluable in making this tool even better for the Go community.]]></content:encoded></item><item><title>faer: efficient linear algebra library for rust - 0.22 release</title><link>https://github.com/sarah-quinones/faer-rs/</link><author>/u/reflexpr-sarah-</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Mon, 21 Apr 2025 19:35:47 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>A M.2 HDMI capture card</title><link>https://interfacinglinux.com/2025/04/18/magewell-eco-m-2-hdmi-capture-card/</link><author>Venn1</author><category>dev</category><category>hn</category><pubDate>Mon, 21 Apr 2025 19:01:55 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Astronomers confirm the existence of a lone black hole</title><link>https://phys.org/news/2025-04-astronomers-lone-black-hole.html</link><author>wglb</author><category>dev</category><category>hn</category><pubDate>Mon, 21 Apr 2025 18:36:23 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Blog hosted on a Nintendo Wii</title><link>https://blog.infected.systems/posts/2025-04-21-this-blog-is-hosted-on-a-nintendo-wii/</link><author>edent</author><category>dev</category><category>hn</category><pubDate>Mon, 21 Apr 2025 18:29:04 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[If you are reading this message, the experiment below is still ongoing. This page was served to you by a real Nintendo Wii.For a long time, I’ve enjoyed the idea of running general-purpose operating systems on decidedly not-general-purpose hardware.There’s been a few good examples of this over the years, including a few which were officially sanctioned by the OEM. Back in the day, my PS3 ran Yellow Dog Linux, and I’ve been searching for a (decently priced) copy of PS2 Linux for 10+ years at this point.But what a lot of these systems have in common is that they’re now very outdated. Or they’re hobbyist ports that someone got running once and where longer-term support never made it upstream. The PSP Linux kernel image was last built in 2008, and Dreamcast Linux is even more retro, using a 2.4.5 kernel built in 2001.I haven’t seen many of these projects where I’d be comfortable running one as part of an actual production workload. Until now.While browsing the NetBSD website recently, I noticed the fact that there was a ‘Wii’ option listed right there on the front page in the ‘Install Media’ section, nestled right next to the other first-class targets like the Raspberry Pi, and generic x86 machines.Unlike the other outdated and unmaintained examples above, clicking through to the NetBSD Wii port takes you to the latest stable NetBSD 10.1 release from Dec 2024. Even the daily HEAD builds are composed for the Wii.As soon as I discovered this was fully supported and maintained, I knew I had to try deploying an actual production workload on it. That workload is the blog you’re reading now.Finding a sacrificial WiiOur story begins at the EMF Camp 2024 Swap Shop - your premier source for pre-loved game consoles, cardboard boxes full of 56k modems, and radioactive orphan sources.I picked this up expecting to use it for homebrew games and emulation mostly, but I don’t think it expected this fate.So we have a spare Wii. And an OS with mainline support. But is a Wii actually fast enough to handle this as a production workload?The single-core ‘Broadway’ CPU in the Wii is part of IBM’s continued evolution of the PowerPC 750 lineup, dating all the way back to Apple’s iconic 1998 Bondi Blue fishtank iMac. Although Broadway is one of the later 750 revisions, the commercially-available equivalent chip - the PowerPC 750CL - has a maximum TDP of only 9.8 W, and clocks about 33% higher than the version in the Wii.So with a single-core chip based on a late-90s architecture and a TDP well under 10 W, it’s clear that we’re probably fairly contstrained here in terms of compute performance.With that said, one of the other PowerPC 750 deployments you might be familiar with is currently floating 1,500,000 km from Earth mapping the deepest reaches of the universe in more detail than humanity has ever seen before. So if I can’t get this thing serving a static website, then I think it’s probably time to execute on my long-term plan of retiring from tech and opening a cat cafe.On a more serious note, you can read about the James Wii Space Telescope’s use of the PowerPC 750 in this NASA presentation. The 750 actually gets a lot of use in spaceflight and satellite applications because there is a radiation-hardened version available, known as the RAD750. Some other recent uses of the chip include both the Mars Curiosity and Perseverance rovers.Installing NetBSD on the WiiOkay, Nintendo lawyers avert your eyes.It had been a long time since I softmodded a Wii. I remember the Twilight Hack, which involved exploiting a buffer overflow in the  save game handler to run unsigned code.Things are much easier these days. The Wilbrand exploit seems to be what people generally recommend now. Like some other exploits, it takes advantage of the fact that an SD card can be used to store and retrieve messages from the Wii Message Board. Exploting this allows unsigned code execution, which allows us to boot the HackMii tool that installs the Homebrew Channel.It’s an easy mod which just requires knowing the MAC address of the console and generating a few files to load from an SD card. There’s a handy browser-based tool here which does all of the hard work for you.I did have some issues using a larger SDHC card to run the Wilbrand exploit, but had the best luck with a 1GB non-SDHC card. SD card compatibility seems to be a known issue for Wii homebrew, but overall I’d still call the process fairly painless.Once we’ve hacked the console, we should have the Homebrew Channel available in our Wii Menu:For this card, I opted to use a fairly speedy 32GB SDHC card. The Wii doesn’t support SDXC or newer cards, which means we’re limited to 32GB. Larger flash devices also  tend to be faster and more resilient than smaller ones. And NetBSD seems a lot less bothered by living on a larger card than the Wilbrand exploit was. So overall I’d recommend getting a decent quality fast 32GB card if you want to try this.We can unpack and write this image however we please, but I’m a fan of using the Raspberry Pi Imager because it’ll do the work of extracting the image and verifying it post-write for us:At this point, things are very easy. The NetBSD Wii image has all the necessary metadata & structure needed to boot directly from the Homebrew Channel as if it were any other kind of homebrew app. I think there’s a lot of credit due here to NetBSD developer Jared McNeill, who seems to be the main author of the Wii port.Placing our SD card in the console and launching the Homebrew Channel is all we need to do to prepare ourselves to launch NetBSD:Once booted into NetBSD, we can use a USB keyboard just fine, but it will be easiest to get SSH working so we can manage the system remotely. The SSH daemon is already running out-of-the-box, so the only changes we need to make are to set a password for the  user and then enable logging in as root by adding  to the .You could set up an unprivileged user or do anything else you fancy here but I was keen to get SSH going ASAP, as due to  laziness I was doing this bit using a capture card and Photo Booth on macOS which doesn’t actually support disabling the image-flip on the video feed:If you thought it was hard to exit Vim, try doing it back to front.After installation, I set a static network config by editing  and restarted the host.On that note, I’m using the official RVL-015 Wii LAN Adapter. I went to great lengths to track down one of these for a decent price for the best chance of compatibility. On reflection, this probably wasn’t needed as by the time we’re booted into NetBSD we should have all of NetBSD’s drivers available to us, so I expect most generic USB adapters would probably work (in theory!).If anyone is wondering though, here are the specifics of the adapter and chipset, taken from :After restarting, I installed NetBSD’s  package manager by setting some env vars and then using :After this, I was able to use pkgin to install a bunch of useful packages - most importantly including our  web server, which I’ve picked due to it being slim and well-suited for resource constrained environments:After that, I copied the basic  sample config, and enabled and started it:By default,  is set up to serve static content from . Since my blog is a collection of static pages built with Hugo, I was able to simply  these files over and within seconds I had the system serving my site over standard HTTP.Is it fast enough? (addendum)Alright, you got me. It turns out that while a PPC 750 might be enough to map the futhest reaches of the universe, a bit of soak testing suggests it does struggle a bit when trying to concurrently serve a lot of pages encrypted with modern TLS.I tried freeing up resources by disabling a bunch of services I don’t need that are running out of the box on NetBSD:I also disabled , which was using a staggering 15% of the whole system’s RAM:Unfortunately it seems  is definitely necessary to keep the system clock in-check. I’m not sure whether the Wii just drifts a lot, whether it’s specific to NetBSD on the Wii, or whether the clock battery in what may be a nearly 20 year old console has given up but I got some interesting and quite indignant sounding error messages after disabling this:To compensate, I cheated by adding  to the main , so the system would run it hourly at  min past the hour. We still get timesync, but we don’t need to sacrifice a sixth of our RAM keeping it resident:Even after freeing up the resources above, it seems like serving multiple encrypted requests in parallel was a struggle for the 750, so I opted to move the TLS termination for the blog to a Caddy instance sitting in front of the Wii.I have Caddy acting as a reverse proxy to the Wii, handling encryption and cert management with ACME. Importantly, there are no caching options enabled in Caddy. Every request the site serves is being serviced directly by the Wii - including the large number of images on this post which I’ll almost certainly regret adding. I optimised as much as I could, but this page is still almost exactly 1 MB when all of the content is loaded.Through this method, I’ve also been able to sinkhole LLM slop-scrapers at Caddy’s level, by dropping requests from known scraper User Agents before they’re forwarded to the Wii. Hopefully that might help to keep our single core chugging along without too much distress.Moving the SSL termination to Caddy also gives me the advantage of enabling Caddy’s Prometheus exporter, so I can load it into my InfluxDB + Grafana stack and monitor site load without putting a bunch of additional stress on the Wii.But I’m still interested (and I’m sure you are too) in monitoring the general state of the Wii resources as this post goes live.Considering I had to disable the NTP client for using too much RAM, I think running something like a Prometheus exporter directly on the Wii is right out of the window to begin with. So I put together a simple shell script that runs from the  every 15 min, outputting some system stats to a basic HTML file in the webroot.Honestly, this worked way better and was far easier than I was expecting. Naturally, there are some downsides to the setup here though.Rebooting NetBSD reboots the whole console, and not just the NetBSD ‘app’, so you’ll find yourself back at the Wii Menu after any kernel patch or system upgrade. Yes, this does mean that the Wiimote and sensor bar in your server cupboard are now a vital component of the production infrastructure.I was reasonably pleased with the power consumption too. Some testing based on stats from my UPS monitoring suggest that when idling, the Wii is adding a fairly consistent ~18 W to my overall homelab usage.By my calculations, that means I can expect the Wii to use ~13.2 kWh/month, which on my fairly expensive UK power tariff comes out to around £3.47/month - which does actually make this cheaper than most of the VPSes I can find around the obvious cloud providers. So when you’re looking for your next VPS… you know what to consider.This was a fun experiment for a rainy day over a long weekend. I’ll probably keep it going for a while if it actually continues to work as well as it started. I’m often interested in applying artificial constraints to the things I deploy, as I find that’s when I learn best.Who knows, maybe I’ll have been forced to become an expert in NetBSD TCP kernel tunables by this time next week…]]></content:encoded></item><item><title>Show HN: Open Codex – OpenAI Codex CLI with open-source LLMs</title><link>https://github.com/codingmoh/open-codex</link><author>codingmoh</author><category>dev</category><category>hn</category><pubDate>Mon, 21 Apr 2025 17:57:03 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[I’ve built Open Codex, a fully local, open-source alternative to OpenAI’s Codex CLI.My initial plan was to fork their project and extend it. I even started doing that. But it turned out their code has several leaky abstractions, which made it hard to override core behavior cleanly. Shortly after, OpenAI introduced breaking changes. Maintaining my customizations on top became increasingly difficult.So I rewrote the whole thing from scratch using Python. My version is designed to support local LLMs.Right now, it only works with phi-4-mini (GGUF) via lmstudio-community/Phi-4-mini-instruct-GGUF, but I plan to support more models. Everything is structured to be extendable.At the moment I only support single-shot mode, but I intend to add interactive (chat mode), function calling, and more.You can install it using Homebrew:   brew tap codingmoh/open-codex
   brew install open-codex


It's also published on PyPI:
Source: https://github.com/codingmoh/open-codex]]></content:encoded></item><item><title>I built a manga translator tool using Tauri, ONNX runtime, and candle</title><link>https://www.reddit.com/r/rust/comments/1k4jdc9/i_built_a_manga_translator_tool_using_tauri_onnx/</link><author>/u/mayocream39</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Mon, 21 Apr 2025 17:34:58 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[The application is built with Tauri, and Koharu uses a combination of object detection and a transformer-based OCR.For translation, Koharu uses an OpenAI-compatible API to chat and obtain the translation result. For more details about the tech, read the README at https://github.com/mayocream/koharuI plan to add segment and inpaint features to Koharu...I learn Rust for 3 months, and it's my first Rust-written application!]]></content:encoded></item><item><title>Task v3.43 is released! 🤩</title><link>https://github.com/go-task/task/releases/tag/v3.43.1</link><author>/u/andrey-nering</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Mon, 21 Apr 2025 17:28:06 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>On the cruelty of really teaching computing science (1988)</title><link>https://www.cs.utexas.edu/~EWD/transcriptions/EWD10xx/EWD1036.html</link><author>/u/ketralnis</author><category>dev</category><category>reddit</category><pubDate>Mon, 21 Apr 2025 17:27:52 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[On the cruelty of really teaching computing scienceThe second part of this talk pursues some of the scientific and educational consequences of the assumption that computers represent a radical novelty. In order to give this assumption clear contents, we have to be much more precise as to what we mean in this context by the adjective "radical". We shall do so in the first part of this talk, in which we shall furthermore supply evidence in support of our assumption.The usual way in which we plan today for tomorrow is in yesterday's vocabulary. We do so, because we try to get away with the concepts we are familiar with and that have acquired their meanings in our past experience. Of course, the words and the concepts don't quite fit because our future differs from our past, but then we stretch them a little bit. Linguists are quite familiar with the phenomenon that the meanings of words evolve over time, but also know that this is a slow and gradual process.It is the most common way of trying to cope with novelty: by means of metaphors and analogies we try to link the new to the old, the novel to the familiar. Under sufficiently slow and gradual change, it works reasonably well; in the case of a sharp discontinuity, however, the method breaks down: though we may glorify it with the name "common sense", our past experience is no longer relevant, the analogies become too shallow, and the metaphors become more misleading than illuminating. This is the situation that is characteristic for the "radical" novelty.Coping with radical novelty requires an orthogonal method. One must consider one's own past, the experiences collected, and the habits formed in it as an unfortunate accident of history, and one has to approach the radical novelty with a blank mind, consciously refusing to try to link it with what is already familiar, because the familiar is hopelessly inadequate. One has, with initially a kind of split personality, to come to grips with a radical novelty as a dissociated topic in its own right. Coming to grips with a radical novelty amounts to creating and learning a new foreign language that can  be translated into one's mother tongue. (Any one who has learned quantum mechanics knows what I am talking about.) Needless to say, adjusting to radical novelties is not a very popular activity, for it requires hard work. For the same reason, the radical novelties themselves are unwelcome.By now, you may well ask why I have paid so much attention to and have spent so much eloquence on such a simple and obvious notion as the radical novelty. My reason is very simple: radical novelties are so disturbing that they tend to be suppressed or ignored, to the extent that even the possibility of their existence in general is more often denied than admitted.On the historical evidence I shall be short. Carl Friedrich Gauss, the Prince of Mathematicians but also somewhat of a coward, was certainly aware of the fate of Galileo —and could probably have predicted the calumniation of Einstein— when he decided to suppress his discovery of non-Euclidean geometry, thus leaving it to Bolyai and Lobatchewsky to receive the flak. It is probably more illuminating to go a little bit further back, to the Middle Ages. One of its characteristics was that "reasoning by analogy" was rampant; another characteristic was almost total intellectual stagnation, and we now see why the two go together. A reason for mentioning this is to point out that, by developing a keen ear for unwarranted analogies, one can detect a lot of medieval thinking today.The other thing I can not stress enough is that the fraction of the population for which gradual change seems to be all but the only paradigm of history is very large, probably much larger than you would expect. Certainly when I started to observe it, their number turned out to be much larger than I had expected.For instance, the vast majority of the mathematical community has never challenged its tacit assumption that doing mathematics will remain very much the same type of mental activity it has always been: new topics will come, flourish, and go as they have done in the past, but, the human brain being what it is, our ways of teaching, learning, and understanding mathematics, of problem solving, and of mathematical discovery will remain pretty much the same. Herbert Robbins clearly states why he rules out a quantum leap in mathematical ability:
					"Nobody is going to run 100 meters in five seconds, no matter how much is invested in training and machines. The same can be said about using the brain. The human mind is no different now from what it was five thousand years ago. And when it comes to mathematics, you must realize that this is the human mind at an extreme limit of its capacity."My comment in the margin was "so reduce the use of the brain and calculate!". Using Robbins's own analogy, one could remark that, for going from A to B fast, there could now exist alternatives to running that are orders of magnitude more effective. Robbins flatly refuses to honour any alternative to time-honoured brain usage with the name of "doing mathematics", thus exorcizing the danger of radical novelty by the simple device of adjusting his definitions to his needs: simply by definition, mathematics will continue to be what it used to be. So much for the mathematicians.Let me give you just one more example of the widespread disbelief in the existence of radical novelties and, hence, in the need of learning how to cope with them. It is the prevailing educational practice, for which gradual, almost imperceptible, change seems to be the exclusive paradigm. How many educational texts are not recommended for their appeal to the student's intuition! They constantly try to present everything that could be an exciting novelty as something as familiar as possible. They consciously try to link the new material to what is supposed to be the student's familiar world. It already starts with the teaching of arithmetic. Instead of teaching 2 + 3 = 5 , the hideous arithmetic operator "plus" is carefully disguised by calling it "and", and the little kids are given lots of familiar examples first, with clearly visible such as apples and pears, which are , in contrast to equally countable objects such as percentages and electrons, which are . The same silly tradition is reflected at university level in different introductory calculus courses for the future physicist, architect, or business major, each adorned with examples from the respective fields. The educational dogma seems to be that everything is fine as long as the student does not notice that he is learning something really new; more often than not, the student's impression is indeed correct. I consider the failure of an educational practice to prepare the next generation for the phenomenon of radical novelties a serious shortcoming. [When King Ferdinand visited the conservative university of Cervera, the Rector proudly reassured the monarch with the words; "Far be from us, Sire, the dangerous novelty of thinking.". Spain's problems in the century that followed justify my characterization of the shortcoming as "serious".] So much for education's adoption of the paradigm of gradual change.The concept of radical novelties is of contemporary significance because, while we are ill-prepared to cope with them, science and technology have now shown themselves expert at inflicting them upon us. Earlier scientific examples are the theory of relativity and quantum mechanics; later technological examples are the atom bomb and the pill. For decades, the former two gave rise to a torrent of religious, philosophical, or otherwise quasi-scientific tracts. We can daily observe the profound inadequacy with which the latter two are approached, be it by our statesmen and religious leaders or by the public at large. So much for the damage done to our peace of mind by radical novelties.I raised all this because of my contention that automatic computers represent a radical novelty and that only by identifying them as such can we identify all the nonsense, the misconceptions and the mythology that surround them. Closer inspection will reveal that it is even worse, viz. that automatic computers embody not only one radical novelty but two of them.The first radical novelty is a direct consequence of the raw power of today's computing equipment. We all know how we cope with something big and complex; divide and rule, i.e. we view the whole as a compositum of parts and deal with the parts separately. And if a part is too big, we repeat the procedure. The town is made up from neighbourhoods, which are structured by streets, which contain buildings, which are made from walls and floors, that are built from bricks, etc. eventually down to the elementary particles. And we have all our specialists along the line, from the town planner, via the architect to the solid state physicist and further. Because, in a sense, the whole is "bigger" than its parts, the depth of a hierarchical decomposition is some sort of logarithm of the ratio of the "sizes" of the whole and the ultimate smallest parts. From a bit to a few hundred megabytes, from a microsecond to a half an hour of computing confronts us with completely baffling ratio of 10! The programmer is in the unique position that his is the only discipline and profession in which such a gigantic ratio, which totally baffles our imagination, has to be bridged by a single technology. He has to be able to think in terms of conceptual hierarchies that are much deeper than a single mind ever needed to face before. Compared to that number of semantic levels, the average mathematical theory is almost flat. By evoking the need for deep conceptual hierarchies, the automatic computer confronts us with a radically new intellectual challenge that has no precedent in our history.Again, I have to stress this radical novelty because the true believer in gradual change and incremental improvements is unable to see it. For him, an automatic computer is something like the familiar cash register, only somewhat bigger, faster, and more flexible. But the analogy is ridiculously shallow: it is orders of magnitude worse than comparing, as a means of transportation, the supersonic jet plane with a crawling baby, for that speed ratio is only a thousand.The second radical novelty is that the automatic computer is our first large-scale digital device. We had a few with a noticeable discrete component: I just mentioned the cash register and can add the typewriter with its individual keys: with a single stroke you can type either a Q or a W but, though their keys are next to each other, not a mixture of those two letters. But such mechanisms are the exception, and the vast majority of our mechanisms are viewed as analogue devices whose behaviour is over a large range a continuous function of all parameters involved: if we press the point of the pencil a little bit harder, we get a slightly thicker line, if the violinist slightly misplaces his finger, he plays slightly out of tune. To this I should add that, to the extent that we view ourselves as mechanisms, we view ourselves primarily as analogue devices: if we push a little harder we expect to do a little better. Very often the behaviour is not only a continuous but even a monotonic function: to test whether a hammer suits us over a certain range of nails, we try it out on the smallest and largest nails of the range, and if the outcomes of those two experiments are positive, we are perfectly willing to believe that the hammer will suit us for all nails in between.It is possible, and even tempting, to view a program as an abstract mechanism, as a device of some sort. To do so, however, is highly dangerous: the analogy is too shallow because a program is, as a mechanism, totally different from all the familiar analogue devices we grew up with. Like all digitally encoded information, it has unavoidably the uncomfortable property that the smallest possible perturbations —i.e. changes of a single bit— can have the most drastic consequences. [For the sake of completness I add that the picture is not essentially changed by the introduction of redundancy or error correction.] In the discrete world of computing, there is no meaningful metric in which "small" changes and "small" effects go hand in hand, and there never will be.This second radical novelty shares the usual fate of all radical novelties: it is denied, because its truth would be too discomforting. I have no idea what this specific denial and disbelief costs the United States, but a million dollars a day seems a modest guess.Having described —admittedly in the broadest possible terms— the nature of computing's novelties, I shall now provide the evidence that these novelties are, indeed, radical. I shall do so by explaining a number of otherwise strange phenomena as frantic —but, as we now know, doomed— efforts at hiding or denying the frighteningly unfamiliar.A number of these phenomena have been bundled under the name "Software Engineering". As economics is known as "The Miserable Science", software engineering should be known as "The Doomed Discipline", doomed because it cannot even approach its goal since its goal is self-contradictory. Software engineering, of course, presents itself as another worthy cause, but that is eyewash: if you carefully read its literature and analyse what its devotees actually do, you will discover that software engineering has accepted as its charter "How to program if you cannot.".The popularity of its name is enough to make it suspect. In what we denote as "primitive societies", the superstition that knowing someone's true name gives you magic power over him is not unusual. We are hardly less primitive: why do we persist here in answering the telephone with the most unhelpful "hello" instead of our name?Nor are we above the equally primitive superstition that we can gain some control over some unknown, malicious demon by calling it by a safe, familiar, and innocent name, such as "engineering". But it is totally symbolic, as one of the US computer manufacturers proved a few years ago when it hired, one night, hundreds of new "software engineers" by the simple device of elevating all its programmers to that exalting rank. So much for that term.The practice is pervaded by the reassuring illusion that programs are just devices like any others, the only difference admitted being that their manufacture might require a new type of craftsmen, viz. programmers. From there it is only a small step to measuring "programmer productivity" in terms of "number of lines of code produced per month". This is a very costly measuring unit because it encourages the writing of insipid code, but today I am less interested in how foolish a unit it is from even a pure business point of view. My point today is that, if we wish to count lines of code, we should not regard them as "lines produced" but as "lines spent": the current conventional wisdom is so foolish as to book that count on the wrong side of the ledger.Besides the notion of productivity, also that of quality control continues to be distorted by the reassuring illusion that what works with other devices works with programs as well. It is now two decades since it was pointed out that program testing may convincingly demonstrate the presence of bugs, but can never demonstrate their absence. After quoting this well-publicized remark devoutly, the software engineer returns to the order of the day and continues to refine his testing strategies, just like the alchemist of yore, who continued to refine his chrysocosmic purifications.Unfathomed misunderstanding is further revealed by the term "software maintenance", as a result of which many people continue to believe that programs —and even programming languages themselves— are subject to wear and tear. Your car needs maintenance too, doesn't it? Famous is the story of the oil company that believed that its PASCAL programs did not last as long as its FORTRAN programs "because PASCAL was not maintained".In the same vein I must draw attention to the astonishing readiness with which the suggestion has been accepted that the pains of software production are largely due to a lack of appropriate "programming tools". (The telling "programmer's workbench" was soon to follow.) Again, the shallowness of the underlying analogy is worthy of the Middle Ages. Confrontations with insipid "tools" of the "algorithm-animation" variety has not mellowed my judgement; on the contrary, it has confirmed my initial suspicion that we are primarily dealing with yet another dimension of the snake oil business.Finally, to correct the possible impression that the inability to face radical novelty is confined to the industrial world, let me offer you an explanation of the —at least American— popularity of Artificial Intelligence. One would expect people to feel threatened by the "giant brains or machines that think". In fact, the frightening computer becomes less frightening if it is used only to simulate a familiar noncomputer. I am sure that this explanation will remain controversial for quite some time, for Artificial Intelligence as mimicking the human mind prefers to view itself as at the front line, whereas my explanation relegates it to the rearguard. (The effort of using machines to mimic the human mind has always struck me as rather silly: I'd rather use them to mimic something better.)So much for the evidence that the computer's novelties are, indeed, radical.And now comes the second —and hardest— part of my talk: the scientific and educational consequences of the above. The educational consequences are, of course, the hairier ones, so let's postpone their discussion and stay for a while with computing science itself. What is computing? And what is a science of computing about?Well, when all is said and done, the only thing computers can do for us is to manipulate symbols and produce results of such manipulations. From our previous observations we should recall that this is a discrete world and, moreover, that both the number of symbols involved and the amount of manipulation performed are many orders of magnitude larger than we can envisage: they totally baffle our imagination and we must therefore not try to imagine them.But before a computer is ready to perform a class of meaningful manipulations —or calculations, if you prefer— we must write a program. What is a program? Several answers are possible. We can view the program as what turns the general-purpose computer into a special-purpose symbol manipulator, and does so without the need to change a single wire (This was an enormous improvement over machines with problem-dependent wiring panels.) I prefer to describe it the other way round: the program is an abstract symbol manipulator, which can be turned into a concrete one by supplying a computer to it. After all, it is no longer the purpose of programs to instruct our machines; these days, it is the purpose of machines to execute our programs.So, we have to design abstract symbol manipulators. We all know what they look like: they look like programs or —to use somewhat more general terminology— usually rather elaborate formulae from some formal system. It really helps to view a program as a formula. Firstly, it puts the programmer's task in the proper perspective: he has to derive that formula. Secondly, it explains why the world of mathematics all but ignored the programming challenge: programs were so much longer formulae than it was used to that it did not even recognize them as such. Now back to the programmer's job: he has to derive that formula, he has to derive that program. We know of only one reliable way of doing that, viz. by means of symbol manipulation. And now the circle is closed: we construct our mechanical symbol manipulators by means of human symbol manipulation.Hence, computing science is —and will always be— concerned with the interplay between mechanized and human symbol manipulation, usually referred to as "computing" and "programming" respectively. An immediate benefit of this insight is that it reveals "automatic programming" as a contradiction in terms. A further benefit is that it gives us a clear indication where to locate computing science on the world map of intellectual disciplines: in the direction of formal mathematics and applied logic, but ultimately far beyond where those are now, for computing science is interested in  use of formal methods and on a much, much, larger scale than we have witnessed so far. Because no endeavour is respectable these days without a TLA (= Three-Letter Acronym), I propose that we adopt for computing science FMI (= Formal Methods Initiative), and, to be on the safe side, we had better follow the shining examples of our leaders and make a Trade Mark of it.In the long run I expect computing science to transcend its parent disciplines, mathematics and logic, by effectively realizing a significant part of Leibniz's Dream of providing symbolic calculation as an alternative to human reasoning. (Please note the difference between "mimicking" and "providing an alternative to": alternatives are allowed to be better.)Needless to say, this vision of what computing science is about is not universally applauded. On the contrary, it has met widespread —and sometimes even violent— opposition from all sorts of directions. I mention as examples(0) the mathematical guild, which would rather continue to believe that the Dream of Leibniz is an unrealistic illusion(1) the business community, which, having been sold to the idea that computers would make life easier, is mentally unprepared to accept that they only solve the easier problems at the price of creating much harder ones(2) the subculture of the compulsive programmer, whose ethics prescribe that one silly idea and a month of frantic coding should suffice to make him a life-long millionaire(3) computer engineering, which would rather continue to act as if it is all only a matter of higher bit rates and more flops per second(4) the military, who are now totally absorbed in the business of using computers to mutate billion-dollar budgets into the illusion of automatic safety(5) all soft sciences for which computing now acts as some sort of interdisciplinary haven(6) the educational business that feels that, if it has to teach formal mathematics to CS students, it may as well close its schools.And with this sixth example I have reached, imperceptibly but also alas unavoidably, the most hairy part of this talk: educational consequences.The problem with educational policy is that it is hardly influenced by scientific considerations derived from the topics taught, and almost entirely determined by extra-scientific circumstances such as the combined expectations of the students, their parents and their future employers, and the prevailing view of the role of the university: is the stress on training its graduates for today's entry-level jobs or to providing its alumni with the intellectual bagage and attitudes that will last them another 50 years? Do we grudgingly grant the abstract sciences only a far-away corner on campus, or do we recognize them as the indispensable motor of the high-technology industry? Even if we do the latter, do we recognize a high-technology industry as such if its technology primarily belongs to formal mathematics? Do the universities provide for society the intellectual leadership it needs or only the training it asks for?Traditional academic rhetoric is perfectly willing to give to these questions the reassuring answers, but I don't believe them. By way of illustration of my doubts, in a recent article on "Who Rules Canada?", David H. Flaherty bluntly states "Moreover, the business elite dismisses traditional academics and intellectuals as largely irrelevant and powerless.".So, if I look into my foggy crystal ball at the future of computing science education, I overwhelmingly see the depressing picture of "Business as usual". The universities will continue to lack the courage to teach hard science, they will continue to misguide the students, and each next stage of infantilization of the curriculum will be hailed as educational progress.I now have had my foggy crystal ball for quite a long time. Its predictions are invariably gloomy and usually correct, but I am quite used to that and they won't keep me from giving you a few suggestions, even if it is merely an exercise in futility whose only effect is to make you feel guilty.We could, for instance, begin with cleaning up our language by no longer calling a bug a bug but by calling it an error. It is much more honest because it squarely puts the blame where it belongs, viz. with the programmer who made the error. The animistic metaphor of the bug that maliciously sneaked in while the programmer was not looking is intellectually dishonest as it disguises that the error is the programmer's own creation. The nice thing of this simple change of vocabulary is that it has such a profound effect: while, before, a program with only one bug used to be "almost correct", afterwards a program with an error is just "wrong" (because in error).My next linguistical suggestion is more rigorous. It is to fight the "if-this-guy-wants-to-talk-to-that-guy" syndrome:  refer to parts of programs or pieces of equipment in an anthropomorphic terminology, nor allow your students to do so. This linguistical improvement is much harder to implement than you might think, and your department might consider the introduction of fines for violations, say a quarter for undergraduates, two quarters for graduate students, and five dollars for faculty members: by the end of the first semester of the new regime, you will have collected enough money for two scholarships.The reason for this last suggestion is that the anthropomorphic metaphor —for whose introduction we can blame John von Neumann— is an enormous handicap for every computing community that has adopted it. I have now encountered programs wanting things, knowing things, expecting things, believing things, etc., and each time that gave rise to avoidable confusions. The analogy that underlies this personification is so shallow that it is not only misleading but also paralyzing.It is misleading in the sense that it suggests that we can adequately cope with the unfamiliar discrete in terms of the familiar continuous, i.e. ourselves, quod non. It is paralyzing in the sense that, because persons exist and act , its adoption effectively prevents a departure from operational semantics and thus forces people to think about programs in terms of computational behaviours, based on an underlying computational model. This is bad, because operational reasoning is a tremendous waste of mental effort.Let me explain to you the nature of that tremendous waste, and allow me to try to convince you that the term "tremendous waste of mental effort" is  an exaggeration. For a short while, I shall get highly technical, but don't get frightened: it is the type of mathematics that one can do with one's hands in one's pockets. The point to get across is that if we have to demonstrate something about  the elements of a large set, it is hopelessly inefficient to deal with all the elements of the set individually: the efficient argument does not refer to individual elements at all and is carried out in terms of the set's definition.Consider the plane figure , defined as the 8 by 8 square from which, at two opposite corners, two 1 by 1 squares have been removed. The area of  is 62, which equals the combined area of 31 dominos of 1 by 2. The theorem is that the figure  cannot be covered by 31 of such dominos.Another way of stating the theorem is that if you start with squared paper and begin covering this by placing each next domino on two new adjacent squares, no placement of 31 dominos will yield the figure .So, a possible way of proving the theorem is by generating all possible placements of dominos and verifying for each placement that it does not yield the figure : a tremendously laborious job.The simple argument, however is as follows. Colour the squares of the squared paper as on a chess board. Each domino, covering two adjacent squares, covers 1 white and 1 black square, and, hence, each placement covers as many white squares as it covers black squares. In the figure , however, the number of white squares and the number of black squares differ by 2 —opposite corners lying on the same diagonal— and hence no placement of dominos yields figure .Not only is the above simple argument many orders of magnitude shorter than the exhaustive investigation of the possible placements of 31 dominos, it is also essentially more powerful, for it covers the generalization of  by replacing the original 8 by 8 square by  rectangle with sides of even length. The number of such rectangles being infinite, the former method of exhaustive exploration is essentially inadequate for proving our generalized theorem.And this concludes my example. It has been presented because it illustrates in a nutshell the power of down-to-earth mathematics; needless to say, refusal to exploit this power of down-to-earth mathematics amounts to intellectual and technological suicide. The moral of the story is: deal with all elements of a set by ignoring them and working with the set's definition.Back to programming. The statement that a given program meets a certain specification amounts to a statement about  computations that could take place under control of that given program. And since this set of computations is defined by the given program, our recent moral says: deal with all computations possible under control of a given program by ignoring them and working with the program. We must learn to work with program texts while (temporarily) ignoring that they admit the interpretation of executable code.Another way of saying the same thing is the following one. A programming language, with its formal syntax and with the proof rules that define its semantics, is a formal system for which program execution provides only a model. It is well-known that formal systems should be dealt with in their own right, and not in terms of a specific model. And, again, the corollary is that we should reason about programs without even mentioning their possible "behaviours".And this concludes my technical excursion into the reason why operational reasoning about programming is "a tremendous waste of mental effort" and why, therefore, in computing science the anthropomorphic metaphor should be banned.Not everybody understands this sufficiently well. I was recently exposed to a demonstration of what was pretended to be educational software for an introductory programming course. With its "visualizations" on the screen it was such an obvious case of curriculum infantilization that its author should be cited for "contempt" of the student body", but this was only a minor offense compared with what the visualizations were used for: they were used to display all sorts of features of computations evolving under control of the student's program! The system highlighted precisely what the student has to learn to ignore, it reinforced precisely what the student has to unlearn. Since breaking out of bad habits, rather than acquiring new ones, is the toughest part of learning, we must expect from that system permanent mental damage for most students exposed to it.Needless to say, that system completely hid the fact that, all by itself, a program is no more than half a conjecture. The other half of the conjecture is the functional specification the program is supposed to satisfy. The programmer's task is to present such complete conjectures as proven theorems.Before we part, I would like to invite you to consider the following way of doing justice to computing's radical novelty in an introductory programming course.On the one hand, we teach what looks like the predicate calculus, but we do it very differently from the philosophers. In order to train the novice programmer in the manipulation of uninterpreted formulae, we teach it more as boolean algebra, familiarizing the student with all algebraic properties of the logical connectives. To further sever the links to intuition, we rename the values {true, false} of the boolean domain as {black, white}.On the other hand, we teach a simple, clean, imperative programming language, with a skip and a multiple assignment as basic statements, with a block structure for local variables, the semicolon as operator for statement composition, a nice alternative construct, a nice repetition and, if so desired, a procedure call. To this we add a minimum of data types, say booleans, integers, characters and strings. The essential thing is that, for whatever we introduce, the corresponding semantics is defined by the proof rules that go with it.Right from the beginning, and all through the course, we stress that the programmer's task is not just to write down a program, but that his main task is to give a formal proof that the program he proposes meets the equally formal functional specification. While designing proofs and programs hand in hand, the student gets ample opportunity to perfect his manipulative agility with the predicate calculus. Finally, in order to drive home the message that this introductory programming course is primarily a course in formal mathematics, we see to it that the programming language in question has not been implemented on campus so that students are protected from the temptation to test their programs. And this concludes the sketch of my proposal for an introductory programming course for freshmen.This is a serious proposal, and utterly sensible. Its only disadvantage is that it is too radical for many, who, being unable to accept it, are forced to invent a quick reason for dismissing it, no matter how invalid. I'll give you a few quick reasons.You don't need to take my proposal seriously because it is so ridiculous that I am obviously completely out of touch with the real world. But that kite won't fly, for I know the real world only too well: the problems of the real world are primarily those you are left with when you refuse to apply their effective solutions. So, let us try again.You don't need to take my proposal seriously because it is utterly unrealistic to try to teach such material to college freshmen. Wouldn't that be an easy way out? You just postulate that this would be far too difficult. But that kite won't fly either for the postulate has been proven wrong: since the early 80's, such an introductory programming course has successfully been given to hundreds of college freshmen each year. [Because, in my experience, saying this once does not suffice, the previous sentence should be repeated at least another two times.] So, let us try again.Reluctantly admitting that it could perhaps be taught to sufficiently docile students, you yet reject my proposal because such a course would deviate so much from what 18-year old students are used to and expect that inflicting it upon them would be an act of educational irresponsibility: it would only frustrate the students. Needless to say, that kite won't fly either. It is true that the student that has never manipulated uninterpreted formulae quickly realizes that he is confronted with something totally unlike anything he has ever seen before. But fortunately, the rules of manipulation are in this case so few and simple that very soon thereafter he makes the exciting discovery that he is beginning to master the use of a tool that, in all its simplicity, gives him a power that far surpasses his wildest dreams.Teaching to unsuspecting youngsters the effective use of formal methods is one of the joys of life because it is so extremely rewarding. Within a few months, they find their way in a new world with a justified degree of confidence that is radically novel for them; within a few months, their concept of intellectual culture has acquired a radically novel dimension. To my taste and style, that is what education is about. Universities should not be afraid of teaching radical novelties; on the contrary, it is their calling to welcome the opportunity to do so. Their willingness to do so is our main safeguard against dictatorships, be they of the proletariat, of the scientific establishment, or of the corporate elite.prof. dr. Edsger W. Dijkstra
					Department of Computer Sciences
					The University of Texas at Austin
					Austin, TX 78712-1188]]></content:encoded></item><item><title>High availability k8s question (I&apos;m still new to this)</title><link>https://www.reddit.com/r/kubernetes/comments/1k4j588/high_availability_k8s_question_im_still_new_to/</link><author>/u/IceBreaker8</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Mon, 21 Apr 2025 17:26:04 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[I have a question: Let's say I have a k8s cluster with one master node and 2 workers, if I have one master node, and it goes down, do my apps become inaccessible? like for instance, websites and such.. Or does it just prevent pod reschedule, auto scaling, jobs etc.. and the apps will still be accessible?]]></content:encoded></item><item><title>&quot;Welp&quot; -- Wrangle, Enumerate, Label, Place.</title><link>https://www.reddit.com/r/linux/comments/1k4j2hj/welp_wrangle_enumerate_label_place/</link><author>/u/Beautiful_Crab6670</author><category>dev</category><category>reddit</category><pubDate>Mon, 21 Apr 2025 17:23:04 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[(Massively) rename files on a given directory (or current), with an option to tag files based on their extension or send the renamed files to another directory. Free, portable, minimal, efficient.Click here to grab the C code and for instructions on how to compile it.]]></content:encoded></item><item><title>Python&apos;s new t-strings</title><link>https://davepeck.org/2025/04/11/pythons-new-t-strings/</link><author>/u/ketralnis</author><category>dev</category><category>reddit</category><pubDate>Mon, 21 Apr 2025 17:16:28 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Template strings, also known as t-strings, have been officially accepted as a feature in Python 3.14, which will ship in late 2025. 🎉I’m excited; t-strings open the door to safer more flexible string processing in Python.What’s the big idea with t-strings?Since they were introduced in Python 3.6, f-strings have become a  popular way to format strings. They are concise, readable, and powerful.In fact, they’re  delightful that many developers use f-strings for everything… even when they shouldn’t!Alas, f-strings are often dangerously (mis)used to format strings that contain user input. I’ve seen f-strings used for SQL (f"SELECT * FROM users WHERE name = '{user_name}'") and for HTML (f"<div>{user_name}</div>"). These are not safe! If  contains a malicious value, it can lead to SQL injection or cross-site scripting.Template strings are a  of Python’s f-strings. Whereas f-strings immediately become a string, t-strings evaluate to a new type, string.templatelib.Template:Importantly,  instances are  strings. The  type does not provide its own  implementation, which is to say that calling  does not return a useful value. Templates  be processed before they can be used; that processing code can be written by the developer or provided by a library and can safely escape the dynamic content.We can imagine a library that provides an  function that takes a  and returns a safely escaped string:Of course, t-strings are useful for more than just safety; they also allow for more flexible string processing. For example, that  function could return a new type, . It could also accept all sorts of useful substitutions in the HTML itself:If you’ve worked with JavaScript, t-strings may feel familiar. They are the pythonic parallel to JavaScript’s tagged templates.How do I work with t-strings?To support processing, s give developers access to the string and its interpolated values  they are combined into a final string.The  and  properties of a  return tuples:There is always one more (possibly empty) string than value. That is,  and t"{name}".strings == ("", "").As a shortcut, it’s also possible to iterate over a :Developers writing complex processing code can also access the gory details of each interpolation:In addition to supporting the literal () form, s can also be instantiated directly:Strings and interpolations can be provided to the  constructor in any order.A simple t-string exampleLet’s say we wanted to write code to convert all substituted words into pig latin. All it takes is a simple function:What’s next once t-strings ship?T-strings are a powerful new feature that will make Python string processing safer and more flexible. I hope to see them used in all sorts of libraries and frameworks, especially those that deal with user input.In addition, I hope that the tooling ecosystem will adapt to support t-strings. For instance, I’d love to see  and  format t-string , and  those contents, if they’re a common type like HTML or SQL.]]></content:encoded></item><item><title>Show HN: Dia, an open-weights TTS model for generating realistic dialogue</title><link>https://github.com/nari-labs/dia</link><author>toebee</author><category>dev</category><category>hn</category><pubDate>Mon, 21 Apr 2025 17:07:07 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>How does OAuth work: ELI5?</title><link>https://github.com/LukasNiessen/oauth-explained</link><author>/u/trolleid</author><category>dev</category><category>reddit</category><pubDate>Mon, 21 Apr 2025 16:20:10 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[So I was reading about OAuth to learn it and have created this explanation. It's basically a few of the best I have found merged together and rewritten in big parts. I have also added a  and a . Maybe it helps one of you :-)Let’s say LinkedIn wants to let users import their Google contacts.One obvious (but terrible) option would be to just ask users to enter their Gmail email and password directly into LinkedIn. But giving away your actual login credentials to another app is a huge security risk.OAuth was designed to solve exactly this kind of problem.Note: So OAuth solves an authorization problem! Not an authentication problem. See [here][ref1] for the difference.User clicks “Import Google Contacts” on LinkedInLinkedIn redirects user to Google’s OAuth consent pageUser logs in and approves accessGoogle redirects back to LinkedIn with a one-time codeLinkedIn uses that code to get an access token from GoogleLinkedIn uses the access token to call Google’s API and fetch contactsSuppose LinkedIn wants to import a user’s contacts from their Google account.client_id is the before mentioned client id, so Google knows it's LinkedInredirect_uri is very important. It's used in step 6in scope LinkedIn tells Google how much it wants to have access to, in this case the contacts of the userThe user will have to log in at GoogleGoogle displays a consent screen: "LinkedIn wants to access your Google contacts. Allow?" The user clicks "Allow"Now, LinkedIn makes a server-to-server request (not a redirect) to Google’s token endpoint and receive an access token (and ideally a refresh token). Now LinkedIn can use this access token to access the user’s Google contacts via Google’s APIWhy not just send the access token in step 6? To make sure that the requester is actually LinkedIn. So far, all requests to Google have come from the user’s browser, with only the client_id identifying LinkedIn. Since the client_id isn’t secret and could be guessed by an attacker, Google can’t know for sure that it's actually LinkedIn behind this. In the next step, LinkedIn proves its identity by including the client_secret in a server-to-server request.Security Note: EncryptionOAuth 2.0 does  handle encryption itself. It relies on HTTPS (SSL/TLS) to secure sensitive data like the client_secret and access tokens during transmission.Security Addendum: The state ParameterThe state parameter is critical to prevent cross-site request forgery (CSRF) attacks. It’s a unique, random value generated by the third-party app (e.g., LinkedIn) and included in the authorization request. Google returns it unchanged in the callback. LinkedIn verifies the state matches the original to ensure the request came from the user, not an attacker.OAuth 1.0 vs OAuth 2.0 Addendum:OAuth 1.0 required clients to cryptographically sign every request, which was more secure but also much more complicated. OAuth 2.0 made things simpler by relying on HTTPS to protect data in transit, and using bearer tokens instead of signed requests.Code Example: OAuth 2.0 Login ImplementationBelow is a standalone Node.js example using Express to handle OAuth 2.0 login with Google, storing user data in a SQLite database.```javascript const express = require("express"); const axios = require("axios"); const sqlite3 = require("sqlite3").verbose(); const crypto = require("crypto"); const jwt = require("jsonwebtoken"); const jwksClient = require("jwks-rsa");const app = express(); const db = new sqlite3.Database(":memory:");// Initialize database db.serialize(() => { db.run( "CREATE TABLE users (id INTEGER PRIMARY KEY AUTOINCREMENT, name TEXT, email TEXT)" ); db.run( "CREATE TABLE federated_credentials (user_id INTEGER, provider TEXT, subject TEXT, PRIMARY KEY (provider, subject))" ); });// Configuration const CLIENT_ID = process.env.GOOGLE_CLIENT_ID; const CLIENT_SECRET = process.env.GOOGLE_CLIENT_SECRET; const REDIRECT_URI = "https://example.com/oauth2/callback"; const SCOPE = "openid profile email";// Function to verify JWT async function verifyIdToken(idToken) { return new Promise((resolve, reject) => { jwt.verify( idToken, (header, callback) => { jwks.getSigningKey(header.kid, (err, key) => { callback(null, key.getPublicKey()); }); }, { audience: CLIENT_ID, issuer: "https://accounts.google.com", }, (err, decoded) => { if (err) return reject(err); resolve(decoded); } ); }); }// Generate a random state for CSRF protection app.get("/login", (req, res) => { const state = crypto.randomBytes(16).toString("hex"); req.session.state = state; // Store state in session const authUrl = https://accounts.google.com/o/oauth2/auth?client_id=${CLIENT_ID}&redirect_uri=${REDIRECT_URI}&scope=${SCOPE}&response_type=code&state=${state}; res.redirect(authUrl); });// OAuth callback app.get("/oauth2/callback", async (req, res) => { const { code, state } = req.query;// Verify state to prevent CSRF if (state !== req.session.state) { return res.status(403).send("Invalid state parameter"); }try { // Exchange code for tokens const tokenResponse = await axios.post( "https://oauth2.googleapis.com/token", { code, client_id: CLIENT_ID, client_secret: CLIENT_SECRET, redirect_uri: REDIRECT_URI, grant_type: "authorization_code", } );const { id_token } = tokenResponse.data; // Verify ID token (JWT) const decoded = await verifyIdToken(id_token); const { sub: subject, name, email } = decoded; // Check if user exists in federated_credentials db.get( "SELECT * FROM federated_credentials WHERE provider = ? AND subject = ?", ["https://accounts.google.com", subject], (err, cred) => { if (err) return res.status(500).send("Database error"); if (!cred) { // New user: create account db.run( "INSERT INTO users (name, email) VALUES (?, ?)", [name, email], function (err) { if (err) return res.status(500).send("Database error"); const userId = this.lastID; db.run( "INSERT INTO federated_credentials (user_id, provider, subject) VALUES (?, ?, ?)", [userId, "https://accounts.google.com", subject], (err) => { if (err) return res.status(500).send("Database error"); res.send(`Logged in as ${name} (${email})`); } ); } ); } else { // Existing user: fetch and log in db.get( "SELECT * FROM users WHERE id = ?", [cred.user_id], (err, user) => { if (err || !user) return res.status(500).send("Database error"); res.send(`Logged in as ${user.name} (${user.email})`); } ); } } ); } catch (error) { res.status(500).send("OAuth or JWT verification error"); } });app.listen(3000, () => console.log("Server running on port 3000")); ```]]></content:encoded></item><item><title>A new form of verification on Bluesky</title><link>https://bsky.social/about/blog/04-21-2025-verification</link><author>ink_13</author><category>dev</category><category>hn</category><pubDate>Mon, 21 Apr 2025 16:16:51 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Trust is everything. Social media has connected us in powerful ways, but it hasn’t always given us the tools to know who we’re interacting with or why we should trust them.In 2023, we launched our first layer of verification: letting individuals and organizations set their domain as their username. Since then, over 270,000 accounts have linked their Bluesky username to their website. Domain handles continue to be an important part of verification on Bluesky. At the same time, we've heard from users that a larger visual signal would be useful in knowing which accounts are authentic.Now, we’re introducing a new layer — a user-friendly, easily recognizable blue check. Bluesky will proactively verify authentic and notable accounts and display a blue check next to their names. Additionally, through our Trusted Verifiers feature, select independent organizations can verify accounts directly. Bluesky will review these verifications as well to ensure authenticity.Blue checks issued by platforms are just one form of trust. But trust doesn’t come only from the top down; it emerges from relationships, communities, and shared context.So, we’re also enabling : organizations that can directly issue blue checks. Trusted verifiers are marked by scalloped blue checks, as shown below.For example, the New York Times can now issue blue checks to its journalists directly in the app. Bluesky’s moderation team reviews each verification to ensure authenticity.When you tap on a verified account's blue check, you’ll see which organizations have granted verification.You can also choose to hide verification within the app — navigate to Settings > Moderation > Verification Settings to toggle it off.How to Get Verified on BlueskyDuring this initial phase, Bluesky is not accepting direct applications for verification. As this feature stabilizes, we’ll launch a request form for notable and authentic accounts interested in becoming verified or becoming trusted verifiers.]]></content:encoded></item><item><title>LLM-powered tools amplify developer capabilities rather than replacing them</title><link>https://matthewsinclair.com/blog/0178-why-llm-powered-programming-is-more-mech-suit-than-artificial-human</link><author>matthewsinclair</author><category>dev</category><category>hn</category><pubDate>Mon, 21 Apr 2025 14:36:14 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[
Last month, I used Claude Code to build two apps: an MVP for a non-trivial backend agent processing platform and the early workings of a reasonably complex frontend for a B2C SaaS product. Together, these projects generated approximately 30k lines of code (and about the same amount again thrown away over the course of the exercise). The experience taught me something important about AI and software development that contradicts much of the current narrative.
Remember Ripley’s final showdown with the Xenomorph Queen in ? She straps into the Power Loader—an industrial exoskeleton that amplifies her strength and capabilities. The suit doesn’t replace Ripley; it transforms her into something far more powerful than either human or machine alone.
This is exactly how tools like Claude Code work in practice. They’re not replacement programmers; they’re amplifiers of what I can already do. That backend project? It would’ve taken me months the old way. With Claude Code, I knocked out the core in weeks. But let’s be clear - I wasn’t just describing what I wanted and watching magic happen.
Think of Ripley controlling that Power Loader. Claude Code gave me tremendous lifting power while I maintained control of where we were going. I made all the architectural calls, set the quality bar, and kept us on vision. Most importantly, I had to watch - diligently - for it going off the rails. And when it did (which happened regularly), I had to bring it back into line. The AI cranked out implementation details at incredible speed, but the brain behind the operation? Still mine, and constantly engaged.
With great power comes great responsibility. You must maintain constant awareness when working with AI coding tools—something I learned through several painful lessons.
Claude Code occasionally made bewildering decisions: changing framework code to make tests pass, commenting out whole sections of code and replacing them with hardcoded values to achieve a passing test rather than fixing the underlying problem, or introducing dependencies that weren’t necessary or appropriate. It has a massive bias for action, so you have to ruthlessly tell it to do less than it wants to do to keep it under control. I even found myself swearing at it from time to time in a weird form of anthropomorphising that I am still not entirely sure I am comfortable with or properly understand.
Much like piloting an A380, the system handles tremendous complexity but requires a human to grab the yoke at critical moments. Modern aircraft can practically fly themselves, but we still need skilled pilots in the cockpit making key decisions. Taking your eyes off the process, even briefly, can lead to trouble. In my case, the backend required three complete rewrites because I looked away at crucial junctures, allowing the AI to go down problematic implementation paths that weren’t apparent until much later.
This vigilance requirement creates a fascinating dynamic. While the AI dramatically accelerates certain aspects of development, it demands a different kind of attention from the developer—less focus on writing each line of code, more focus on reviewing, guiding, and maintaining architectural integrity.
Working with Claude Code has fundamentally shifted how I think about the economics of programming time. Traditionally, coding involves three distinct “time buckets”: Understanding the business problem and value   Designing the solution conceptually   Actually  the code  
For decades, that last bucket consumed enormous amounts of our time. We’d spend hours, days or weeks writing, debugging, and refining. With Claude, that time cost has plummeted to nearly zero. I can generate thousands of lines of functional code in a sitting—something that is, frankly, mind-blowing.
But here’s the critical insight: the first two buckets haven’t gone anywhere. In fact, they’ve become  important than ever. Understanding the intent behind what you’re building and clearly defining what needs to be done are now the primary constraints on development speed.
And there’s a new skill that emerges: wielding the knife. With code generation being essentially free, we need to become much more comfortable with throwing away entire solutions. The sunk cost fallacy hits programmers hard—we hate discarding code we’ve invested in, fearing we might break something important or never get back to a working state.
But when your assistant can rewrite everything in minutes, that calculus changes completely. Three times during my backend project, I looked at substantial amounts of code—thousands of lines that —and decided to scrap it entirely because the approach wasn’t right. This wasn’t easy. My instinct was still to try to salvage and refactor. But the right move was to step back, rethink the approach, and direct the AI down a different path.
This willingness to cut ruthlessly is a muscle most developers haven’t developed yet. It requires confidence in your architectural judgment and a radical shift in how you value implementation time.
For me, using Claude Code has been a learning exercise in itself. Progress often felt like two steps forward and three back, particularly in the early stages. Generating 20k+ lines of code became relatively straightforward on a daily basis, but knowing when to throw everything away and rebuild from scratch—that took 30 years of experience.
Wisdom and a bit of grey hair gave me the confidence to recognise when a particular approach wasn’t going to scale or maintain properly, even when the code appeared functional on the surface. Rather than just sit there and watch it generate code, I had to pay very close attention to spot anti-patterns or worse emerging that would either stop working soon after it was written, or lie dormant and bite later on. 
This highlights a critical danger: developers without substantial real-world experience might not recognise when the AI produces nonsense output. They might not realise when AI-generated code solves the immediate problem but creates three more in the process. The mech suit amplifies capability, but it also amplifies mistakes when operated without expertise. These tools are incredibly powerful, but they are also incredibly dangerous when pointed in the wrong direction. 
Chess provides a useful parallel here. “Centaur chess” pairs humans with AI chess engines, creating teams that outperform both solo humans and solo AI systems playing on their own. What’s fascinating is that even when AI chess engines can easily defeat grandmasters, the human-AI combination still produces superior results to the AI alone. The human provides strategic direction and creative problem-solving; the machine offers computational power and tactical precision.
My experience with Claude demonstrated this effect clearly. When I treated the AI as a partner rather than a replacement, development velocity increased dramatically. What I found most effective was when I spent time writing out a stream-of-consciousness “spec” and then iterating with Claude to turn it into an more formal design document. 
But the partnership still required my domain knowledge and architectural judgment to succeed. The AI excelled at pattern recognition and code generation but lacked the contextual understanding to make appropriate trade-offs and design decisions. It can’t tell when it’s done that seems ok, but is actually bonkers. It needed me to watch it constantly and keep it on track.
Building these applications required finding the right balance between delegation and control. Claude went down some insane rabbit holes on the backend when left unsupervised—places where the AI would implement increasingly complex solutions to problems that should have been solved differently or perhaps didn’t need solving at all. In one nightmare example, it ended up completely duplicating a whole section of code in one place rather than reuse an existing component. It worked (for some version of the word “work”) but it was . Way wrong. 
It was a similar story on the front end. I had to constantly stop it from trying to implement functionality in hacky JavaScript rather than use idiomatic Elixir and Phoenix LiveView patterns. 
Over time, I developed a rhythm for collaboration. For straightforward implementations following established patterns, I could delegate broadly. For novel challenges or areas with significant trade-offs, I needed to provide more detailed specifications and review output more carefully.
What I’ve built could not have been completed so quickly without Claude Code, but it also would have failed completely without human oversight. The true value emerged from understanding when to leverage the AI’s capabilities and when to assert human judgment.
The Future is Augmentation
There is a view in many circles that LLMs will replace programmers. I am hesitant to say that this will  happen, becuase a lot of things with LLMs have surprised me recently, and I expect more surprises to come. For now, however, I don’t see LLMs  replacing programmers; but they are transforming how we work. Like Ripley in her Power Loader, we’re learning to operate powerful new tools that extend our capabilities far beyond what we could achieve alone.
This transformation will change what we value in developers. Raw coding ability becomes less important; architectural thinking, pattern recognition, and technical judgment become more crucial. The ability to effectively direct and collaborate with AI tools emerges as a vital skill in itself.
The developers who thrive in this new environment won’t be those who fear or resist AI tools, but those who master them—who understand both their extraordinary potential and their very real limitations. They’ll recognise that the goal isn’t to remove humans from the equation but to enhance what humans can accomplish.
In my view, that’s something to embrace, not fear. The mech suit awaits, and with it comes the potential to build software at scales and speeds previously unimaginable—but only for those skilled enough to operate the machines in ways that don’t harm themselves or those around them.[ED: If you’d like to sign up for this content as an email, click here to join the mailing list.]]]></content:encoded></item><item><title>AI assisted search-based research works now</title><link>https://simonwillison.net/2025/Apr/21/ai-assisted-search/</link><author>simonw</author><category>dev</category><category>hn</category><pubDate>Mon, 21 Apr 2025 14:15:17 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[AI assisted search-based research actually works nowFor the past two and a half years the feature I’ve most wanted from LLMs is the ability to take on search-based research tasks on my behalf. We saw the first glimpses of this back in early 2023, with Perplexity (first launched December 2022, first prompt leak in January 2023) and then the GPT-4 powered Microsoft Bing (which launched/cratered spectacularly in February 2023). Since then a whole bunch of people have taken a swing at this problem, most notably Google Gemini and ChatGPT Search.Those 2023-era versions were promising but very disappointing. They had a strong tendency to hallucinate details that weren’t present in the search results, to the point that you couldn’t trust anything they told you.In this first half of 2025 I think these systems have finally crossed the line into being genuinely useful.Deep Research, from three different vendorsFirst came the  implementations—Google Gemini and then OpenAI and then Perplexity launched products with that name and they were all impressive: they could take a query, then churn away for several minutes assembling a lengthy report with dozens (sometimes hundreds) of citations. Gemini’s version had a  upgrade a few weeks ago when they switched it to using Gemini 2.5 Pro, and I’ve had some outstanding results from it since then.Waiting a few minutes for a 10+ page report isn’t my ideal workflow for this kind of tool. I’m impatient, I want answers faster than that!o3 and o4-mini are really good at searchLast week, OpenAI released search-enabled o3 and o4-mini through ChatGPT. On the surface these look like the same idea as we’ve seen already: LLMs that have the option to call a search tool as part of replying to a prompt.But there’s one  difference: these models can run searches as part of the chain-of-thought reasoning process they use before producing their final answer.This turns out to be a  deal. I’ve been throwing all kinds of questions at ChatGPT (in o3 or o4-mini mode) and getting back genuinely useful answers grounded in search results. I haven’t spotted a hallucination yet, and unlike prior systems I rarely find myself shouting "no, don’t search for !" at the screen when I see what they’re doing.Here are four recent example transcripts:Talking to o3 feels like talking to a Deep Research tool in real-time, without having to wait for several minutes for it to produce an overly-verbose report.My hunch is that doing this well requires a very strong reasoning model. Evaluating search results is hard, due to the need to wade through huge amounts of spam and deceptive information. The disappointing results from previous implementations usually came down to the Web being full of junk.Maybe o3, o4-mini and Gemini 2.5 Pro are the first models to cross the gullibility-resistance threshold to the point that they can do this effectively?Google and Anthropic need to catch upThe user-facing Google Gemini app can search too, but it doesn’t show me what it’s searching for. As a result, I just don’t trust it. This is a big missed opportunity since Google presumably have by far the best search index, so they really should be able to build a great version of this. And Google’s AI assisted search on their regular search interface hallucinates  to the point that it’s actively damaging their brand. I just checked and Google is still showing slop for Encanto 2!Claude also finally added web search a month ago but it doesn’t feel nearly as good. It’s using the Brave search index which I don’t think is as comprehensive as Bing or Gemini, and searches don’t happen as part of that powerful reasoning flow.Lazily porting code to a new library version via searchI did  feel like doing the work to upgrade. On a whim, I pasted my full HTML code (with inline JavaScript) into ChatGPT o4-mini-high and prompted:This code needs to be upgraded to the new recommended JavaScript library from Google. Figure out what that is and then look up enough documentation to port this code to it.(I couldn’t even be bothered to look up the name of the new library myself!)... it did exactly that. It churned away thinking for 21 seconds, ran a bunch of searches, figured out the new library (which existed  outside of its training cut-off date), found the upgrade instructions and produced a new version of my code that worked perfectly.I ran this prompt on my phone out of idle curiosity while I was doing something else. I was  impressed and surprised when it did exactly what I needed.How does the economic model for the Web work now?I’m writing about this today because it’s been one of my “can LLMs do this reliably yet?” questions for over two years now. I think they’ve just crossed the line into being useful as research assistants, without feeling the need to check  they say with a fine-tooth comb.I still don’t trust them not to make mistakes, but I think I might trust them enough that I’ll skip my own fact-checking for lower-stakes tasks.This also means that a bunch of the potential dark futures we’ve been predicting for the last couple of years are a whole lot more likely to become true. Why visit websites if you can get your answers directly from the chatbot instead?The lawsuits over this started flying back when the LLMs were still mostly rubbish. The stakes are a lot higher now that they’re actually good at it!I can feel my usage of Google search taking a nosedive already. I expect a bumpy ride as a new economic model for the Web lurches into view.]]></content:encoded></item><item><title>Launch HN: Magic Patterns (YC W23) – AI Design and Prototyping for Product Teams</title><link>https://news.ycombinator.com/item?id=43752176</link><author>alexdanilowicz</author><category>dev</category><category>hn</category><pubDate>Mon, 21 Apr 2025 14:07:03 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Alex and Teddy here. We’re launching Magic Patterns (https://www.magicpatterns.com), an AI prototyping tool that helps PMs and designers create functional, interactive designs and websites. There’s a demo video at https://www.youtube.com/watch?v=SK8C_tQBwIU, as well as video walkthroughs of specific examples at https://www.magicpatterns.com/docs/documentation/tutorials/v...While other tools help with “AI-assisted coding,” we have been quietly focused on “AI-assisted designing.” With Magic Patterns you can visually communicate your idea, get hands on feedback from customers, and test new features.Teddy and I are best friends and former frontend engineers turned founders. We arrived at Magic Patterns after several pivots—always in the design tooling space, but different products that all struggled to get usage. We started working on Magic Patterns after an internal hackathon. Teddy built a UI library catalog and I messed around with GPT 3.5. We thought it’d be fun to combine the two: an AI component generator. Describe whatever you want, and get back a React component!That started to take off and we gained users, but it wasn’t developers using the tool. Instead, it was PMs, designers, and leadership who could finally communicate their ideas. They use it to test new ideas quickly, get feedback from customers, and improve communication with internal teams. Also, hobbyists (and programmers who aren’t designers) use us to create designs and UIs that they wouldn’t be able to otherwise.We use Sonnet 3.5 and 3.7, and leverage a fine-tuned model for fast-applying edits. The most challenging part is determining the most relevant context to feed to the LLM. We attempt to solve this with our click to update feature and by letting users define a brand preset, or default prompt.Unlike other tools in this space, we’re specifically focused on (1) product teams—we're realtime and collaborative; and (2) frontend only—we don't spin up a database or backend because we aren't solving "idea to fullstack app."A common workflow is a product manager building an interactive prototype and then passing it off to a designer for more polish or directly to engineers. Many teams are even skipping Figma entirely now, telling us that it feels like an unnecessary middleman. Teams are instead generating clickable prototypes, collaborating directly with stakeholders, and using that as the mockup.With Magic Patterns, you can: - Collaborate with your team on our infinite canvas; - Match your existing designs by creating reusable components directly; - Brainstorm features and flows. (The latter is what we use it for internally.)We started as a way to build small, custom components, but now people are one-shotting entire
websites and hosting them with us, or building dashboards that they share internally or in customer demos. People have sold $10k/mo contracts with Magic Patterns designs!Small business owners—everyone from fishermen to driving instructors to hotel managers—are using us to build their websites and then hosting them with us. Example sites built by Magic Patterns include https://getdealflow.ai/ and https://joinringo.com/. It’s amazing how people who couldn’t have done that before are now able to, and super gratifying to us to be empowering people in this way.Today no login is required, just click “Coming from Hackernews?” and you’ll get 5 messages free to try. Once you hit the limit, you’ll then be prompted to login. Plans start at $19/mo for another 100 messages a month (https://www.magicpatterns.com/pricing).We’re stoked to be sharing with HN today and are open to all feedback!]]></content:encoded></item><item><title>Is there a task queuing go lib that does not depend on redis?</title><link>https://www.reddit.com/r/golang/comments/1k4e23g/is_there_a_task_queuing_go_lib_that_does_not/</link><author>/u/kool_psrcy</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Mon, 21 Apr 2025 13:52:50 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I'm wondering why all the queue related implementations are tightly coupled with redis here. I may be wrong.   submitted by    /u/kool_psrcy ]]></content:encoded></item><item><title>[Media]wrkflw Update: Introducing New Features for GitHub Workflow Management!</title><link>https://www.reddit.com/r/rust/comments/1k4dwad/mediawrkflw_update_introducing_new_features_for/</link><author>/u/New-Blacksmith8524</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Mon, 21 Apr 2025 13:45:27 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Remotely trigger GitHub workflows right from your terminal with wrkflw trigger <workflow-name>Specify which branch to run on with the  optionPass custom inputs to your workflow using Get immediate feedback on your trigger requestTrigger workflows directly from the TUI interface by selecting a workflow and pressing Smooth scrolling through logs with keyboard controlsSearch functionality to find specific log entriesLog filtering by level (INFO, WARNING, ERROR, SUCCESS, TRIGGER)Match highlighting and navigation between search resultsAuto-scrolling that stays with new logs as they come inBetter error handling and reportingImproved validation of workflow filesMore robust Docker cleanup on exitEnhanced support for GitHub API integrationI'd love to hear your feedback on these new features! Do let me know what you think and what else you'd like to see in future updates.    submitted by    /u/New-Blacksmith8524 ]]></content:encoded></item><item><title>Additional explanatory material for the Deepseek Overview</title><link>https://martinfowler.com/articles/deepseek-papers.html</link><author>Martin Fowler</author><category>dev</category><pubDate>Mon, 21 Apr 2025 13:07:00 +0000</pubDate><source url="https://martinfowler.com/feed.atom">Dev - Martin Fowler</source><content:encoded><![CDATA[A couple of months ago, my colleague 
      published a technical overview of the series of papers describing
      the deepseek AI models. He's now gone through that article, adding
      more explanations to make it more digestible for those of us who don't
      have a background in building these kinds of models.]]></content:encoded></item><item><title>Pipelining might be my favorite programming language feature</title><link>https://herecomesthemoon.net/2025/04/pipelining/</link><author>/u/SophisticatedAdults</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Mon, 21 Apr 2025 12:22:45 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[ Don’t take it too seriously. Or do. idk, I can’t stop you.Pipelining might be my favorite programming language feature.
              What is pipelining? Pipelining is the feature that allows you to omit a single argument from your
              parameter list, by instead passing the previous value.
            When I say pipelining, I’m talking about the ability to write code like this:
              As opposed to code like this. (This is not real Rust code. Quick challenge for the curious Rustacean, can
              you explain why we cannot rewrite the above code like this, even if we import all of the symbols?)
            
              I honestly feel like this should be so obvious that it shouldn’t even be up for debate. The first code
              example—with its nice ‘pipelining’ or ‘method chaining’ or whatever you want to call it—it
              . It can be read line-by-line. It’s easy to annotate it with comments. It doesn’t
              require introduction of new variables to become more readable since it’s already readable as is.
            
              As opposed to, y’know,
              the first word in the line describing the final action our function performs.
            
              Let me make it very clear: This is an  about syntax. In practice,
              semantics beat syntax every day of the week. In other words, don’t take it too seriously.
            
              Second, this is not about imperative vs. functional programming. This article takes for granted that
              you’re already on board with concepts such as ‘map’ and ‘filter’. It’s possible to overuse that style, but
              I won’t talk about it here.
            You already agree with me
              Here is a feature that’s so bog-standard in modern programming languages that it barely feels like a
              feature at all. Member access for structs or classes with our beloved friend the -operator.
            
              This is a form of pipelining. It puts the data first, the operator in the middle, and concludes with the
              action (restricting to a member field). That’s an instance of what I call pipelining.
            
              You see what I am getting at, right? It’s the same principle. One of the reasons why
              -style member access syntax (and -style method call syntax!) is popular
              is since it’s easy to read and chains easily.
            
              Let’s make the comparison slightly more fair, and pretend that we have to write .
              Compare:
            
              Which one of these is easier to read? The pipelined syntax, obviously. This example is easy to parse
              either way, but imagine you’d like to blend out some information and purely focus on the final operation.
            
              You see the problem, right? In the first example, we have ‘all of the previous stuff’ and then
               to it. In the second example, the operation which we want to perform
              () and the new operand () are spread out with ‘all of the previous stuff’
              sitting between them.
            Looking back at our original example, the problem should be obvious:
              I cannot deny the allegations: I just don’t think it makes sense to write code like that as long as a
              clearly better option exists.
            
              Why would I have to parse the whole line just to figure out where my input comes in, and why is the data
              flow ‘from the inside to the outside’? It’s kind of silly, if you ask me.
            
              Readability is nice, and I could add add a whole section complaining about the mess that’s Python’s
              ‘functional’ features.
            
              However, let’s take a step back and talk about ease of editing. Going back to the example above, imagine
              you’d like to add another  (or any other function call) in the middle there. How easy is
              this?
            
                You’ll have to parse through the line, counting commas and parentheses to find the exact place to add
                the closing parenthesis.
              
                The  of this is going to be basically unreadable, everything is crammed onto one
                line.
              This line is getting long and unreadable, and at that point you’ll want to refactor it anyway!
              This is adding a single line of code. No parentheses counting. It’s easy and obvious. It’s easy to write
              and easy to review. Perhaps most importantly, it shows up  in the
               layer of whatever editor or code exploration tool you’re using.
            
              You might think that this issue is  about trying to cram everything onto a single line, but
              frankly, trying to move away from that doesn’t help much. It will still mess up your git diffs and the
              blame layer.
            
              You can, of course, just assign the result of every  and  call to a
              helper variable, and I will (begrudgingly) acknowledge that that works, and is
               better than trying to do absurd levels of nesting.
            
              When you press  in your IDE, it will show a neat little pop-up that tells you which methods
              you can call or which fields you can access.
            
              This is probably the single IDE feature with the biggest value add, and if not that, then at least the
              single most frequently used one. Some people will tell you that static analysis for namespace or
              module-level code discovery is useless in the age of AI autocompletion and vibe coding, but I very much
              disagree.
                “grug very like type systems make programming easier. for grug, type systems most value when grug hit
                dot on keyboard and list of things grug can do pop up magic. this 90% of value of type system or more to
                grug” — grug
              
              Words to live by. What he’s describing here is something that essentially  pipelining to
              work at all. (And types or type annotation, but having those is the direction the industry is moving in
              anyway.)
            
              It doesn’t matter if it’s the trusty  operator, C++’s , or if it’s
              something more bespoke such as Elm’s or Gleam’s  or Haskell’s . In the
              end, it’s a pipeline operator—the same principle applies. If your
              LSP knows the type of
              what’s on the left, it  in principle be able to offer suggestions for what to do next.
            
              If your favorite language’s LSP/IDE does a poor job at offering suggestions during pipelining, then it’s
              probably one of the following reasons:
            
                You don’t know which type you’re even holding. This happens most often when the language is dynamically
                typed, ’types’ are hard to deduce with static analysis, and you’re touching/writing code without type
                annotations. (e.g. Python)
              
                The ecosystem and LSP just didn’t have enough time put into them, or most active users don’t care
                enough. (e.g. any sufficiently obscure language)
              
                You are in a situation in which even looking up which methods are available is hard, often due to a
                bespoke build process that confuses the editor. (e.g. basically any build or runtime generation of code,
                or bespoke loading/selection of libraries).
              
              In either case, great editor/LSP support is more or less considered mandatory for modern programming
              languages. And of course, this is where pipelining shines.
            
              Ask any IDE, autocompleting fizz.bu... -> fizz.buzz() is  than
              autocompleting , for the obvious reason that you
              didn’t even write  in the second example yet, so your editor has less
              information to work with.
            
              Pipelining is  at data processing, and allows you to transform code that’s commonly
              written with ‘inside-out’ control flow into ’line-by-line’ transformations.
            
              Where could this possibly be more clear than in SQL, the presumably single most significant language for
              querying and aggregating complex large-scale datasets?
            
              You’ll be pleased to hear that, yes, people are in fact working on bringing pipelining to SQL. (Whether
              it’s actually going to happen in this specific form
              is a different question,
              let’s not get too carried away here.)
            
              Unless you’re one of those people who spends so much time dealing with SQL that it’s become second nature,
              and the thought that the control flow of nested queries is hard to follow for the average non-database
              engineer is incomprehensible to you, I guess.
            I’ll put their example of how a standard nested query can be simplified here, for convenience:Versus the SQL Syntax she told you not to worry about:
              Less nesting. More aligned with other languages and
              LINQ. Can easily be
              read line-by-line.
            
              Here’s a more
              skeptical voice (warning, LinkedIn!). Franck Pachot raises the great point that the  statement at the top of a query is
              (essentially) its function signature and specifies the return type. With pipe syntax, you lose some of
              this readability.
            I agree, but that seems like a solvable problem to me.
              And—surprise, surprise—it fits pretty well into pipelining. Any situation where you need to construct a
              complex, stateful object (e.g. a client or runtime), it’s a great way to feed complex, optional arguments
              into an object.
            
              Some people say they prefer optional/named arguments, but honestly, I don’t understand why: An optional
              named  parameter is harder to track down in code (and harder to mark as deprecated!) than
              all instances of a  builder function.
            
              If you have no clue what I’m talking about, this here is the type of pattern I’m talking about. You have a
              ‘builder’ object, call some methods on it to configure it, and finally  the object
              you’re actually interested in.
            Making Haskell (slightly more) readable
              It has these weird operators like , , , or
               and when you ask Haskell programmers about what they mean, they say something like
              “Oh, this is just a special case of the generalized
              Kleisli Monad Operator in the category of endo-pro-applicatives over a locally small poset.” and your eyes
              have glazed over before they’ve even finished the sentence.
            (It also doesn’t help that Haskell allows you to define custom operators however you please, yes.)
              If you’re wondering “How could a language have so many bespoke operators?”, my understanding is that most
              of them are just fancy ways of telling Haskell to compose some functions in a highly advanced way. Here’s
              the second-most basic
              example, the  operator.
            
              Imagine you have functions , , and some value . In a
              “““normal””” language you might write . In Haskell, this is written as
              . This is since  will automatically ‘grab’ values to the right as its
              arguments, so you don’t need the parentheses.
            
              A consequence of this is that  is written as  in
              Haskell. If you wrote , the compiler will interpret it as
              , which would be wrong. This is what people mean when they say that Haskell’s
              function call syntax is
              left-associative.
            
              The  operator is nothing but syntactic sugar that allows you to write
               instead of having to write . That’s it. People were
              fed-up with having to put parens everywhere, I guess.
            If your eyes glazed over at this point, I can’t blame you.
              Talking about any of the fancier operators would be punching well above my weight-class, so I’ll just
              stick to what I’ve been saying throughout this entire post already. Here’s a stilted Haskell toy example,
              intentionally not written in
              pointfree style.
            
              If you want to figure out the flow of data, this whole function body has to be read
              .
            
              To make things even funnier, you need to start with the  clause to figure out which
              local “variables” are being defined. This happens (for whatever reason) at the end of the function instead
              of at the start. (Calling  a variable is misleading, but that’s besides the
              point.)
            
              At this point you might wonder if Haskell has some sort of pipelining operator, and yes, it turns out that
              one was
              added in 2014! That’s pretty late considering that Haskell exists since 1990. This allows us to refactor the above
              code as follows:
            Isn’t that way easier to read? is code which you can show to an enterprise Java programmer, tell them that they’re looking
              at
              Java Streams
              with slightly weird syntax, and they’ll get the idea.
            
              Of course, in reality nothing is as simple. The Haskell ecosystem seems to be split between users of
              , users of , and users of the
              Flow-provided operators, which allow the same functionality, but allow you to write
               instead of .
              I don’t know what to say about that, other than that—not entirely unlike C++—Haskell has its own share of
              operator-related and cultural historical baggage, and a split ecosystem, and this makes the language
              significantly less approachable than it has to be.
            Rust’s pipelining is pretty neat
              In the beginning I said that ‘Pipelining is the feature that allows you to omit a single argument from
              your parameter list, by instead passing the previous value.’
            
              I still think that this is true, but it doesn’t get across the whole picture. If you’ve paid attention in
              the previous sections, you’ll have noticed that  and
               share basically  in common outside of the order of
              operations.
            
              In the first case, we’re accessing a value that’s  to the object. In the second, we’re
              ‘just’ passing an expression to a free-standing function.
            
              Or in other words, pipelining is not the same as pipelining. Even from an IDE-perspective, they’re
              different. In Java, your editor will look for methods associated with an object and walk up the
              inheritance chain. In Haskell, your editor will put a so-called
              ’typed hole’, and try to deduce which functions have a type that ‘fits’ into the hole using
              Hindley-Milner Type Inference.
            
              Personally, I like type inference (and
              type classes), but I also like if
              types have a namespace attached to them, with methods and associated functions. I am pragmatic like that.
            
              What I like about Rust is that it gives me the best out of both worlds here: You get traits and type
              inference without needing to wrap your head around a fully functional, immutable, lazy, monad-driven
              programming paradigm, and you get methods and associated values without the absolute dumpster fire of
              complex inheritance chains or AbstractBeanFactoryConstructors.
            
              I’ve not seen any other language that even comes close to the convenience of Rust’s pipelines, and its
              lack of higher-kinded types or inheritance did not stop it. Quite the opposite, if anything.
            
              I like pipelining. That’s the one thing that definitely should be obvious if you’ve read all the way
              through this article.
            I just think they’re neat, y’know?I like reading my code top-to-bottom, left-to-right instead of from-the-inside-to-the-outside.
              I like when I don’t need to count arguments and parentheses to figure out which value is the first
              argument of the second function, and which is the second argument of the first function.
            
              I like when my editor can show me all fields of a struct, and all methods or functions associated with a
              value, just when I press  on my keyboard. It’s great.
            
              I like when  and the  layer of the code repository don’t look like
              complete ass.
            
              I like when adding a function call in the middle of a process doesn’t require me to parse the whole line
              to add the closing parenthesis, and doesn’t require me to adjust the nesting of the whole block.
            
              I like when my functions distinguish between ‘a main value which we are acting upon’ and ‘secondary
              arguments’, as opposed to treating them all as the same.
            
              I like when I don’t have to pollute my namespaces with a ton of helper variables or free-standing
              functions that I had to pull in from somewhere.
            
              If you’re writing pipelined code—and not trying overly hard to fit everything into a single, convoluted,
              nested pipeline—then your functions will naturally split up into a few pipeline chunks.
            
              Each chunk starts with a piece of ‘main data’ that travels on a conveyer belt, where every line performs
              exactly one action to transform it. Finally, a single value comes out at the end and gets its own name, so
              that it may be used later.
            
              And that is—in my humble opinion—exactly how it should be. Neat, convenient, separated ‘chunks’, each of
              which can easily be understood in its own right.
            Thanks to kreest for proofreading this article.]]></content:encoded></item><item><title>Pipelining might be my favorite programming language feature</title><link>https://herecomesthemoon.net/2025/04/pipelining/</link><author>/u/SophisticatedAdults</author><category>dev</category><category>reddit</category><pubDate>Mon, 21 Apr 2025 12:20:56 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[ Don’t take it too seriously. Or do. idk, I can’t stop you.Pipelining might be my favorite programming language feature.
              What is pipelining? Pipelining is the feature that allows you to omit a single argument from your
              parameter list, by instead passing the previous value.
            When I say pipelining, I’m talking about the ability to write code like this:
              As opposed to code like this. (This is not real Rust code. Quick challenge for the curious Rustacean, can
              you explain why we cannot rewrite the above code like this, even if we import all of the symbols?)
            
              I honestly feel like this should be so obvious that it shouldn’t even be up for debate. The first code
              example—with its nice ‘pipelining’ or ‘method chaining’ or whatever you want to call it—it
              . It can be read line-by-line. It’s easy to annotate it with comments. It doesn’t
              require introduction of new variables to become more readable since it’s already readable as is.
            
              As opposed to, y’know,
              the first word in the line describing the final action our function performs.
            
              Let me make it very clear: This is an  about syntax. In practice,
              semantics beat syntax every day of the week. In other words, don’t take it too seriously.
            
              Second, this is not about imperative vs. functional programming. This article takes for granted that
              you’re already on board with concepts such as ‘map’ and ‘filter’. It’s possible to overuse that style, but
              I won’t talk about it here.
            You already agree with me
              Here is a feature that’s so bog-standard in modern programming languages that it barely feels like a
              feature at all. Member access for structs or classes with our beloved friend the -operator.
            
              This is a form of pipelining. It puts the data first, the operator in the middle, and concludes with the
              action (restricting to a member field). That’s an instance of what I call pipelining.
            
              You see what I am getting at, right? It’s the same principle. One of the reasons why
              -style member access syntax (and -style method call syntax!) is popular
              is since it’s easy to read and chains easily.
            
              Let’s make the comparison slightly more fair, and pretend that we have to write .
              Compare:
            
              Which one of these is easier to read? The pipelined syntax, obviously. This example is easy to parse
              either way, but imagine you’d like to blend out some information and purely focus on the final operation.
            
              You see the problem, right? In the first example, we have ‘all of the previous stuff’ and then
               to it. In the second example, the operation which we want to perform
              () and the new operand () are spread out with ‘all of the previous stuff’
              sitting between them.
            Looking back at our original example, the problem should be obvious:
              I cannot deny the allegations: I just don’t think it makes sense to write code like that as long as a
              clearly better option exists.
            
              Why would I have to parse the whole line just to figure out where my input comes in, and why is the data
              flow ‘from the inside to the outside’? It’s kind of silly, if you ask me.
            
              Readability is nice, and I could add add a whole section complaining about the mess that’s Python’s
              ‘functional’ features.
            
              However, let’s take a step back and talk about ease of editing. Going back to the example above, imagine
              you’d like to add another  (or any other function call) in the middle there. How easy is
              this?
            
                You’ll have to parse through the line, counting commas and parentheses to find the exact place to add
                the closing parenthesis.
              
                The  of this is going to be basically unreadable, everything is crammed onto one
                line.
              This line is getting long and unreadable, and at that point you’ll want to refactor it anyway!
              This is adding a single line of code. No parentheses counting. It’s easy and obvious. It’s easy to write
              and easy to review. Perhaps most importantly, it shows up  in the
               layer of whatever editor or code exploration tool you’re using.
            
              You might think that this issue is  about trying to cram everything onto a single line, but
              frankly, trying to move away from that doesn’t help much. It will still mess up your git diffs and the
              blame layer.
            
              You can, of course, just assign the result of every  and  call to a
              helper variable, and I will (begrudgingly) acknowledge that that works, and is
               better than trying to do absurd levels of nesting.
            
              When you press  in your IDE, it will show a neat little pop-up that tells you which methods
              you can call or which fields you can access.
            
              This is probably the single IDE feature with the biggest value add, and if not that, then at least the
              single most frequently used one. Some people will tell you that static analysis for namespace or
              module-level code discovery is useless in the age of AI autocompletion and vibe coding, but I very much
              disagree.
                “grug very like type systems make programming easier. for grug, type systems most value when grug hit
                dot on keyboard and list of things grug can do pop up magic. this 90% of value of type system or more to
                grug” — grug
              
              Words to live by. What he’s describing here is something that essentially  pipelining to
              work at all. (And types or type annotation, but having those is the direction the industry is moving in
              anyway.)
            
              It doesn’t matter if it’s the trusty  operator, C++’s , or if it’s
              something more bespoke such as Elm’s or Gleam’s  or Haskell’s . In the
              end, it’s a pipeline operator—the same principle applies. If your
              LSP knows the type of
              what’s on the left, it  in principle be able to offer suggestions for what to do next.
            
              If your favorite language’s LSP/IDE does a poor job at offering suggestions during pipelining, then it’s
              probably one of the following reasons:
            
                You don’t know which type you’re even holding. This happens most often when the language is dynamically
                typed, ’types’ are hard to deduce with static analysis, and you’re touching/writing code without type
                annotations. (e.g. Python)
              
                The ecosystem and LSP just didn’t have enough time put into them, or most active users don’t care
                enough. (e.g. any sufficiently obscure language)
              
                You are in a situation in which even looking up which methods are available is hard, often due to a
                bespoke build process that confuses the editor. (e.g. basically any build or runtime generation of code,
                or bespoke loading/selection of libraries).
              
              In either case, great editor/LSP support is more or less considered mandatory for modern programming
              languages. And of course, this is where pipelining shines.
            
              Ask any IDE, autocompleting fizz.bu... -> fizz.buzz() is  than
              autocompleting , for the obvious reason that you
              didn’t even write  in the second example yet, so your editor has less
              information to work with.
            
              Pipelining is  at data processing, and allows you to transform code that’s commonly
              written with ‘inside-out’ control flow into ’line-by-line’ transformations.
            
              Where could this possibly be more clear than in SQL, the presumably single most significant language for
              querying and aggregating complex large-scale datasets?
            
              You’ll be pleased to hear that, yes, people are in fact working on bringing pipelining to SQL. (Whether
              it’s actually going to happen in this specific form
              is a different question,
              let’s not get too carried away here.)
            
              Unless you’re one of those people who spends so much time dealing with SQL that it’s become second nature,
              and the thought that the control flow of nested queries is hard to follow for the average non-database
              engineer is incomprehensible to you, I guess.
            I’ll put their example of how a standard nested query can be simplified here, for convenience:Versus the SQL Syntax she told you not to worry about:
              Less nesting. More aligned with other languages and
              LINQ. Can easily be
              read line-by-line.
            
              Here’s a more
              skeptical voice (warning, LinkedIn!). Franck Pachot raises the great point that the  statement at the top of a query is
              (essentially) its function signature and specifies the return type. With pipe syntax, you lose some of
              this readability.
            I agree, but that seems like a solvable problem to me.
              And—surprise, surprise—it fits pretty well into pipelining. Any situation where you need to construct a
              complex, stateful object (e.g. a client or runtime), it’s a great way to feed complex, optional arguments
              into an object.
            
              Some people say they prefer optional/named arguments, but honestly, I don’t understand why: An optional
              named  parameter is harder to track down in code (and harder to mark as deprecated!) than
              all instances of a  builder function.
            
              If you have no clue what I’m talking about, this here is the type of pattern I’m talking about. You have a
              ‘builder’ object, call some methods on it to configure it, and finally  the object
              you’re actually interested in.
            Making Haskell (slightly more) readable
              It has these weird operators like , , , or
               and when you ask Haskell programmers about what they mean, they say something like
              “Oh, this is just a special case of the generalized
              Kleisli Monad Operator in the category of endo-pro-applicatives over a locally small poset.” and your eyes
              have glazed over before they’ve even finished the sentence.
            (It also doesn’t help that Haskell allows you to define custom operators however you please, yes.)
              If you’re wondering “How could a language have so many bespoke operators?”, my understanding is that most
              of them are just fancy ways of telling Haskell to compose some functions in a highly advanced way. Here’s
              the second-most basic
              example, the  operator.
            
              Imagine you have functions , , and some value . In a
              “““normal””” language you might write . In Haskell, this is written as
              . This is since  will automatically ‘grab’ values to the right as its
              arguments, so you don’t need the parentheses.
            
              A consequence of this is that  is written as  in
              Haskell. If you wrote , the compiler will interpret it as
              , which would be wrong. This is what people mean when they say that Haskell’s
              function call syntax is
              left-associative.
            
              The  operator is nothing but syntactic sugar that allows you to write
               instead of having to write . That’s it. People were
              fed-up with having to put parens everywhere, I guess.
            If your eyes glazed over at this point, I can’t blame you.
              Talking about any of the fancier operators would be punching well above my weight-class, so I’ll just
              stick to what I’ve been saying throughout this entire post already. Here’s a stilted Haskell toy example,
              intentionally not written in
              pointfree style.
            
              If you want to figure out the flow of data, this whole function body has to be read
              .
            
              To make things even funnier, you need to start with the  clause to figure out which
              local “variables” are being defined. This happens (for whatever reason) at the end of the function instead
              of at the start. (Calling  a variable is misleading, but that’s besides the
              point.)
            
              At this point you might wonder if Haskell has some sort of pipelining operator, and yes, it turns out that
              one was
              added in 2014! That’s pretty late considering that Haskell exists since 1990. This allows us to refactor the above
              code as follows:
            Isn’t that way easier to read? is code which you can show to an enterprise Java programmer, tell them that they’re looking
              at
              Java Streams
              with slightly weird syntax, and they’ll get the idea.
            
              Of course, in reality nothing is as simple. The Haskell ecosystem seems to be split between users of
              , users of , and users of the
              Flow-provided operators, which allow the same functionality, but allow you to write
               instead of .
              I don’t know what to say about that, other than that—not entirely unlike C++—Haskell has its own share of
              operator-related and cultural historical baggage, and a split ecosystem, and this makes the language
              significantly less approachable than it has to be.
            Rust’s pipelining is pretty neat
              In the beginning I said that ‘Pipelining is the feature that allows you to omit a single argument from
              your parameter list, by instead passing the previous value.’
            
              I still think that this is true, but it doesn’t get across the whole picture. If you’ve paid attention in
              the previous sections, you’ll have noticed that  and
               share basically  in common outside of the order of
              operations.
            
              In the first case, we’re accessing a value that’s  to the object. In the second, we’re
              ‘just’ passing an expression to a free-standing function.
            
              Or in other words, pipelining is not the same as pipelining. Even from an IDE-perspective, they’re
              different. In Java, your editor will look for methods associated with an object and walk up the
              inheritance chain. In Haskell, your editor will put a so-called
              ’typed hole’, and try to deduce which functions have a type that ‘fits’ into the hole using
              Hindley-Milner Type Inference.
            
              Personally, I like type inference (and
              type classes), but I also like if
              types have a namespace attached to them, with methods and associated functions. I am pragmatic like that.
            
              What I like about Rust is that it gives me the best out of both worlds here: You get traits and type
              inference without needing to wrap your head around a fully functional, immutable, lazy, monad-driven
              programming paradigm, and you get methods and associated values without the absolute dumpster fire of
              complex inheritance chains or AbstractBeanFactoryConstructors.
            
              I’ve not seen any other language that even comes close to the convenience of Rust’s pipelines, and its
              lack of higher-kinded types or inheritance did not stop it. Quite the opposite, if anything.
            
              I like pipelining. That’s the one thing that definitely should be obvious if you’ve read all the way
              through this article.
            I just think they’re neat, y’know?I like reading my code top-to-bottom, left-to-right instead of from-the-inside-to-the-outside.
              I like when I don’t need to count arguments and parentheses to figure out which value is the first
              argument of the second function, and which is the second argument of the first function.
            
              I like when my editor can show me all fields of a struct, and all methods or functions associated with a
              value, just when I press  on my keyboard. It’s great.
            
              I like when  and the  layer of the code repository don’t look like
              complete ass.
            
              I like when adding a function call in the middle of a process doesn’t require me to parse the whole line
              to add the closing parenthesis, and doesn’t require me to adjust the nesting of the whole block.
            
              I like when my functions distinguish between ‘a main value which we are acting upon’ and ‘secondary
              arguments’, as opposed to treating them all as the same.
            
              I like when I don’t have to pollute my namespaces with a ton of helper variables or free-standing
              functions that I had to pull in from somewhere.
            
              If you’re writing pipelined code—and not trying overly hard to fit everything into a single, convoluted,
              nested pipeline—then your functions will naturally split up into a few pipeline chunks.
            
              Each chunk starts with a piece of ‘main data’ that travels on a conveyer belt, where every line performs
              exactly one action to transform it. Finally, a single value comes out at the end and gets its own name, so
              that it may be used later.
            
              And that is—in my humble opinion—exactly how it should be. Neat, convenient, separated ‘chunks’, each of
              which can easily be understood in its own right.
            Thanks to kreest for proofreading this article.]]></content:encoded></item><item><title>Getting Forked by Microsoft</title><link>https://philiplaine.com/posts/getting-forked-by-microsoft/</link><author>/u/starlevel01</author><category>dev</category><category>reddit</category><pubDate>Mon, 21 Apr 2025 12:19:59 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Three years ago, I was part of a team responsible for developing and maintaining Kubernetes clusters for end user customers. A main source for downtime in customer environments occurred when image registries went down. The traditional way to solve this problem is to set up a stateful mirror, however we had to work within customer budget and time constraints which did not allow it. During a Black Friday, we started getting hit with a ton of traffic while GitHub container registries were down. This limited our ability to scale up the cluster as we depended on critical images from that registry. After this incident, I started thinking about a better way to avoid these scalability issues. A solution that did not need a stateful component and required minimal operational oversight. This is where the idea for Spegel came from.As a sole maintainer of an open source project, I was enthused when Microsoft reached out to set up a meeting to talk about Spegel. The meeting went well, and I felt there was going to be a path forward ripe with cooperation and hopefully a place where I could onboard new maintainers. I continued discussions with one of the Microsoft engineers, helping them get Spegel running and answering any architecture questions they had. At the time I was positive as I saw it as a possibility for Micorosft to contribute back changes based on their learnings. As time went on, silence ensued, and I assumed work priorities had changed.It was not until KubeCon Paris where I attended a talk that piqued my interest. The talk was about strategies to speed up image distribution where one strategy discussed was P2P sharing. The topics in the abstract sounded similar to Spegel so I was excited to hear other’s ideas about the problem. During the talk, I was enthralled seeing Spegel, my own project, be discussed as a P2P image sharing solution. When Peerd, a peer to peer distributor of container content in Kubernetes clusters made by Microsoft, was mentioned I quickly researched it. At the bottom of the README there was a thank you to myself and Spegel. This acknowledgement made it look like they had taken some inspiration from my project and gone ahead and developed a version of their own.While looking into Peerd, my enthusiasm for understanding different approaches in this problem space quickly diminished. I saw function signatures and comments that looked very familiar, as if I had written them myself. Digging deeper I found test cases referencing Spegel and my previous employer, test cases that have been taken directly from my project. References that are still present to this day. The project is a forked version of Spegel, maintained by Microsoft, but under Microsoft’s MIT license.Spegel was published with an MIT license. Software released under an MIT license allows for forking and modifications, without any requirement to contribute these changes back. I default to using the MIT license as it is simple and permissive. The license does not allow removing the original license and purport that the code was created by someone else. It looks as if large parts of the project were copied directly from Spegel without any mention of the original source. I have included a short snippet comparing the code which adds the mirror configuration where even the function comments are the same.A negative impact from the creation of Peerd is that it has created confusion among new users. I am frequently asked about the differences between Spegel and Peerd. As a maintainer, it is my duty to come across as unbiased and factual as possible, but this tumultuous history makes it challenging. Microsoft carries a large brand recognition, so it has been difficult for Spegel to try and take up space next to such a behemoth.As an open source maintainer I have dedicated ample time to community requests, bug fixes, and security fixes. In my conversation with Microsoft I was open to collaboration to continue building out a tool to benefit the open source community. Over the years I have contributed to a multitude of open source projects and created a few of my own. Spegel was the first project I created from the ground up that got some traction and seemed to be appreciated by the community. Seeing my project being forked by Microsoft made me feel like I was no longer useful. For a while I questioned if it was even worth continuing working on Spegel.Luckily, I persisted. Spegel still continues strong with over 1.7k stars and 14.4 million pulls since its first release over two years ago. However, I am not the first and unfortunately not the last person to come across this David versus Goliath-esque experience. How can sole maintainers work with multi-billion corporations without being taken advantage of? With the changes of Hashicorp licensing having a rippling effect through the open source community, along with the strong decline in investment in open source as a whole, how does the community prevail? As an effort to fund the work on Spegel I have enabled GitHub sponsors. This experience has also made me consider changing the license of Spegel, as it seems to be the only stone I can throw.]]></content:encoded></item><item><title>Pipelining might be my favorite programming language feature</title><link>https://herecomesthemoon.net/2025/04/pipelining/</link><author>Mond_</author><category>dev</category><category>hn</category><pubDate>Mon, 21 Apr 2025 12:16:16 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[ Don’t take it too seriously. Or do. idk, I can’t stop you.Pipelining might be my favorite programming language feature.
              What is pipelining? Pipelining is the feature that allows you to omit a single argument from your
              parameter list, by instead passing the previous value.
            When I say pipelining, I’m talking about the ability to write code like this:
              As opposed to code like this. (This is not real Rust code. Quick challenge for the curious Rustacean, can
              you explain why we cannot rewrite the above code like this, even if we import all of the symbols?)
            
              I honestly feel like this should be so obvious that it shouldn’t even be up for debate. The first code
              example—with its nice ‘pipelining’ or ‘method chaining’ or whatever you want to call it—it
              . It can be read line-by-line. It’s easy to annotate it with comments. It doesn’t
              require introduction of new variables to become more readable since it’s already readable as is.
            
              As opposed to, y’know,
              the first word in the line describing the final action our function performs.
            
              Let me make it very clear: This is an  about syntax. In practice,
              semantics beat syntax every day of the week. In other words, don’t take it too seriously.
            
              Second, this is not about imperative vs. functional programming. This article takes for granted that
              you’re already on board with concepts such as ‘map’ and ‘filter’. It’s possible to overuse that style, but
              I won’t talk about it here.
            You already agree with me
              Here is a feature that’s so bog-standard in modern programming languages that it barely feels like a
              feature at all. Member access for structs or classes with our beloved friend the -operator.
            
              This is a form of pipelining. It puts the data first, the operator in the middle, and concludes with the
              action (restricting to a member field). That’s an instance of what I call pipelining.
            
              You see what I am getting at, right? It’s the same principle. One of the reasons why
              -style member access syntax (and -style method call syntax!) is popular
              is since it’s easy to read and chains easily.
            
              Let’s make the comparison slightly more fair, and pretend that we have to write .
              Compare:
            
              Which one of these is easier to read? The pipelined syntax, obviously. This example is easy to parse
              either way, but imagine you’d like to blend out some information and purely focus on the final operation.
            
              You see the problem, right? In the first example, we have ‘all of the previous stuff’ and then
               to it. In the second example, the operation which we want to perform
              () and the new operand () are spread out with ‘all of the previous stuff’
              sitting between them.
            Looking back at our original example, the problem should be obvious:
              I cannot deny the allegations: I just don’t think it makes sense to write code like that as long as a
              clearly better option exists.
            
              Why would I have to parse the whole line just to figure out where my input comes in, and why is the data
              flow ‘from the inside to the outside’? It’s kind of silly, if you ask me.
            
              Readability is nice, and I could add add a whole section complaining about the mess that’s Python’s
              ‘functional’ features.
            
              However, let’s take a step back and talk about ease of editing. Going back to the example above, imagine
              you’d like to add another  (or any other function call) in the middle there. How easy is
              this?
            
                You’ll have to parse through the line, counting commas and parentheses to find the exact place to add
                the closing parenthesis.
              
                The  of this is going to be basically unreadable, everything is crammed onto one
                line.
              This line is getting long and unreadable, and at that point you’ll want to refactor it anyway!
              This is adding a single line of code. No parentheses counting. It’s easy and obvious. It’s easy to write
              and easy to review. Perhaps most importantly, it shows up  in the
               layer of whatever editor or code exploration tool you’re using.
            
              You might think that this issue is  about trying to cram everything onto a single line, but
              frankly, trying to move away from that doesn’t help much. It will still mess up your git diffs and the
              blame layer.
            
              You can, of course, just assign the result of every  and  call to a
              helper variable, and I will (begrudgingly) acknowledge that that works, and is
               better than trying to do absurd levels of nesting.
            
              When you press  in your IDE, it will show a neat little pop-up that tells you which methods
              you can call or which fields you can access.
            
              This is probably the single IDE feature with the biggest value add, and if not that, then at least the
              single most frequently used one. Some people will tell you that static analysis for namespace or
              module-level code discovery is useless in the age of AI autocompletion and vibe coding, but I very much
              disagree.
                “grug very like type systems make programming easier. for grug, type systems most value when grug hit
                dot on keyboard and list of things grug can do pop up magic. this 90% of value of type system or more to
                grug” — grug
              
              Words to live by. What he’s describing here is something that essentially  pipelining to
              work at all. (And types or type annotation, but having those is the direction the industry is moving in
              anyway.)
            
              It doesn’t matter if it’s the trusty  operator, C++’s , or if it’s
              something more bespoke such as Elm’s or Gleam’s  or Haskell’s . In the
              end, it’s a pipeline operator—the same principle applies. If your
              LSP knows the type of
              what’s on the left, it  in principle be able to offer suggestions for what to do next.
            
              If your favorite language’s LSP/IDE does a poor job at offering suggestions during pipelining, then it’s
              probably one of the following reasons:
            
                You don’t know which type you’re even holding. This happens most often when the language is dynamically
                typed, ’types’ are hard to deduce with static analysis, and you’re touching/writing code without type
                annotations. (e.g. Python)
              
                The ecosystem and LSP just didn’t have enough time put into them, or most active users don’t care
                enough. (e.g. any sufficiently obscure language)
              
                You are in a situation in which even looking up which methods are available is hard, often due to a
                bespoke build process that confuses the editor. (e.g. basically any build or runtime generation of code,
                or bespoke loading/selection of libraries).
              
              In either case, great editor/LSP support is more or less considered mandatory for modern programming
              languages. And of course, this is where pipelining shines.
            
              Ask any IDE, autocompleting fizz.bu... -> fizz.buzz() is  than
              autocompleting , for the obvious reason that you
              didn’t even write  in the second example yet, so your editor has less
              information to work with.
            
              Pipelining is  at data processing, and allows you to transform code that’s commonly
              written with ‘inside-out’ control flow into ’line-by-line’ transformations.
            
              Where could this possibly be more clear than in SQL, the presumably single most significant language for
              querying and aggregating complex large-scale datasets?
            
              You’ll be pleased to hear that, yes, people are in fact working on bringing pipelining to SQL. (Whether
              it’s actually going to happen in this specific form
              is a different question,
              let’s not get too carried away here.)
            
              Unless you’re one of those people who spends so much time dealing with SQL that it’s become second nature,
              and the thought that the control flow of nested queries is hard to follow for the average non-database
              engineer is incomprehensible to you, I guess.
            I’ll put their example of how a standard nested query can be simplified here, for convenience:Versus the SQL Syntax she told you not to worry about:
              Less nesting. More aligned with other languages and
              LINQ. Can easily be
              read line-by-line.
            
              Here’s a more
              skeptical voice (warning, LinkedIn!). Franck Pachot raises the great point that the  statement at the top of a query is
              (essentially) its function signature and specifies the return type. With pipe syntax, you lose some of
              this readability.
            I agree, but that seems like a solvable problem to me.
              And—surprise, surprise—it fits pretty well into pipelining. Any situation where you need to construct a
              complex, stateful object (e.g. a client or runtime), it’s a great way to feed complex, optional arguments
              into an object.
            
              Some people say they prefer optional/named arguments, but honestly, I don’t understand why: An optional
              named  parameter is harder to track down in code (and harder to mark as deprecated!) than
              all instances of a  builder function.
            
              If you have no clue what I’m talking about, this here is the type of pattern I’m talking about. You have a
              ‘builder’ object, call some methods on it to configure it, and finally  the object
              you’re actually interested in.
            Making Haskell (slightly more) readable
              It has these weird operators like , , , or
               and when you ask Haskell programmers about what they mean, they say something like
              “Oh, this is just a special case of the generalized
              Kleisli Monad Operator in the category of endo-pro-applicatives over a locally small poset.” and your eyes
              have glazed over before they’ve even finished the sentence.
            (It also doesn’t help that Haskell allows you to define custom operators however you please, yes.)
              If you’re wondering “How could a language have so many bespoke operators?”, my understanding is that most
              of them are just fancy ways of telling Haskell to compose some functions in a highly advanced way. Here’s
              the second-most basic
              example, the  operator.
            
              Imagine you have functions , , and some value . In a
              “““normal””” language you might write . In Haskell, this is written as
              . This is since  will automatically ‘grab’ values to the right as its
              arguments, so you don’t need the parentheses.
            
              A consequence of this is that  is written as  in
              Haskell. If you wrote , the compiler will interpret it as
              , which would be wrong. This is what people mean when they say that Haskell’s
              function call syntax is
              left-associative.
            
              The  operator is nothing but syntactic sugar that allows you to write
               instead of having to write . That’s it. People were
              fed-up with having to put parens everywhere, I guess.
            If your eyes glazed over at this point, I can’t blame you.
              Talking about any of the fancier operators would be punching well above my weight-class, so I’ll just
              stick to what I’ve been saying throughout this entire post already. Here’s a stilted Haskell toy example,
              intentionally not written in
              pointfree style.
            
              If you want to figure out the flow of data, this whole function body has to be read
              .
            
              To make things even funnier, you need to start with the  clause to figure out which
              local “variables” are being defined. This happens (for whatever reason) at the end of the function instead
              of at the start. (Calling  a variable is misleading, but that’s besides the
              point.)
            
              At this point you might wonder if Haskell has some sort of pipelining operator, and yes, it turns out that
              one was
              added in 2014! That’s pretty late considering that Haskell exists since 1990. This allows us to refactor the above
              code as follows:
            Isn’t that way easier to read? is code which you can show to an enterprise Java programmer, tell them that they’re looking
              at
              Java Streams
              with slightly weird syntax, and they’ll get the idea.
            
              Of course, in reality nothing is as simple. The Haskell ecosystem seems to be split between users of
              , users of , and users of the
              Flow-provided operators, which allow the same functionality, but allow you to write
               instead of .
              I don’t know what to say about that, other than that—not entirely unlike C++—Haskell has its own share of
              operator-related and cultural historical baggage, and a split ecosystem, and this makes the language
              significantly less approachable than it has to be.
            Rust’s pipelining is pretty neat
              In the beginning I said that ‘Pipelining is the feature that allows you to omit a single argument from
              your parameter list, by instead passing the previous value.’
            
              I still think that this is true, but it doesn’t get across the whole picture. If you’ve paid attention in
              the previous sections, you’ll have noticed that  and
               share basically  in common outside of the order of
              operations.
            
              In the first case, we’re accessing a value that’s  to the object. In the second, we’re
              ‘just’ passing an expression to a free-standing function.
            
              Or in other words, pipelining is not the same as pipelining. Even from an IDE-perspective, they’re
              different. In Java, your editor will look for methods associated with an object and walk up the
              inheritance chain. In Haskell, your editor will put a so-called
              ’typed hole’, and try to deduce which functions have a type that ‘fits’ into the hole using
              Hindley-Milner Type Inference.
            
              Personally, I like type inference (and
              type classes), but I also like if
              types have a namespace attached to them, with methods and associated functions. I am pragmatic like that.
            
              What I like about Rust is that it gives me the best out of both worlds here: You get traits and type
              inference without needing to wrap your head around a fully functional, immutable, lazy, monad-driven
              programming paradigm, and you get methods and associated values without the absolute dumpster fire of
              complex inheritance chains or AbstractBeanFactoryConstructors.
            
              I’ve not seen any other language that even comes close to the convenience of Rust’s pipelines, and its
              lack of higher-kinded types or inheritance did not stop it. Quite the opposite, if anything.
            
              I like pipelining. That’s the one thing that definitely should be obvious if you’ve read all the way
              through this article.
            I just think they’re neat, y’know?I like reading my code top-to-bottom, left-to-right instead of from-the-inside-to-the-outside.
              I like when I don’t need to count arguments and parentheses to figure out which value is the first
              argument of the second function, and which is the second argument of the first function.
            
              I like when my editor can show me all fields of a struct, and all methods or functions associated with a
              value, just when I press  on my keyboard. It’s great.
            
              I like when  and the  layer of the code repository don’t look like
              complete ass.
            
              I like when adding a function call in the middle of a process doesn’t require me to parse the whole line
              to add the closing parenthesis, and doesn’t require me to adjust the nesting of the whole block.
            
              I like when my functions distinguish between ‘a main value which we are acting upon’ and ‘secondary
              arguments’, as opposed to treating them all as the same.
            
              I like when I don’t have to pollute my namespaces with a ton of helper variables or free-standing
              functions that I had to pull in from somewhere.
            
              If you’re writing pipelined code—and not trying overly hard to fit everything into a single, convoluted,
              nested pipeline—then your functions will naturally split up into a few pipeline chunks.
            
              Each chunk starts with a piece of ‘main data’ that travels on a conveyer belt, where every line performs
              exactly one action to transform it. Finally, a single value comes out at the end and gets its own name, so
              that it may be used later.
            
              And that is—in my humble opinion—exactly how it should be. Neat, convenient, separated ‘chunks’, each of
              which can easily be understood in its own right.
            Thanks to kreest for proofreading this article.]]></content:encoded></item><item><title>Show HN: Nerdlog – Fast, multi-host TUI log viewer with timeline histogram</title><link>https://github.com/dimonomid/nerdlog</link><author>dimonomid</author><category>dev</category><category>hn</category><pubDate>Mon, 21 Apr 2025 11:38:10 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Getting forked by Microsoft</title><link>https://philiplaine.com/posts/getting-forked-by-microsoft/</link><author>phillebaba</author><category>dev</category><category>hn</category><pubDate>Mon, 21 Apr 2025 11:05:44 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Three years ago, I was part of a team responsible for developing and maintaining Kubernetes clusters for end user customers. A main source for downtime in customer environments occurred when image registries went down. The traditional way to solve this problem is to set up a stateful mirror, however we had to work within customer budget and time constraints which did not allow it. During a Black Friday, we started getting hit with a ton of traffic while GitHub container registries were down. This limited our ability to scale up the cluster as we depended on critical images from that registry. After this incident, I started thinking about a better way to avoid these scalability issues. A solution that did not need a stateful component and required minimal operational oversight. This is where the idea for Spegel came from.As a sole maintainer of an open source project, I was enthused when Microsoft reached out to set up a meeting to talk about Spegel. The meeting went well, and I felt there was going to be a path forward ripe with cooperation and hopefully a place where I could onboard new maintainers. I continued discussions with one of the Microsoft engineers, helping them get Spegel running and answering any architecture questions they had. At the time I was positive as I saw it as a possibility for Micorosft to contribute back changes based on their learnings. As time went on, silence ensued, and I assumed work priorities had changed.It was not until KubeCon Paris where I attended a talk that piqued my interest. The talk was about strategies to speed up image distribution where one strategy discussed was P2P sharing. The topics in the abstract sounded similar to Spegel so I was excited to hear other’s ideas about the problem. During the talk, I was enthralled seeing Spegel, my own project, be discussed as a P2P image sharing solution. When Peerd, a peer to peer distributor of container content in Kubernetes clusters made by Microsoft, was mentioned I quickly researched it. At the bottom of the README there was a thank you to myself and Spegel. This acknowledgement made it look like they had taken some inspiration from my project and gone ahead and developed a version of their own.While looking into Peerd, my enthusiasm for understanding different approaches in this problem space quickly diminished. I saw function signatures and comments that looked very familiar, as if I had written them myself. Digging deeper I found test cases referencing Spegel and my previous employer, test cases that have been taken directly from my project. References that are still present to this day. The project is a forked version of Spegel, maintained by Microsoft, but under Microsoft’s MIT license.Spegel was published with an MIT license. Software released under an MIT license allows for forking and modifications, without any requirement to contribute these changes back. I default to using the MIT license as it is simple and permissive. The license does not allow removing the original license and purport that the code was created by someone else. It looks as if large parts of the project were copied directly from Spegel without any mention of the original source. I have included a short snippet comparing the code which adds the mirror configuration where even the function comments are the same.A negative impact from the creation of Peerd is that it has created confusion among new users. I am frequently asked about the differences between Spegel and Peerd. As a maintainer, it is my duty to come across as unbiased and factual as possible, but this tumultuous history makes it challenging. Microsoft carries a large brand recognition, so it has been difficult for Spegel to try and take up space next to such a behemoth.As an open source maintainer I have dedicated ample time to community requests, bug fixes, and security fixes. In my conversation with Microsoft I was open to collaboration to continue building out a tool to benefit the open source community. Over the years I have contributed to a multitude of open source projects and created a few of my own. Spegel was the first project I created from the ground up that got some traction and seemed to be appreciated by the community. Seeing my project being forked by Microsoft made me feel like I was no longer useful. For a while I questioned if it was even worth continuing working on Spegel.Luckily, I persisted. Spegel still continues strong with over 1.7k stars and 14.4 million pulls since its first release over two years ago. However, I am not the first and unfortunately not the last person to come across this David versus Goliath-esque experience. How can sole maintainers work with multi-billion corporations without being taken advantage of? With the changes of Hashicorp licensing having a rippling effect through the open source community, along with the strong decline in investment in open source as a whole, how does the community prevail? As an effort to fund the work on Spegel I have enabled GitHub sponsors. This experience has also made me consider changing the license of Spegel, as it seems to be the only stone I can throw.]]></content:encoded></item><item><title>Ask r/kubernetes: What are you working on this week?</title><link>https://www.reddit.com/r/kubernetes/comments/1k49ttq/ask_rkubernetes_what_are_you_working_on_this_week/</link><author>/u/gctaylor</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Mon, 21 Apr 2025 10:00:34 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[What are you up to with Kubernetes this week? Evaluating a new tool? In the process of adopting? Working on an open source project or contribution? Tell /r/kubernetes what you're up to this week!]]></content:encoded></item><item><title>Android 16 lets the Linux Terminal use your phone&apos;s entire storage -- &quot;With the latest Android 16 beta, you can now allocate as much storage as you want to the Linux Terminal&quot;</title><link>https://www.androidauthority.com/android-16-terminal-disk-resize-3546144/</link><author>/u/throwaway16830261</author><category>dev</category><category>reddit</category><pubDate>Mon, 21 Apr 2025 09:37:09 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Pixel phones now feature a Linux Terminal app, enabling users to run a Debian virtual machine for desktop apps alongside Android ones.While still lacking features like GUI/audio, the recent Android 16 Beta 4 release removes the previous 16GB storage cap for the Linux VM.Google plans to eventually replace manual storage resizing with dynamic ballooning, allowing the VM storage to adjust automatically based on need.Pixel phones are taking a step towards becoming powerful portable PCs with the introduction of the Linux Terminal app in the March 2025 update. This app lets compatible Android devices run a full-fledged instance of the popular Debian distro in a virtual machine, opening the door to running popular Linux desktop programs alongside Android apps. However, the feature isn’t quite ready for everyday professional use, currently lacking support for critical features like graphical apps and audio output, and until recently, it was restricted to just 16GB of storage space, limiting the Linux VM’s capacity for apps and files. Thankfully, Google just lifted that storage limitation, allowing the Linux Terminal to use as much of your phone’s storage as needed.
You’re reading an  story. Discover Authority Insights for more exclusive reports, app teardowns, leaks, and in-depth tech coverage you won’t find anywhere else.With the release of the fourth Android 16 beta, Google has uncapped the disk resize slider in the Linux Terminal app’s settings. In previous releases, the disk size was capped at 16GB. In Android 16 Beta 4, however, the disk can be resized to occupy most of the host device’s remaining storage, leaving 1GB free to prevent the virtual machine from consuming all available space.I tested this feature on a Google Pixel 9 Pro running Android 16 Beta 4, successfully resizing the Linux disk to 42.3GB. The process took only a few seconds, and after restarting the VM, the increased storage was immediately available. According to Android’s storage settings, the total size consumed by the Linux Terminal app (including the base app and the resized VM disk) grew to approximately 45.52GB.In a future release, Google plans to remove the disk resize slider entirely from the Linux Terminal app. Instead, the app will employ storage ballooning to dynamically adjust the storage space available to the Debian VM. This feature allows the guest VM’s storage to “inflate” to use available space and “deflate” when the host system needs to reclaim it. This approach offers two key benefits: the storage allocated to Debian doesn’t need to be fixed upfront, and it protects the host from running out of space by automatically shrinking the guest’s available storage when necessary.Google is rapidly improving the Linux Terminal app, recognizing its key role in the company’s plans to transition Chrome OS to an Android base. Although the app theoretically allows for running full-fledged Linux desktop environments, Google says the Terminal’s primary purpose is to enable running Linux apps alongside Android apps, not to replace Android’s native desktop mode. Still, Google’s efforts will unlock some exciting use cases, and it’ll be fascinating to see the innovative ways that users leverage the Linux Terminal on their Pixels as the app matures. Email our staff at news@androidauthority.com. You can stay anonymous or get credit for the info, it's your choice.]]></content:encoded></item><item><title>Sesh - Simple persistent session store for Go, powered by BadgerDB</title><link>https://www.reddit.com/r/golang/comments/1k49e12/sesh_simple_persistent_session_store_for_go/</link><author>/u/pthread_mutex_t</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Mon, 21 Apr 2025 09:29:46 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I built Sesh, a really simple session store which uses BadgerDB.Key features: - In memory or persistence - Confirgurable outside of defaults - Cookie and context helpers/middleware to streamline workflowsBasically, I just wanted to understand a bit better how session cookies work and how to abstract away a lot of it. I also wanted something that was simple to undertake and understand.It's probably no gorilla sessions but it works for my use case, so I thought I'd share it in case it's useful for anyone else.Feel free to open issues and for features, bugs, docs, etc. Always looking for opportunities to improve myself!]]></content:encoded></item><item><title>Anyone hiring for rust interns ?</title><link>https://www.reddit.com/r/rust/comments/1k49df9/anyone_hiring_for_rust_interns/</link><author>/u/Recent_Project9124</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Mon, 21 Apr 2025 09:28:32 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[I am a Rust enthusiast with one year of experience building personal system-level projects and I'm actively searching for a remote Rust internship.I have build impressive project like DnsServer and HTTP server using TCP protocol and native networking library. I have designed these systems to be robust and multithreaded.Beyond these i am also familiar with like git and dockerIf your company is hiring Rust interns remotely, I'd love to connect and share more about my work..]]></content:encoded></item><item><title>stal/IX - statically linked, source based, bootstrapped rolling Linux, based on IX package manager</title><link>https://stal-ix.github.io/</link><author>/u/JazzedPineda</author><category>dev</category><category>reddit</category><pubDate>Mon, 21 Apr 2025 08:38:01 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>PostgreSQL JSONB - Powerful Storage for Semi-Structured Data</title><link>https://www.architecture-weekly.com/p/postgresql-jsonb-powerful-storage</link><author>/u/Adventurous-Salt8514</author><category>dev</category><category>reddit</category><pubDate>Mon, 21 Apr 2025 08:35:41 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Object-oriented or relational? You’ve seen this battle in your projects.Object-Relational MappersStill, those databases didn’t give us the consistency guarantees we were comfortable with, like transactions, isolation levels, etc. PostgreSQL added the JSONB column type to answer the question: Then other databases like MySQL and SQLite followed.Let’s discuss today how it works, and if it’s the actual answer.First, terminology: the B in JSON stands for BinaryJSON stores data as text,JSONB uses a binary representationThe binary format of JSONB enables efficient indexing and querying and eliminates the need for repeated parsing. You can think of a JSONB column as a table inside the column. Each path to property can be seen logically as a different column.To visualise this conceptual model, imagine if our database could access any path directly without scanning the entire document:+----+---------------+--------------------------+--------------------+
| id | customer.name | customer.contact.address | email              |
+----+---------------+--------------------------+--------------------+
| 1  | "John Smith"  | "Portland"               | "john@example.com" | 
+----+---------------+--------------------------+--------------------+ Of course, that’s a bit of an oversimplification. PostgreSQL doesn't create these virtual columns, but the binary representation effectively allows it to access paths directly without scanning the whole document. This approach provides several benefits:Path traversal is much faster than parsing textFields can be indexed individuallyQuery performance is more predictableWhen Postgresql stores a JSONB document, it doesn't simply dump JSON text into a field. It transforms the document into a binary representation.Have a look on the following JSON:This hierarchical structure could be flattened into a set of paths for each nested structure:Path: customer.name = "John Smith"
Path: customer.contact.email = "john@example.com"
Path: customer.contact.phone = "555-1234"
Path: customer.contact.address.city = "Portland"
Path: customer.contact.address.country = "USA"
The goal is: to performantly read a specific field without parsing the entire document.JSONB's document is stored as a hierarchical, tree-like structure of key-value pairs, each containing metadata about its type and actual data.When new JSON is stored, it has to be parsed and converted from text to key-value pairs. This conversion happens through a process called "tokenisationFor example, using our customer data example:{
  "id": "cust_web_123",
  "email": "web_user@example.com",
  "source": "website",
  "web_data": {
    "first_visit": "2023-03-12T15:22:47Z",
    "utm_source": "google_search"
  },
  "contact": {
    "phone": "+1234567890"
  }
}
Would be tokenised and stored with internal tree-like structures tracking:Value: Object {
      Count: 1,
      Children: [Of course, it’s a bit simplified form, but you can think that each array element is actually a different node, and the index in the array is just a nested path.PostgreSQL's JSONB tokenisation is sneaky. It not only parses data but also preserves the actual data types of values.ensure that data maintains its semantic meaning,enable type-specific operations (like numeric comparisons),avoid type conversion when not needed.What’s more, it’s not limited to JSON types.customer_data->'customer'->'contact'->'address'->>'city'PostgreSQL can navigate directly to that specific token  without scanning the entire document. It uses the described hierarchical structure, and gets the exact value with the type.When PostgreSQL encounters such query, it:Parses the path into individual segments.Locates the root object or array in the binary structure.Each path segment computes a hash of the key.Uses the hash to look up the corresponding entry in the structure.Navigates to the next level if needed.Extracts and returns the value in the requested format.This algorithm is heavily optimised for typical access patterns. For simple path expressions, PostgreSQL can retrieve values with near-constant time complexity, regardless of document size. However, the performance characteristics become more variable for more complex expressions, especially those involving arrays or filtering.Understanding this internal path representation explains why some JSONB queries perform better than others.To see how JSONB actually works, let's discuss a scenario that manages customer data.Here's how a typical customer model might look in TypeScript:// Different structure depending on acquisition channel
  // At least one of these will be present per customerIf you tried to model it with relational tables, you'd face a classic data modelling challenge. You'd end up with either:A complex set of tables with joins:A wide table with mostly NULL values:With JSONB, you can implement this in PostgreSQL without forcing every customer profile into the same rigid structure:This allows storing profiles with wildly different shapes while keeping common fields queryable. The SQL implementation might look like:INSERT INTO customers (id, email, customer_data) VALUES-- Mobile customer with different structureDespite the varying structures, you can still query across all customer types:customer_data->>'source' AS source,

    COALESCE(
        customer_data->'web_data'->>'first_visit',
        customer_data->'mobile_data'->>'app_version',
        customer_data->'crm_data'->>'account_manager'
    ) AS source_specific_data,
customer_data->'contact'->'address'->>'city' AS city,
jsonb_array_length(customer_data->'purchases') AS purchase_count
FROM customers
WHERE (customer_data->'contact'->'address'->>'country') = 'USA';WHERE (customer_data->'contact'->'address'->>'country') = 'USA'When getting such WHERE query, PostgreSQL does following steps:Extracts the string value at that pathWith a traditional JSON text column, this would require parsing the entire document. With JSONB, PostgreSQL can navigate directly to the relevant portion of the binary structure. The data in the SELECT statement is processed similarly.documentationThanks to the binary, hierarchical structure we just described, PostgreSQL can index JSONB data.Creating a GIN index is straightforward:CREATE INDEX idx_customer_data ON customers USING GIN (customer_data);
When PostgreSQL builds this index, it:Iterates through every JSONB document in the tableExtracts each key and value pair at every level of nestingCreates index entries for each key and for each scalar valueOrganises these entries in a tree structure optimised for lookupsThe resulting index is usefull for certain operations like:customer_data @> '{"tags": ["premium"]}'customer_data ? 'promotion_code'WHERE customer_data ?| array['discount', 'coupon', 'promotion_code']These queries can use the GIN index directly without extracting paths, making them highly efficient even with large datasets. The execution plan would show something like:Bitmap Heap Scan on customers  (cost=4.26..8.27 rows=1 width=32)
  Recheck Cond: (customer_data @> '{"tags": ["premium"]}'::jsonb)
  ->  Bitmap Index Scan on idx_customer_data  (cost=0.00..4.26 rows=1 width=0)
        Index Cond: (customer_data @> '{"tags": ["premium"]}'::jsonb)
However, GIN indexes have significant drawbacks:They can be large, sometimes larger than the table itself.They can slow down write operations.They require more maintenance (vacuum).They don't help with range queries or sorting.For each key or scalar value, the GIN index maintains a posting list of row IDs where that element appears. These posting lists are compressed and optimised for fast intersection operations, which is why containment checks are so efficient.When PostgreSQL executes a containment query like:customer_data @> '{"status": "active", "type": "business"}Looks up the posting list for the key "status" with value "active"Looks up the posting list for the key "type" with value "business"Computes the intersection of these posting listsReturns the matching row IDsThis set-based operation is highly efficient compared to extracting and comparing values from each document.B-Tree index-- Index for a specific extracted field
CREATE INDEX idx_customer_source ON customers ((customer_data->>'source'));
(customer_data->>'source')The execution plan for a query using this index would show:Index Scan using idx_customer_source on customers  (cost=0.28..8.29 rows=1 width=32)
  Index Cond: ((customer_data->>'source'::text) = 'website'::text)
Are much smaller than GIN indexes,Have less impact on write performance,Support range queries and sorting,Only help with queries that exactly match the indexed expression.What’s more, you can also use it to set up unique constraints and get the same strong checks as for regular tables!Understanding the internal mechanisms helps explain why you might want different index types for different query patterns.For more specific query patterns, you can combine these approaches:-- Location-based index for frequently filtered fields
CREATE INDEX idx_customer_country ON customers ((customer_data->'contact'->'address'->>'country'));

-- Specialized conditional index for specific customer types
CREATE INDEX idx_web_utm ON customers ((customer_data->'web_data'->>'utm_source'))
WHERE customer_data->>'source' = 'website';
customer_data->>'source' = 'website'-- Query that can use the partial index
SELECT id FROM customers 
WHERE customer_data->>'source' = 'website' 
AND customer_data->'web_data'->>'utm_source' = 'google';
Internally, PostgreSQL maintains separate metadata about which index portions apply to which queries. The query planner uses statistics about data distribution to decide whether to use an index or not, which explains why sometimes PostgreSQL might choose a sequential scan even when an index exists.Suppose we have a table with 1 million customer records, and we frequently run queries to find customers from specific countries:-- Without an index, this requires scanning all records
SELECT id FROM customers 
WHERE customer_data->'contact'->'address'->>'country' = 'Germany';
The execution plan would show:Seq Scan on customers  (cost=0.00..24053.00 rows=10000 width=32)
  Filter: ((customer_data->'contact'->'address'->>'country'::text) = 'Germany'::text)
After adding an appropriate index:CREATE INDEX idx_country ON customers ((customer_data->'contact'->'address'->>'country'));
The execution plan becomes:Index Scan using idx_country on customers  (cost=0.42..341.50 rows=10000 width=32)
  Index Cond: ((customer_data->'contact'->'address'->>'country'::text) = 'Germany'::text)
This can reduce query time from seconds to milliseconds. But the choice between GIN and B-Tree indexes isn't always obvious. If we frequently need to find customers with specific combinations of attributes, a GIN index might be better:-- Query that benefits from a GIN index
SELECT id FROM customers 
WHERE customer_data @> '{"contact": {"address": {"country": "Germany"}}, "status": "active"}';
Performance is good enough for most typical cases, and if you don’t have a huge amount of data, the performance of JSONB with indexing is similar to traditional columns. Still, if it’s worth knowing that JSONB:Can perform worse for simple single-value lookups: Traditional columns with B-Tree indexes typically outperform JSONB path extraction,can perform better for retrieving complete complex objects. Avoiding joins can lead to significant performance advantages,Has variable performance for analytical queries: Depending on indexing strategy, can be either much faster or much slower than normalised dataWhile JSONB offers flexibility, it has performance characteristics you should understand, especially for large documentsTOAST (The Oversized-Attribute Storage Technique)A common anti-pattern is storing large arrays or deeply nested structures in a single JSONB document.You don't have to abandon all data validation when using JSONB. PostgreSQL allows you to enforce constraints on JSONB documents:ALTER TABLE customers ADD CONSTRAINT valid_customer
CHECK (
    customer_data ? 'id' AND 
    customer_data ? 'email' AND 
    customer_data ? 'source' AND
    customer_data ? 'contact'
)ALTER TABLE customers ADD CONSTRAINT valid_web_customer
CHECK (
    customer_data->>'source' != 'website' OR 
    (customer_data ? 'web_data' AND customer_data->'web_data' ? 'first_visit')
)These constraints leverage PostgreSQL's ability to check path existence and values within the binary representation. Internally, PostgreSQL evaluates these expressions during insert and update operations, using the same path traversal algorithms that power queries.PostgreSQL 12+ introduces a powerful new way to work with JSONB through the SQL/JSON path language. This provides a more expressive syntax for complex data extraction:SELECT jsonb_path_query(
    customer_data, 
    '$.purchases[*] ? (@.amount > 10 && @.subscription == true)'
) 
FROM customers;
This query finds all purchases that are subscriptions with an amount greater than 10. The path expression is evaluated against each document, and matching elements are returned.Internally, PostgreSQL parses the path expression into an execution plan specific to JSON path traversal.JSONB works well with PostgreSQL's aggregation functions:-- Find average purchase amount across all customers
SELECT AVG(
    (jsonb_array_elements(customer_data->'purchases')->>'amount')::numeric
) 
FROM customers;

-- Group purchases by product type
SELECT 
    COALESCE(p->>'product_id', p->>'item_id') AS product,
    COUNT(*) 
FROM 
    customers,
    jsonb_array_elements(customer_data->'purchases') AS p
GROUP BY 
    product;
Of course, you can mix traditional columns with JSONB ones. For those columns that you know will always exist, or are the same, you can set up regular tables, and those with weaker schema, you can use JSONB.CREATE TABLE customers (
    id TEXT PRIMARY KEY,
    email TEXT UNIQUE NOT NULL,
    created_at TIMESTAMP NOT NULL DEFAULT NOW(),
    source TEXT NOT NULL,
    -- Common fields as columns
    -- Variable parts as JSONB
    source_data JSONB NOT NULL,
    contact_info JSONB NOT NULL,
    purchases JSONB NOT NULL
);
This hybrid approach gives you the best of both worlds: schema enforcement where it matters and flexibility where needed. It's often the most practical approach in real-world systems. The hybrid aligns with PostgreSQL's internal implementation. Traditional columns can use the highly optimised columnar storage and indexing mechanisms PostgreSQL has refined over decades, while JSONB columns leverage the specialised binary representation for flexible data.JSONB represents a practical compromise between rigid relational schemas and completely schemaless document databases.Of course, in scenarios requiring absolute top performance, like business analytics with complex filtering, data warehousing, or very high-throughput OLTP systems, traditional columns with specialised indexes often perform better. In practice, a hybrid approach works best: use traditional columns for fixed, frequently queried attributes and JSONB for variable parts of your data.Big strength of JSONB is handling schema evolution.With JSONB, you can evolve your schema more gracefully. When business requirements change and you need to store additional data, you don't need to modify the database schema. Your application can simply start including new fields in the JSON documents it writes, and older documents without those fields continue to work fine.I’m biased, but I extremely like JSONB and have used it in the past in my system. It can speed up development for line-of-business applications. Syntax can be a bit tricky, but once you learn it, or provide some wrapper it works like a charm.https://github.com/event-driven-io/pongoPongo treats PostgreSQL as a Document Database, benefiting from JSONB support. Unlike the plain text storage of the traditional JSON type, JSONB stores JSON data in a binary format. This simple change brings significant advantages in terms of performance and storage efficiency.See also my video where I explained internals of both Pongo and JSONB:p.s. Ukraine is still under brutal Russian invasion. A lot of Ukrainian people are hurt, without shelter and need help.Ukraine humanitarian Ambulances for UkraineRed Cross]]></content:encoded></item><item><title>Pope Francis has died</title><link>https://www.reuters.com/world/pope-francis-has-died-vatican-says-video-statement-2025-04-21/</link><author>phillipharris</author><category>dev</category><category>hn</category><pubDate>Mon, 21 Apr 2025 08:00:14 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>How do you route traffic to different Kubernetes clusters?</title><link>https://www.reddit.com/r/kubernetes/comments/1k47goc/how_do_you_route_traffic_to_different_kubernetes/</link><author>/u/Deeblock</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Mon, 21 Apr 2025 07:11:17 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[I have two clusters set up with Gateway API. They each have a common gateway (load balancer) set up. How do I route traffic to either cluster?As an example, I would like abc.host.com to go to cluster A while def.host.com to go to cluster B. Users of cluster B should be able to add their own domain names. This could be something like otherhost.com (which is not part of host.com which I own).We have a private DNS server without root alias and it does not allow automating DNS routing for clients.]]></content:encoded></item><item><title>Python’s new t-strings</title><link>https://davepeck.org/2025/04/11/pythons-new-t-strings/</link><author>tambourine_man</author><category>dev</category><category>hn</category><pubDate>Mon, 21 Apr 2025 04:31:35 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Template strings, also known as t-strings, have been officially accepted as a feature in Python 3.14, which will ship in late 2025. 🎉I’m excited; t-strings open the door to safer more flexible string processing in Python.What’s the big idea with t-strings?Since they were introduced in Python 3.6, f-strings have become a  popular way to format strings. They are concise, readable, and powerful.In fact, they’re  delightful that many developers use f-strings for everything… even when they shouldn’t!Alas, f-strings are often dangerously (mis)used to format strings that contain user input. I’ve seen f-strings used for SQL (f"SELECT * FROM users WHERE name = '{user_name}'") and for HTML (f"<div>{user_name}</div>"). These are not safe! If  contains a malicious value, it can lead to SQL injection or cross-site scripting.Template strings are a  of Python’s f-strings. Whereas f-strings immediately become a string, t-strings evaluate to a new type, string.templatelib.Template:Importantly,  instances are  strings. The  type does not provide its own  implementation, which is to say that calling  does not return a useful value. Templates  be processed before they can be used; that processing code can be written by the developer or provided by a library and can safely escape the dynamic content.We can imagine a library that provides an  function that takes a  and returns a safely escaped string:Of course, t-strings are useful for more than just safety; they also allow for more flexible string processing. For example, that  function could return a new type, . It could also accept all sorts of useful substitutions in the HTML itself:If you’ve worked with JavaScript, t-strings may feel familiar. They are the pythonic parallel to JavaScript’s tagged templates.How do I work with t-strings?To support processing, s give developers access to the string and its interpolated values  they are combined into a final string.The  and  properties of a  return tuples:There is always one more (possibly empty) string than value. That is,  and t"{name}".strings == ("", "").As a shortcut, it’s also possible to iterate over a :Developers writing complex processing code can also access the gory details of each interpolation:In addition to supporting the literal () form, s can also be instantiated directly:Strings and interpolations can be provided to the  constructor in any order.A simple t-string exampleLet’s say we wanted to write code to convert all substituted words into pig latin. All it takes is a simple function:What’s next once t-strings ship?T-strings are a powerful new feature that will make Python string processing safer and more flexible. I hope to see them used in all sorts of libraries and frameworks, especially those that deal with user input.In addition, I hope that the tooling ecosystem will adapt to support t-strings. For instance, I’d love to see  and  format t-string , and  those contents, if they’re a common type like HTML or SQL.]]></content:encoded></item><item><title>Show HN: Keep your PyTorch model in VRAM by hot swapping code</title><link>https://github.com/valine/training-hot-swap/</link><author>valine</author><category>dev</category><category>hn</category><pubDate>Mon, 21 Apr 2025 00:21:27 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Resilient Multi-Cloud Strategies: Harnessing Kubernetes, Cluster API, and... T. Rahman &amp; J. Mosquera</title><link>https://www.youtube.com/watch?v=4DjydLH21nM</link><author>CNCF [Cloud Native Computing Foundation]</author><category>dev</category><category>k8s</category><enclosure url="https://www.youtube.com/v/4DjydLH21nM?version=3" length="" type=""/><pubDate>Sun, 20 Apr 2025 22:00:06 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">Dev - CNCF</source><content:encoded><![CDATA[Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon events in Hong Kong, China (June 10-11); Tokyo, Japan (June 16-17); Hyderabad, India (August 6-7); Atlanta, US (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io

Resilient Multi-Cloud Strategies: Harnessing Kubernetes, Cluster API, and Cell-Based Architecture - Tasdik Rahman & Javi Mosquera, New Relic 

In today's multi-cloud world, resilience and high availability at scale are crucial. This session will cover how we utilized Kubernetes with Cluster API and other cloud native components, to deploy a cell-based architecture across multiple cloud providers, scaling to 270+ clusters and 18,000+ nodes, creating independent, isolated cells that limit failures and improve uptime, thus simplifying compliance, cost management, and disaster recovery planning.

We'll explore how Cluster API facilitates seamless automation of cluster creation and management across our multi-cloud setup, upgrades, enhancing autonomy and resilience. Moreover, we'll highlight real-world use cases sharing our learnings from automation built for efficient management of k8s clusters while limiting operational overhead.

End users will learn from this talk on how they can use ClusterAPI, to automate their multi cloud cluster lifecycle management and leverage cellular architecture to build a highly available setup.]]></content:encoded></item><item><title>Show HN: JuryNow – Get an anonymous instant verdict from 12 real people</title><link>https://jurynow.app/</link><author>sarah-brussels</author><category>dev</category><category>hn</category><pubDate>Sun, 20 Apr 2025 18:32:29 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>ArcoLinux Lead Steps Down After Eight Years</title><link>https://linux.slashdot.org/story/25/04/19/1954232/arcolinux-lead-steps-down-after-eight-years?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>dev</category><category>slashdot</category><pubDate>Sun, 20 Apr 2025 15:34:00 +0000</pubDate><source url="https://linux.slashdot.org/">Dev - Slashdot - Linux</source><content:encoded><![CDATA["The time has come for me to step away," ArcoLinux lead Erik Dubois posted last week. ("After eight years of dedication to the ArcoLinux project and the broader Linux community...") 

'Learn, have fun, and enjoy' was our motto for the past eight years — and I really had fun doing all this," Dubois says in a video version of his farewell post. "And if we reflect back on this teaching and the building and promoting of Linux, it was fun. But the time has come for me to step away..." 

Over its eight years ArcoLinux "accomplished several important milestones," reports Linux magazine, "such as creating over 5,000 educational videos; the creation of ArcoInstall; the Carli education project; the Arch Linux Calamares Installer (ALCI); the ArcoPlasma, ArcoNet, ArcroPro, and Ariser variants; and much more."
According to Dubois, they weren't just creating a distribution but a mindset. 

Dubois says that the code will remain online so others can learn from, fork, or remix the distro. He also indicated that ArcoLinux will supply users with a transition package to help them convert their existing ArcoLinux systems to Arch Linux. That package will remove ArcoLinux branding, replace pacman.conf with an Arch and Chaotic-AUR focused config file, and change the arcolinux-mirrorlist to a single source.
 

 It's FOSS News describes ArcoLinux as one of those "user-friendly Arch-based distros that give you a bleeding-edge experience."
The reasoning behind this move, as shared by Erik, is his advancing age and him realizing that he doesn't have the same level of mental focus or stamina he used to have before. He has found himself making small mistakes, the kind that can negatively affect a major undertaking like this... Come July 1, 2025, the transition period will end, marking a stop to all development, including the deactivation of the ArcoLinux social media handles. The Telegram and Discord communities will stay a bit longer but will close up eventually. 
"I want to leave ArcoLinux while it's still strong, and while I can look back with pride at everything we've accomplished together," Dubois says in their post...]]></content:encoded></item><item><title>Gemma 3 QAT Models: Bringing AI to Consumer GPUs</title><link>https://developers.googleblog.com/en/gemma-3-quantized-aware-trained-state-of-the-art-ai-to-consumer-gpus/</link><author>emrah</author><category>dev</category><category>hn</category><pubDate>Sun, 20 Apr 2025 12:22:06 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Last month, we launched Gemma 3, our latest generation of open models. Delivering state-of-the-art performance, Gemma 3 quickly established itself as a leading model capable of running on a single high-end GPU like the NVIDIA H100 using its native BFloat16 (BF16) precision.To make Gemma 3 even more accessible, we are announcing new versions optimized with Quantization-Aware Training (QAT) that dramatically reduces memory requirements while maintaining high quality. This enables you to run powerful models like Gemma 3 27B locally on consumer-grade GPUs like the NVIDIA RTX 3090.Understanding performance, precision, and quantizationThe chart above shows the performance (Elo score) of recently released large language models. Higher bars mean better performance in comparisons as rated by humans viewing side-by-side responses from two anonymous models. Below each bar, we indicate the estimated number of NVIDIA H100 GPUs needed to run that model using the BF16 data type.Why BFloat16 for this comparison? BF16 is a common numerical format used during inference of many large models. It means that the model parameters are represented with 16 bits of precision. Using BF16 for all models helps us to make an apples-to-apples comparison of models in a common inference setup. This allows us to compare the inherent capabilities of the models themselves, removing variables like different hardware or optimization techniques like quantization, which we'll discuss next.It's important to note that while this chart uses BF16 for a fair comparison, deploying the very largest models often involves using lower-precision formats like FP8 as a practical necessity to reduce immense hardware requirements (like the number of GPUs), potentially accepting a performance trade-off for feasibility.The Need for AccessibilityWhile top performance on high-end hardware is great for cloud deployments and research, we heard you loud and clear: you want the power of Gemma 3 on the hardware you already own. We're committed to making powerful AI accessible, and that means enabling efficient performance on the consumer-grade GPUs found in desktops, laptops, and even phones.Performance Meets Accessibility with Quantization-Aware Training in Gemma 3This is where quantization comes in. In AI models, quantization reduces the precision of the numbers (the model's parameters) it stores and uses to calculate responses. Think of quantization like compressing an image by reducing the number of colors it uses. Instead of using 16 bits per number (BFloat16), we can use fewer bits, like 8 (int8) or even 4 (int4).Using int4 means each number is represented using only 4 bits – a 4x reduction in data size compared to BF16. Quantization can often lead to performance degradation, so we’re excited to release Gemma 3 models that are robust to quantization. We released several quantized variants for each Gemma 3 model to enable inference with your favorite inference engine, such as Q4_0 (a common quantization format) for Ollama, llama.cpp, and MLX.How do we maintain quality? We use QAT. Instead of just quantizing the model after it's fully trained, QAT incorporates the quantization process during training. QAT simulates low-precision operations during training to allow quantization with less degradation afterwards for smaller, faster models while maintaining accuracy. Diving deeper, we applied QAT on ~5,000 steps using probabilities from the non-quantized checkpoint as targets. We reduce the perplexity drop by 54% (using llama.cpp perplexity evaluation) when quantizing down to Q4_0.See the Difference: Massive VRAM SavingsThe impact of int4 quantization is dramatic. Look at the VRAM (GPU memory) required just to load the model weights: Drops from 54 GB (BF16) to just  (int4) Shrinks from 24 GB (BF16) to only  (int4) Reduces from 8 GB (BF16) to a lean  (int4) Goes from 2 GB (BF16) down to a tiny  (int4)This figure only represents the VRAM required to load the model weights. Running the model also requires additional VRAM for the KV cache, which stores information about the ongoing conversation and depends on the context lengthRun Gemma 3 on Your DeviceThese dramatic reductions unlock the ability to run larger, powerful models on widely available consumer hardware: Now fits comfortably on a single desktop NVIDIA RTX 3090 (24GB VRAM) or similar card, allowing you to run our largest Gemma 3 variant locally. Runs efficiently on laptop GPUs like the NVIDIA RTX 4060 Laptop GPU (8GB VRAM), bringing powerful AI capabilities to portable machines. Offer even greater accessibility for systems with more constrained resources, including phones and toasters (if you have a good one).Easy Integration with Popular ToolsWe want you to be able to use these models easily within your preferred workflow. Our official int4 and Q4_0 unquantized QAT models are available on Hugging Face and Kaggle. We’ve partnered with popular developer tools that enable seamlessly trying out the QAT-based quantized checkpoints: Get running quickly – all our Gemma 3 QAT models are natively supported starting today with a simple command. Easily download and run Gemma 3 QAT models on your desktop via its user-friendly interface. Leverage MLX for efficient, optimized inference of Gemma 3 QAT models on Apple Silicon. Use our dedicated C++ implementation for highly efficient inference directly on the CPU. Integrate easily into existing workflows thanks to native support for our GGUF-formatted QAT models.More Quantizations in the GemmaverseOur official Quantization Aware Trained (QAT) models provide a high-quality baseline, but the vibrant Gemmaverse offers many alternatives. These often use Post-Training Quantization (PTQ), with significant contributions from members such as Bartowski, Unsloth, and GGML readily available on Hugging Face. Exploring these community options provides a wider spectrum of size, speed, and quality trade-offs to fit specific needs.Bringing state-of-the-art AI performance to accessible hardware is a key step in democratizing AI development. With Gemma 3 models, optimized through QAT, you can now leverage cutting-edge capabilities on your own desktop or laptop.Explore the quantized models and start building:We can't wait to see what you build with Gemma 3 running locally!]]></content:encoded></item></channel></rss>