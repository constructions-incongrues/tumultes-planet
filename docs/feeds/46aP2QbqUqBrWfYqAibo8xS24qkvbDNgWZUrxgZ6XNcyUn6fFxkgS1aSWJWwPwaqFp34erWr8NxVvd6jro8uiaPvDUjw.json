{"id":"46aP2QbqUqBrWfYqAibo8xS24qkvbDNgWZUrxgZ6XNcyUn6fFxkgS1aSWJWwPwaqFp34erWr8NxVvd6jro8uiaPvDUjw","title":"top scoring links : kubernetes","displayTitle":"Reddit - Kubernetes","url":"https://www.reddit.com/r/kubernetes/top/.rss?sort=top&t=day&limit=6","feedLink":"https://www.reddit.com/r/kubernetes/top/?sort=top&t=day&limit=6","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":6,"items":[{"title":"Multizone cluster cost optimization","url":"https://www.reddit.com/r/kubernetes/comments/1k4y3v4/multizone_cluster_cost_optimization/","date":1745297300,"author":"/u/elephantum","guid":409,"unread":true,"content":"<p>So, I recently realized, that at least 30% of my GKE bill is traffic between zones \"Network Inter Zone Data Transfer\" SKU. This project is very heavy on internal traffic, so I can see how monthly data exchange between services can be in terms of hundreds of terabytes</p><p>My cluster was setup by default with nodes scattered across all zones in the region (default setup if I'm not mistaken)</p><p>At this moment I decided to force all nodes into a single zone, which brought cost down, but it goes against all the recommendations about availability</p><p>So it got me thinking, if I want to achieve both goals at once: - have multi AZ cluster for availability - keep intra AZ traffic at minimum </p><p>I know how to do it by hand: deploy separate app stack for each AZ and loadbalance traffic between them, but it seems like an overcomplication</p><p>Is there a less explicit way to prefer local communication between services in k8s?</p>","contentLength":902,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"EKS Multicluster service mesh","url":"https://www.reddit.com/r/kubernetes/comments/1k4rwnv/eks_multicluster_service_mesh/","date":1745278052,"author":"/u/IllustriousStorage28","guid":405,"unread":true,"content":"<p>I work for an enterprise company with 2 clusters for production running same set of applications and being load balanced by aws alb. </p><p>We are looking to introduce service mesh in our environment, while evaluating multiple meshes we came across istio and kuma both being a good fit for multi-cluster environment. </p><p>On one hand kuma looks to be very easy to setup and built with multi-cluster architecture. Though docs are lacking a lot of information and donâ€™t see much community support either. </p><p>On the other hand istio has been battle tested in multiple production environments and has a great community support and documentations. Though multi-cluster setup is more sort of extension than built in capability. Also, various tools required to manage configs and visualise metrics. </p><p>We would want capabilities to control traffic effectively and ability to load balance between multiple cluster not being connected directly ( separate vpc with peering and non-peering connections). And ability to be able add a new cluster as we want. </p><p>Is there anyone here who has used istio or kuma multi-cluster. Also, please do share your experience with either of them in managing, debugging and upgrading them. </p>","contentLength":1194,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Thoughts on Upwind alternative to Wiz?","url":"https://www.reddit.com/r/kubernetes/comments/1k4p81b/thoughts_on_upwind_alternative_to_wiz/","date":1745270964,"author":"/u/pxrage","guid":408,"unread":true,"content":"<p>I'm contracting as a fCTO for enterprise health tech, wrapping up a project focused on optimizing their k8s monitoring costs. We are nearly done implementing and rolling out a new eBPF based solution to further cut cost.</p><p>In the same time I'm tackling their security tooling related costs. They're currently heavily invested in AWS-native tools, and we're exploring alternatives that might offer better value. Potentially integrating more smoothly with our BYOC infra.</p><p>I've already begun PoV using Upwind. Finished initial deep dive exploring their run-time powered cloud security stack and seems like it's the right fit for us. While not completely validated, I am impressed by the claim of reducing noise by up to 95% and the speed improvement up root cause analysis (via client case studies). Their use of eBPF for agentless sensors also resonates with our goal of maintaining efficiency.</p><p>Before we dive deeper, I wanted to tap into the community's collective wisdom:</p><ol><li><p>\"Runtime-powered\" reality check: For those who have experience, how well does the \"runtime-powered\" aspect deliver in practice? Does it truly leverage runtime context effectively to prioritize real threats and reduce alert fatigue compared to more traditional CNAPP solutions or native cloud provider tools? How seamless is the integration of its CSPM, CWPP, Vulnerability Management, etc., under this runtime umbrella?</p></li><li><p>eBPF monitoring and security in one: we've already invested in building out an eBPF-based o11y stack. Has anyone successfully leveraged eBPF for both monitoring/observability and security within the same k8s environment? Are there tangible synergies (performance benefits, reduced overhead, unified data plane) or is it more practical to keep these stacks separate, even if both utilize eBPF? Does using eBPF security stack alongside an existing eBPF monitoring solution create conflicts or complexities?</p></li></ol><p>Lastly, we're still early in the discovery phase that I'm allowed to look beyond one single security provider. Are there other runtime-focused security platforms (especially those leveraging eBPF) that you've found particularly effective in complex K8s environments, specifically when cost optimization and reducing tool sprawl are key drivers?</p><p>Appreciate any insights, thanks!</p>","contentLength":2265,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"High availability k8s question (I'm still new to this)","url":"https://www.reddit.com/r/kubernetes/comments/1k4j588/high_availability_k8s_question_im_still_new_to/","date":1745256364,"author":"/u/IceBreaker8","guid":410,"unread":true,"content":"<p>I have a question: Let's say I have a k8s cluster with one master node and 2 workers, if I have one master node, and it goes down, do my apps become inaccessible? like for instance, websites and such.. Or does it just prevent pod reschedule, auto scaling, jobs etc.. and the apps will still be accessible?</p>","contentLength":305,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Ask r/kubernetes: What are you working on this week?","url":"https://www.reddit.com/r/kubernetes/comments/1k49ttq/ask_rkubernetes_what_are_you_working_on_this_week/","date":1745229634,"author":"/u/gctaylor","guid":407,"unread":true,"content":"<p>What are you up to with Kubernetes this week? Evaluating a new tool? In the process of adopting? Working on an open source project or contribution? Tell <a href=\"https://www.reddit.com/r/kubernetes\">/r/kubernetes</a> what you're up to this week!</p>","contentLength":195,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How do you route traffic to different Kubernetes clusters?","url":"https://www.reddit.com/r/kubernetes/comments/1k47goc/how_do_you_route_traffic_to_different_kubernetes/","date":1745219477,"author":"/u/Deeblock","guid":406,"unread":true,"content":"<p>I have two clusters set up with Gateway API. They each have a common gateway (load balancer) set up. How do I route traffic to either cluster?</p><p>As an example, I would like abc.host.com to go to cluster A while def.host.com to go to cluster B. Users of cluster B should be able to add their own domain names. This could be something like otherhost.com (which is not part of host.com which I own).</p><p>We have a private DNS server without root alias and it does not allow automating DNS routing for clients.</p>","contentLength":498,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null}],"tags":["dev","reddit","k8s"]}