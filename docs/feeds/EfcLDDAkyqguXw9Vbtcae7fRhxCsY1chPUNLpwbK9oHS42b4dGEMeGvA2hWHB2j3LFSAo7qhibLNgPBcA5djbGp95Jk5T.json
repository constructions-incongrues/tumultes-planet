{"id":"EfcLDDAkyqguXw9Vbtcae7fRhxCsY1chPUNLpwbK9oHS42b4dGEMeGvA2hWHB2j3LFSAo7qhibLNgPBcA5djbGp95Jk5T","title":"top scoring links : programming","displayTitle":"Reddit - Programming","url":"https://www.reddit.com/r/programming/top/.rss?sort=top&t=day&limit=6","feedLink":"https://www.reddit.com/r/programming/top/?sort=top&t=day&limit=6","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":6,"items":[{"title":"On the cruelty of really teaching computing science (1988)","url":"https://www.cs.utexas.edu/~EWD/transcriptions/EWD10xx/EWD1036.html","date":1745256472,"author":"/u/ketralnis","guid":417,"unread":true,"content":"<p><u>On the cruelty of really teaching computing science</u></p><p>The second part of this talk pursues some of the scientific and educational consequences of the assumption that computers represent a radical novelty. In order to give this assumption clear contents, we have to be much more precise as to what we mean in this context by the adjective \"radical\". We shall do so in the first part of this talk, in which we shall furthermore supply evidence in support of our assumption.</p><p>The usual way in which we plan today for tomorrow is in yesterday's vocabulary. We do so, because we try to get away with the concepts we are familiar with and that have acquired their meanings in our past experience. Of course, the words and the concepts don't quite fit because our future differs from our past, but then we stretch them a little bit. Linguists are quite familiar with the phenomenon that the meanings of words evolve over time, but also know that this is a slow and gradual process.</p><p>It is the most common way of trying to cope with novelty: by means of metaphors and analogies we try to link the new to the old, the novel to the familiar. Under sufficiently slow and gradual change, it works reasonably well; in the case of a sharp discontinuity, however, the method breaks down: though we may glorify it with the name \"common sense\", our past experience is no longer relevant, the analogies become too shallow, and the metaphors become more misleading than illuminating. This is the situation that is characteristic for the \"radical\" novelty.</p><p>Coping with radical novelty requires an orthogonal method. One must consider one's own past, the experiences collected, and the habits formed in it as an unfortunate accident of history, and one has to approach the radical novelty with a blank mind, consciously refusing to try to link it with what is already familiar, because the familiar is hopelessly inadequate. One has, with initially a kind of split personality, to come to grips with a radical novelty as a dissociated topic in its own right. Coming to grips with a radical novelty amounts to creating and learning a new foreign language that can  be translated into one's mother tongue. (Any one who has learned quantum mechanics knows what I am talking about.) Needless to say, adjusting to radical novelties is not a very popular activity, for it requires hard work. For the same reason, the radical novelties themselves are unwelcome.</p><p>By now, you may well ask why I have paid so much attention to and have spent so much eloquence on such a simple and obvious notion as the radical novelty. My reason is very simple: radical novelties are so disturbing that they tend to be suppressed or ignored, to the extent that even the possibility of their existence in general is more often denied than admitted.</p><p>On the historical evidence I shall be short. Carl Friedrich Gauss, the Prince of Mathematicians but also somewhat of a coward, was certainly aware of the fate of Galileo —and could probably have predicted the calumniation of Einstein— when he decided to suppress his discovery of non-Euclidean geometry, thus leaving it to Bolyai and Lobatchewsky to receive the flak. It is probably more illuminating to go a little bit further back, to the Middle Ages. One of its characteristics was that \"reasoning by analogy\" was rampant; another characteristic was almost total intellectual stagnation, and we now see why the two go together. A reason for mentioning this is to point out that, by developing a keen ear for unwarranted analogies, one can detect a lot of medieval thinking today.</p><p>The other thing I can not stress enough is that the fraction of the population for which gradual change seems to be all but the only paradigm of history is very large, probably much larger than you would expect. Certainly when I started to observe it, their number turned out to be much larger than I had expected.</p><p>For instance, the vast majority of the mathematical community has never challenged its tacit assumption that doing mathematics will remain very much the same type of mental activity it has always been: new topics will come, flourish, and go as they have done in the past, but, the human brain being what it is, our ways of teaching, learning, and understanding mathematics, of problem solving, and of mathematical discovery will remain pretty much the same. Herbert Robbins clearly states why he rules out a quantum leap in mathematical ability:</p><blockquote>\n\t\t\t\t\t\"Nobody is going to run 100 meters in five seconds, no matter how much is invested in training and machines. The same can be said about using the brain. The human mind is no different now from what it was five thousand years ago. And when it comes to mathematics, you must realize that this is the human mind at an extreme limit of its capacity.\"</blockquote><p>My comment in the margin was \"so reduce the use of the brain and calculate!\". Using Robbins's own analogy, one could remark that, for going from A to B fast, there could now exist alternatives to running that are orders of magnitude more effective. Robbins flatly refuses to honour any alternative to time-honoured brain usage with the name of \"doing mathematics\", thus exorcizing the danger of radical novelty by the simple device of adjusting his definitions to his needs: simply by definition, mathematics will continue to be what it used to be. So much for the mathematicians.</p><p>Let me give you just one more example of the widespread disbelief in the existence of radical novelties and, hence, in the need of learning how to cope with them. It is the prevailing educational practice, for which gradual, almost imperceptible, change seems to be the exclusive paradigm. How many educational texts are not recommended for their appeal to the student's intuition! They constantly try to present everything that could be an exciting novelty as something as familiar as possible. They consciously try to link the new material to what is supposed to be the student's familiar world. It already starts with the teaching of arithmetic. Instead of teaching 2 + 3 = 5 , the hideous arithmetic operator \"plus\" is carefully disguised by calling it \"and\", and the little kids are given lots of familiar examples first, with clearly visible such as apples and pears, which are , in contrast to equally countable objects such as percentages and electrons, which are . The same silly tradition is reflected at university level in different introductory calculus courses for the future physicist, architect, or business major, each adorned with examples from the respective fields. The educational dogma seems to be that everything is fine as long as the student does not notice that he is learning something really new; more often than not, the student's impression is indeed correct. I consider the failure of an educational practice to prepare the next generation for the phenomenon of radical novelties a serious shortcoming. [When King Ferdinand visited the conservative university of Cervera, the Rector proudly reassured the monarch with the words; \"Far be from us, Sire, the dangerous novelty of thinking.\". Spain's problems in the century that followed justify my characterization of the shortcoming as \"serious\".] So much for education's adoption of the paradigm of gradual change.</p><p>The concept of radical novelties is of contemporary significance because, while we are ill-prepared to cope with them, science and technology have now shown themselves expert at inflicting them upon us. Earlier scientific examples are the theory of relativity and quantum mechanics; later technological examples are the atom bomb and the pill. For decades, the former two gave rise to a torrent of religious, philosophical, or otherwise quasi-scientific tracts. We can daily observe the profound inadequacy with which the latter two are approached, be it by our statesmen and religious leaders or by the public at large. So much for the damage done to our peace of mind by radical novelties.</p><p>I raised all this because of my contention that automatic computers represent a radical novelty and that only by identifying them as such can we identify all the nonsense, the misconceptions and the mythology that surround them. Closer inspection will reveal that it is even worse, viz. that automatic computers embody not only one radical novelty but two of them.</p><p>The first radical novelty is a direct consequence of the raw power of today's computing equipment. We all know how we cope with something big and complex; divide and rule, i.e. we view the whole as a compositum of parts and deal with the parts separately. And if a part is too big, we repeat the procedure. The town is made up from neighbourhoods, which are structured by streets, which contain buildings, which are made from walls and floors, that are built from bricks, etc. eventually down to the elementary particles. And we have all our specialists along the line, from the town planner, via the architect to the solid state physicist and further. Because, in a sense, the whole is \"bigger\" than its parts, the depth of a hierarchical decomposition is some sort of logarithm of the ratio of the \"sizes\" of the whole and the ultimate smallest parts. From a bit to a few hundred megabytes, from a microsecond to a half an hour of computing confronts us with completely baffling ratio of 10! The programmer is in the unique position that his is the only discipline and profession in which such a gigantic ratio, which totally baffles our imagination, has to be bridged by a single technology. He has to be able to think in terms of conceptual hierarchies that are much deeper than a single mind ever needed to face before. Compared to that number of semantic levels, the average mathematical theory is almost flat. By evoking the need for deep conceptual hierarchies, the automatic computer confronts us with a radically new intellectual challenge that has no precedent in our history.</p><p>Again, I have to stress this radical novelty because the true believer in gradual change and incremental improvements is unable to see it. For him, an automatic computer is something like the familiar cash register, only somewhat bigger, faster, and more flexible. But the analogy is ridiculously shallow: it is orders of magnitude worse than comparing, as a means of transportation, the supersonic jet plane with a crawling baby, for that speed ratio is only a thousand.</p><p>The second radical novelty is that the automatic computer is our first large-scale digital device. We had a few with a noticeable discrete component: I just mentioned the cash register and can add the typewriter with its individual keys: with a single stroke you can type either a Q or a W but, though their keys are next to each other, not a mixture of those two letters. But such mechanisms are the exception, and the vast majority of our mechanisms are viewed as analogue devices whose behaviour is over a large range a continuous function of all parameters involved: if we press the point of the pencil a little bit harder, we get a slightly thicker line, if the violinist slightly misplaces his finger, he plays slightly out of tune. To this I should add that, to the extent that we view ourselves as mechanisms, we view ourselves primarily as analogue devices: if we push a little harder we expect to do a little better. Very often the behaviour is not only a continuous but even a monotonic function: to test whether a hammer suits us over a certain range of nails, we try it out on the smallest and largest nails of the range, and if the outcomes of those two experiments are positive, we are perfectly willing to believe that the hammer will suit us for all nails in between.</p><p>It is possible, and even tempting, to view a program as an abstract mechanism, as a device of some sort. To do so, however, is highly dangerous: the analogy is too shallow because a program is, as a mechanism, totally different from all the familiar analogue devices we grew up with. Like all digitally encoded information, it has unavoidably the uncomfortable property that the smallest possible perturbations —i.e. changes of a single bit— can have the most drastic consequences. [For the sake of completness I add that the picture is not essentially changed by the introduction of redundancy or error correction.] In the discrete world of computing, there is no meaningful metric in which \"small\" changes and \"small\" effects go hand in hand, and there never will be.</p><p>This second radical novelty shares the usual fate of all radical novelties: it is denied, because its truth would be too discomforting. I have no idea what this specific denial and disbelief costs the United States, but a million dollars a day seems a modest guess.</p><p>Having described —admittedly in the broadest possible terms— the nature of computing's novelties, I shall now provide the evidence that these novelties are, indeed, radical. I shall do so by explaining a number of otherwise strange phenomena as frantic —but, as we now know, doomed— efforts at hiding or denying the frighteningly unfamiliar.</p><p>A number of these phenomena have been bundled under the name \"Software Engineering\". As economics is known as \"The Miserable Science\", software engineering should be known as \"The Doomed Discipline\", doomed because it cannot even approach its goal since its goal is self-contradictory. Software engineering, of course, presents itself as another worthy cause, but that is eyewash: if you carefully read its literature and analyse what its devotees actually do, you will discover that software engineering has accepted as its charter \"How to program if you cannot.\".</p><p>The popularity of its name is enough to make it suspect. In what we denote as \"primitive societies\", the superstition that knowing someone's true name gives you magic power over him is not unusual. We are hardly less primitive: why do we persist here in answering the telephone with the most unhelpful \"hello\" instead of our name?</p><p>Nor are we above the equally primitive superstition that we can gain some control over some unknown, malicious demon by calling it by a safe, familiar, and innocent name, such as \"engineering\". But it is totally symbolic, as one of the US computer manufacturers proved a few years ago when it hired, one night, hundreds of new \"software engineers\" by the simple device of elevating all its programmers to that exalting rank. So much for that term.</p><p>The practice is pervaded by the reassuring illusion that programs are just devices like any others, the only difference admitted being that their manufacture might require a new type of craftsmen, viz. programmers. From there it is only a small step to measuring \"programmer productivity\" in terms of \"number of lines of code produced per month\". This is a very costly measuring unit because it encourages the writing of insipid code, but today I am less interested in how foolish a unit it is from even a pure business point of view. My point today is that, if we wish to count lines of code, we should not regard them as \"lines produced\" but as \"lines spent\": the current conventional wisdom is so foolish as to book that count on the wrong side of the ledger.</p><p>Besides the notion of productivity, also that of quality control continues to be distorted by the reassuring illusion that what works with other devices works with programs as well. It is now two decades since it was pointed out that program testing may convincingly demonstrate the presence of bugs, but can never demonstrate their absence. After quoting this well-publicized remark devoutly, the software engineer returns to the order of the day and continues to refine his testing strategies, just like the alchemist of yore, who continued to refine his chrysocosmic purifications.</p><p>Unfathomed misunderstanding is further revealed by the term \"software maintenance\", as a result of which many people continue to believe that programs —and even programming languages themselves— are subject to wear and tear. Your car needs maintenance too, doesn't it? Famous is the story of the oil company that believed that its PASCAL programs did not last as long as its FORTRAN programs \"because PASCAL was not maintained\".</p><p>In the same vein I must draw attention to the astonishing readiness with which the suggestion has been accepted that the pains of software production are largely due to a lack of appropriate \"programming tools\". (The telling \"programmer's workbench\" was soon to follow.) Again, the shallowness of the underlying analogy is worthy of the Middle Ages. Confrontations with insipid \"tools\" of the \"algorithm-animation\" variety has not mellowed my judgement; on the contrary, it has confirmed my initial suspicion that we are primarily dealing with yet another dimension of the snake oil business.</p><p>Finally, to correct the possible impression that the inability to face radical novelty is confined to the industrial world, let me offer you an explanation of the —at least American— popularity of Artificial Intelligence. One would expect people to feel threatened by the \"giant brains or machines that think\". In fact, the frightening computer becomes less frightening if it is used only to simulate a familiar noncomputer. I am sure that this explanation will remain controversial for quite some time, for Artificial Intelligence as mimicking the human mind prefers to view itself as at the front line, whereas my explanation relegates it to the rearguard. (The effort of using machines to mimic the human mind has always struck me as rather silly: I'd rather use them to mimic something better.)</p><p>So much for the evidence that the computer's novelties are, indeed, radical.</p><p>And now comes the second —and hardest— part of my talk: the scientific and educational consequences of the above. The educational consequences are, of course, the hairier ones, so let's postpone their discussion and stay for a while with computing science itself. What is computing? And what is a science of computing about?</p><p>Well, when all is said and done, the only thing computers can do for us is to manipulate symbols and produce results of such manipulations. From our previous observations we should recall that this is a discrete world and, moreover, that both the number of symbols involved and the amount of manipulation performed are many orders of magnitude larger than we can envisage: they totally baffle our imagination and we must therefore not try to imagine them.</p><p>But before a computer is ready to perform a class of meaningful manipulations —or calculations, if you prefer— we must write a program. What is a program? Several answers are possible. We can view the program as what turns the general-purpose computer into a special-purpose symbol manipulator, and does so without the need to change a single wire (This was an enormous improvement over machines with problem-dependent wiring panels.) I prefer to describe it the other way round: the program is an abstract symbol manipulator, which can be turned into a concrete one by supplying a computer to it. After all, it is no longer the purpose of programs to instruct our machines; these days, it is the purpose of machines to execute our programs.</p><p>So, we have to design abstract symbol manipulators. We all know what they look like: they look like programs or —to use somewhat more general terminology— usually rather elaborate formulae from some formal system. It really helps to view a program as a formula. Firstly, it puts the programmer's task in the proper perspective: he has to derive that formula. Secondly, it explains why the world of mathematics all but ignored the programming challenge: programs were so much longer formulae than it was used to that it did not even recognize them as such. Now back to the programmer's job: he has to derive that formula, he has to derive that program. We know of only one reliable way of doing that, viz. by means of symbol manipulation. And now the circle is closed: we construct our mechanical symbol manipulators by means of human symbol manipulation.</p><p>Hence, computing science is —and will always be— concerned with the interplay between mechanized and human symbol manipulation, usually referred to as \"computing\" and \"programming\" respectively. An immediate benefit of this insight is that it reveals \"automatic programming\" as a contradiction in terms. A further benefit is that it gives us a clear indication where to locate computing science on the world map of intellectual disciplines: in the direction of formal mathematics and applied logic, but ultimately far beyond where those are now, for computing science is interested in  use of formal methods and on a much, much, larger scale than we have witnessed so far. Because no endeavour is respectable these days without a TLA (= Three-Letter Acronym), I propose that we adopt for computing science FMI (= Formal Methods Initiative), and, to be on the safe side, we had better follow the shining examples of our leaders and make a Trade Mark of it.</p><p>In the long run I expect computing science to transcend its parent disciplines, mathematics and logic, by effectively realizing a significant part of Leibniz's Dream of providing symbolic calculation as an alternative to human reasoning. (Please note the difference between \"mimicking\" and \"providing an alternative to\": alternatives are allowed to be better.)</p><p>Needless to say, this vision of what computing science is about is not universally applauded. On the contrary, it has met widespread —and sometimes even violent— opposition from all sorts of directions. I mention as examples</p><p>(0) the mathematical guild, which would rather continue to believe that the Dream of Leibniz is an unrealistic illusion</p><p>(1) the business community, which, having been sold to the idea that computers would make life easier, is mentally unprepared to accept that they only solve the easier problems at the price of creating much harder ones</p><p>(2) the subculture of the compulsive programmer, whose ethics prescribe that one silly idea and a month of frantic coding should suffice to make him a life-long millionaire</p><p>(3) computer engineering, which would rather continue to act as if it is all only a matter of higher bit rates and more flops per second</p><p>(4) the military, who are now totally absorbed in the business of using computers to mutate billion-dollar budgets into the illusion of automatic safety</p><p>(5) all soft sciences for which computing now acts as some sort of interdisciplinary haven</p><p>(6) the educational business that feels that, if it has to teach formal mathematics to CS students, it may as well close its schools.</p><p>And with this sixth example I have reached, imperceptibly but also alas unavoidably, the most hairy part of this talk: educational consequences.</p><p>The problem with educational policy is that it is hardly influenced by scientific considerations derived from the topics taught, and almost entirely determined by extra-scientific circumstances such as the combined expectations of the students, their parents and their future employers, and the prevailing view of the role of the university: is the stress on training its graduates for today's entry-level jobs or to providing its alumni with the intellectual bagage and attitudes that will last them another 50 years? Do we grudgingly grant the abstract sciences only a far-away corner on campus, or do we recognize them as the indispensable motor of the high-technology industry? Even if we do the latter, do we recognize a high-technology industry as such if its technology primarily belongs to formal mathematics? Do the universities provide for society the intellectual leadership it needs or only the training it asks for?</p><p>Traditional academic rhetoric is perfectly willing to give to these questions the reassuring answers, but I don't believe them. By way of illustration of my doubts, in a recent article on \"Who Rules Canada?\", David H. Flaherty bluntly states \"Moreover, the business elite dismisses traditional academics and intellectuals as largely irrelevant and powerless.\".</p><p>So, if I look into my foggy crystal ball at the future of computing science education, I overwhelmingly see the depressing picture of \"Business as usual\". The universities will continue to lack the courage to teach hard science, they will continue to misguide the students, and each next stage of infantilization of the curriculum will be hailed as educational progress.</p><p>I now have had my foggy crystal ball for quite a long time. Its predictions are invariably gloomy and usually correct, but I am quite used to that and they won't keep me from giving you a few suggestions, even if it is merely an exercise in futility whose only effect is to make you feel guilty.</p><p>We could, for instance, begin with cleaning up our language by no longer calling a bug a bug but by calling it an error. It is much more honest because it squarely puts the blame where it belongs, viz. with the programmer who made the error. The animistic metaphor of the bug that maliciously sneaked in while the programmer was not looking is intellectually dishonest as it disguises that the error is the programmer's own creation. The nice thing of this simple change of vocabulary is that it has such a profound effect: while, before, a program with only one bug used to be \"almost correct\", afterwards a program with an error is just \"wrong\" (because in error).</p><p>My next linguistical suggestion is more rigorous. It is to fight the \"if-this-guy-wants-to-talk-to-that-guy\" syndrome:  refer to parts of programs or pieces of equipment in an anthropomorphic terminology, nor allow your students to do so. This linguistical improvement is much harder to implement than you might think, and your department might consider the introduction of fines for violations, say a quarter for undergraduates, two quarters for graduate students, and five dollars for faculty members: by the end of the first semester of the new regime, you will have collected enough money for two scholarships.</p><p>The reason for this last suggestion is that the anthropomorphic metaphor —for whose introduction we can blame John von Neumann— is an enormous handicap for every computing community that has adopted it. I have now encountered programs wanting things, knowing things, expecting things, believing things, etc., and each time that gave rise to avoidable confusions. The analogy that underlies this personification is so shallow that it is not only misleading but also paralyzing.</p><p>It is misleading in the sense that it suggests that we can adequately cope with the unfamiliar discrete in terms of the familiar continuous, i.e. ourselves, quod non. It is paralyzing in the sense that, because persons exist and act , its adoption effectively prevents a departure from operational semantics and thus forces people to think about programs in terms of computational behaviours, based on an underlying computational model. This is bad, because operational reasoning is a tremendous waste of mental effort.</p><p>Let me explain to you the nature of that tremendous waste, and allow me to try to convince you that the term \"tremendous waste of mental effort\" is  an exaggeration. For a short while, I shall get highly technical, but don't get frightened: it is the type of mathematics that one can do with one's hands in one's pockets. The point to get across is that if we have to demonstrate something about  the elements of a large set, it is hopelessly inefficient to deal with all the elements of the set individually: the efficient argument does not refer to individual elements at all and is carried out in terms of the set's definition.</p><p>Consider the plane figure , defined as the 8 by 8 square from which, at two opposite corners, two 1 by 1 squares have been removed. The area of  is 62, which equals the combined area of 31 dominos of 1 by 2. The theorem is that the figure  cannot be covered by 31 of such dominos.</p><p>Another way of stating the theorem is that if you start with squared paper and begin covering this by placing each next domino on two new adjacent squares, no placement of 31 dominos will yield the figure .</p><p>So, a possible way of proving the theorem is by generating all possible placements of dominos and verifying for each placement that it does not yield the figure : a tremendously laborious job.</p><p>The simple argument, however is as follows. Colour the squares of the squared paper as on a chess board. Each domino, covering two adjacent squares, covers 1 white and 1 black square, and, hence, each placement covers as many white squares as it covers black squares. In the figure , however, the number of white squares and the number of black squares differ by 2 —opposite corners lying on the same diagonal— and hence no placement of dominos yields figure .</p><p>Not only is the above simple argument many orders of magnitude shorter than the exhaustive investigation of the possible placements of 31 dominos, it is also essentially more powerful, for it covers the generalization of  by replacing the original 8 by 8 square by  rectangle with sides of even length. The number of such rectangles being infinite, the former method of exhaustive exploration is essentially inadequate for proving our generalized theorem.</p><p>And this concludes my example. It has been presented because it illustrates in a nutshell the power of down-to-earth mathematics; needless to say, refusal to exploit this power of down-to-earth mathematics amounts to intellectual and technological suicide. The moral of the story is: deal with all elements of a set by ignoring them and working with the set's definition.</p><p>Back to programming. The statement that a given program meets a certain specification amounts to a statement about  computations that could take place under control of that given program. And since this set of computations is defined by the given program, our recent moral says: deal with all computations possible under control of a given program by ignoring them and working with the program. We must learn to work with program texts while (temporarily) ignoring that they admit the interpretation of executable code.</p><p>Another way of saying the same thing is the following one. A programming language, with its formal syntax and with the proof rules that define its semantics, is a formal system for which program execution provides only a model. It is well-known that formal systems should be dealt with in their own right, and not in terms of a specific model. And, again, the corollary is that we should reason about programs without even mentioning their possible \"behaviours\".</p><p>And this concludes my technical excursion into the reason why operational reasoning about programming is \"a tremendous waste of mental effort\" and why, therefore, in computing science the anthropomorphic metaphor should be banned.</p><p>Not everybody understands this sufficiently well. I was recently exposed to a demonstration of what was pretended to be educational software for an introductory programming course. With its \"visualizations\" on the screen it was such an obvious case of curriculum infantilization that its author should be cited for \"contempt\" of the student body\", but this was only a minor offense compared with what the visualizations were used for: they were used to display all sorts of features of computations evolving under control of the student's program! The system highlighted precisely what the student has to learn to ignore, it reinforced precisely what the student has to unlearn. Since breaking out of bad habits, rather than acquiring new ones, is the toughest part of learning, we must expect from that system permanent mental damage for most students exposed to it.</p><p>Needless to say, that system completely hid the fact that, all by itself, a program is no more than half a conjecture. The other half of the conjecture is the functional specification the program is supposed to satisfy. The programmer's task is to present such complete conjectures as proven theorems.</p><p>Before we part, I would like to invite you to consider the following way of doing justice to computing's radical novelty in an introductory programming course.</p><p>On the one hand, we teach what looks like the predicate calculus, but we do it very differently from the philosophers. In order to train the novice programmer in the manipulation of uninterpreted formulae, we teach it more as boolean algebra, familiarizing the student with all algebraic properties of the logical connectives. To further sever the links to intuition, we rename the values {true, false} of the boolean domain as {black, white}.</p><p>On the other hand, we teach a simple, clean, imperative programming language, with a skip and a multiple assignment as basic statements, with a block structure for local variables, the semicolon as operator for statement composition, a nice alternative construct, a nice repetition and, if so desired, a procedure call. To this we add a minimum of data types, say booleans, integers, characters and strings. The essential thing is that, for whatever we introduce, the corresponding semantics is defined by the proof rules that go with it.</p><p>Right from the beginning, and all through the course, we stress that the programmer's task is not just to write down a program, but that his main task is to give a formal proof that the program he proposes meets the equally formal functional specification. While designing proofs and programs hand in hand, the student gets ample opportunity to perfect his manipulative agility with the predicate calculus. Finally, in order to drive home the message that this introductory programming course is primarily a course in formal mathematics, we see to it that the programming language in question has not been implemented on campus so that students are protected from the temptation to test their programs. And this concludes the sketch of my proposal for an introductory programming course for freshmen.</p><p>This is a serious proposal, and utterly sensible. Its only disadvantage is that it is too radical for many, who, being unable to accept it, are forced to invent a quick reason for dismissing it, no matter how invalid. I'll give you a few quick reasons.</p><p>You don't need to take my proposal seriously because it is so ridiculous that I am obviously completely out of touch with the real world. But that kite won't fly, for I know the real world only too well: the problems of the real world are primarily those you are left with when you refuse to apply their effective solutions. So, let us try again.</p><p>You don't need to take my proposal seriously because it is utterly unrealistic to try to teach such material to college freshmen. Wouldn't that be an easy way out? You just postulate that this would be far too difficult. But that kite won't fly either for the postulate has been proven wrong: since the early 80's, such an introductory programming course has successfully been given to hundreds of college freshmen each year. [Because, in my experience, saying this once does not suffice, the previous sentence should be repeated at least another two times.] So, let us try again.</p><p>Reluctantly admitting that it could perhaps be taught to sufficiently docile students, you yet reject my proposal because such a course would deviate so much from what 18-year old students are used to and expect that inflicting it upon them would be an act of educational irresponsibility: it would only frustrate the students. Needless to say, that kite won't fly either. It is true that the student that has never manipulated uninterpreted formulae quickly realizes that he is confronted with something totally unlike anything he has ever seen before. But fortunately, the rules of manipulation are in this case so few and simple that very soon thereafter he makes the exciting discovery that he is beginning to master the use of a tool that, in all its simplicity, gives him a power that far surpasses his wildest dreams.</p><p>Teaching to unsuspecting youngsters the effective use of formal methods is one of the joys of life because it is so extremely rewarding. Within a few months, they find their way in a new world with a justified degree of confidence that is radically novel for them; within a few months, their concept of intellectual culture has acquired a radically novel dimension. To my taste and style, that is what education is about. Universities should not be afraid of teaching radical novelties; on the contrary, it is their calling to welcome the opportunity to do so. Their willingness to do so is our main safeguard against dictatorships, be they of the proletariat, of the scientific establishment, or of the corporate elite.</p><p>prof. dr. Edsger W. Dijkstra\n\t\t\t\t\tDepartment of Computer Sciences<p>\n\t\t\t\t\tThe University of Texas at Austin</p>\n\t\t\t\t\tAustin, TX 78712-1188</p>","contentLength":36736,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1k4j6us/on_the_cruelty_of_really_teaching_computing/"},{"title":"Python's new t-strings","url":"https://davepeck.org/2025/04/11/pythons-new-t-strings/","date":1745255788,"author":"/u/ketralnis","guid":418,"unread":true,"content":"<p>Template strings, also known as t-strings, have been <a href=\"https://peps.python.org/pep-0750/\">officially accepted</a> as a feature in Python 3.14, which will ship in late 2025. 🎉</p><p>I’m excited; t-strings open the door to safer more flexible string processing in Python.</p><h4>What’s the big idea with t-strings?</h4><p>Since they were introduced in Python 3.6, <a href=\"https://docs.python.org/3/tutorial/inputoutput.html#formatted-string-literals\">f-strings</a> have become a  popular way to format strings. They are concise, readable, and powerful.</p><p>In fact, they’re  delightful that many developers use f-strings for everything… even when they shouldn’t!</p><p>Alas, f-strings are often dangerously (mis)used to format strings that contain user input. I’ve seen f-strings used for SQL (<code>f\"SELECT * FROM users WHERE name = '{user_name}'\"</code>) and for HTML (<code>f\"&lt;div&gt;{user_name}&lt;/div&gt;\"</code>). These are not safe! If  contains a malicious value, it can lead to <a href=\"https://owasp.org/www-community/attacks/SQL_Injection\">SQL injection</a> or <a href=\"https://owasp.org/www-community/attacks/xss/\">cross-site scripting</a>.</p><p>Template strings are a  of Python’s f-strings. Whereas f-strings immediately become a string, t-strings evaluate to a new type, <code>string.templatelib.Template</code>:</p><pre tabindex=\"0\" data-language=\"python\"><code></code></pre><p>Importantly,  instances are  strings. The  type does not provide its own  implementation, which is to say that calling  does not return a useful value. Templates  be processed before they can be used; that processing code can be written by the developer or provided by a library and can safely escape the dynamic content.</p><p>We can imagine a library that provides an  function that takes a  and returns a safely escaped string:</p><pre tabindex=\"0\" data-language=\"python\"><code></code></pre><p>Of course, t-strings are useful for more than just safety; they also allow for more flexible string processing. For example, that  function could return a new type, . It could also accept all sorts of useful substitutions in the HTML itself:</p><pre tabindex=\"0\" data-language=\"python\"><code></code></pre><p>If you’ve worked with JavaScript, t-strings may feel familiar. They are the pythonic parallel to JavaScript’s <a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Template_literals#tagged_templates\">tagged templates</a>.</p><h4>How do I work with t-strings?</h4><p>To support processing, s give developers access to the string and its interpolated values  they are combined into a final string.</p><p>The  and  properties of a  return tuples:</p><pre tabindex=\"0\" data-language=\"python\"><code></code></pre><p>There is always one more (possibly empty) string than value. That is,  and <code>t\"{name}\".strings == (\"\", \"\")</code>.</p><p>As a shortcut, it’s also possible to iterate over a :</p><pre tabindex=\"0\" data-language=\"python\"><code></code></pre><p>Developers writing complex processing code can also access the gory details of each interpolation:</p><pre tabindex=\"0\" data-language=\"python\"><code></code></pre><p>In addition to supporting the literal () form, s can also be instantiated directly:</p><pre tabindex=\"0\" data-language=\"python\"><code></code></pre><p>Strings and interpolations can be provided to the  constructor in any order.</p><h4>A simple t-string example</h4><p>Let’s say we wanted to write code to convert all substituted words into pig latin. All it takes is a simple function:</p><pre tabindex=\"0\" data-language=\"python\"><code></code></pre><h4>What’s next once t-strings ship?</h4><p>T-strings are a powerful new feature that will make Python string processing safer and more flexible. I hope to see them used in all sorts of libraries and frameworks, especially those that deal with user input.</p><p>In addition, I hope that the tooling ecosystem will adapt to support t-strings. For instance, I’d love to see  and  format t-string , and  those contents, if they’re a common type like HTML or SQL.</p>","contentLength":2997,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1k4iwkq/pythons_new_tstrings/"},{"title":"How does OAuth work: ELI5?","url":"https://github.com/LukasNiessen/oauth-explained","date":1745252410,"author":"/u/trolleid","guid":421,"unread":true,"content":"<p>So I was reading about OAuth to learn it and have created this explanation. It's basically a few of the best I have found merged together and rewritten in big parts. I have also added a  and a . Maybe it helps one of you :-)</p><p>Let’s say LinkedIn wants to let users import their Google contacts.</p><p>One obvious (but terrible) option would be to just ask users to enter their Gmail email and password directly into LinkedIn. But giving away your actual login credentials to another app is a huge security risk.</p><p>OAuth was designed to solve exactly this kind of problem.</p><p>Note: So OAuth solves an authorization problem! Not an authentication problem. See [here][ref1] for the difference.</p><ul><li>User clicks “Import Google Contacts” on LinkedIn</li><li>LinkedIn redirects user to Google’s OAuth consent page</li><li>User logs in and approves access</li><li>Google redirects back to LinkedIn with a one-time code</li><li>LinkedIn uses that code to get an access token from Google</li><li>LinkedIn uses the access token to call Google’s API and fetch contacts</li></ul><p>Suppose LinkedIn wants to import a user’s contacts from their Google account.</p><ul><li>client_id is the before mentioned client id, so Google knows it's LinkedIn</li><li>redirect_uri is very important. It's used in step 6</li><li>in scope LinkedIn tells Google how much it wants to have access to, in this case the contacts of the user</li></ul><ol><li>The user will have to log in at Google</li><li>Google displays a consent screen: \"LinkedIn wants to access your Google contacts. Allow?\" The user clicks \"Allow\"</li><li>Now, LinkedIn makes a server-to-server request (not a redirect) to Google’s token endpoint and receive an access token (and ideally a refresh token)</li><li>. Now LinkedIn can use this access token to access the user’s Google contacts via Google’s API</li></ol><p><em>Why not just send the access token in step 6?</em></p><p> To make sure that the requester is actually LinkedIn. So far, all requests to Google have come from the user’s browser, with only the client_id identifying LinkedIn. Since the client_id isn’t secret and could be guessed by an attacker, Google can’t know for sure that it's actually LinkedIn behind this. In the next step, LinkedIn proves its identity by including the client_secret in a server-to-server request.</p><h2>Security Note: Encryption</h2><p>OAuth 2.0 does  handle encryption itself. It relies on HTTPS (SSL/TLS) to secure sensitive data like the client_secret and access tokens during transmission.</p><h2>Security Addendum: The state Parameter</h2><p>The state parameter is critical to prevent cross-site request forgery (CSRF) attacks. It’s a unique, random value generated by the third-party app (e.g., LinkedIn) and included in the authorization request. Google returns it unchanged in the callback. LinkedIn verifies the state matches the original to ensure the request came from the user, not an attacker.</p><h2>OAuth 1.0 vs OAuth 2.0 Addendum:</h2><p>OAuth 1.0 required clients to cryptographically sign every request, which was more secure but also much more complicated. OAuth 2.0 made things simpler by relying on HTTPS to protect data in transit, and using bearer tokens instead of signed requests.</p><h2>Code Example: OAuth 2.0 Login Implementation</h2><p>Below is a standalone Node.js example using Express to handle OAuth 2.0 login with Google, storing user data in a SQLite database.</p><p>```javascript const express = require(\"express\"); const axios = require(\"axios\"); const sqlite3 = require(\"sqlite3\").verbose(); const crypto = require(\"crypto\"); const jwt = require(\"jsonwebtoken\"); const jwksClient = require(\"jwks-rsa\");</p><p>const app = express(); const db = new sqlite3.Database(\":memory:\");</p><p>// Initialize database db.serialize(() =&gt; { db.run( \"CREATE TABLE users (id INTEGER PRIMARY KEY AUTOINCREMENT, name TEXT, email TEXT)\" ); db.run( \"CREATE TABLE federated_credentials (user_id INTEGER, provider TEXT, subject TEXT, PRIMARY KEY (provider, subject))\" ); });</p><p>// Configuration const CLIENT_ID = process.env.GOOGLE_CLIENT_ID; const CLIENT_SECRET = process.env.GOOGLE_CLIENT_SECRET; const REDIRECT_URI = \"<a href=\"https://example.com/oauth2/callback\">https://example.com/oauth2/callback</a>\"; const SCOPE = \"openid profile email\";</p><p>// Function to verify JWT async function verifyIdToken(idToken) { return new Promise((resolve, reject) =&gt; { jwt.verify( idToken, (header, callback) =&gt; { jwks.getSigningKey(header.kid, (err, key) =&gt; { callback(null, key.getPublicKey()); }); }, { audience: CLIENT_ID, issuer: \"<a href=\"https://accounts.google.com\">https://accounts.google.com</a>\", }, (err, decoded) =&gt; { if (err) return reject(err); resolve(decoded); } ); }); }</p><p>// Generate a random state for CSRF protection app.get(\"/login\", (req, res) =&gt; { const state = crypto.randomBytes(16).toString(\"hex\"); req.session.state = state; // Store state in session const authUrl = <code>https://accounts.google.com/o/oauth2/auth?client_id=${CLIENT_ID}&amp;redirect_uri=${REDIRECT_URI}&amp;scope=${SCOPE}&amp;response_type=code&amp;state=${state}</code>; res.redirect(authUrl); });</p><p>// OAuth callback app.get(\"/oauth2/callback\", async (req, res) =&gt; { const { code, state } = req.query;</p><p>// Verify state to prevent CSRF if (state !== req.session.state) { return res.status(403).send(\"Invalid state parameter\"); }</p><p>try { // Exchange code for tokens const tokenResponse = await axios.post( \"<a href=\"https://oauth2.googleapis.com/token\">https://oauth2.googleapis.com/token</a>\", { code, client_id: CLIENT_ID, client_secret: CLIENT_SECRET, redirect_uri: REDIRECT_URI, grant_type: \"authorization_code\", } );</p><pre><code>const { id_token } = tokenResponse.data; // Verify ID token (JWT) const decoded = await verifyIdToken(id_token); const { sub: subject, name, email } = decoded; // Check if user exists in federated_credentials db.get( \"SELECT * FROM federated_credentials WHERE provider = ? AND subject = ?\", [\"https://accounts.google.com\", subject], (err, cred) =&gt; { if (err) return res.status(500).send(\"Database error\"); if (!cred) { // New user: create account db.run( \"INSERT INTO users (name, email) VALUES (?, ?)\", [name, email], function (err) { if (err) return res.status(500).send(\"Database error\"); const userId = this.lastID; db.run( \"INSERT INTO federated_credentials (user_id, provider, subject) VALUES (?, ?, ?)\", [userId, \"https://accounts.google.com\", subject], (err) =&gt; { if (err) return res.status(500).send(\"Database error\"); res.send(`Logged in as ${name} (${email})`); } ); } ); } else { // Existing user: fetch and log in db.get( \"SELECT * FROM users WHERE id = ?\", [cred.user_id], (err, user) =&gt; { if (err || !user) return res.status(500).send(\"Database error\"); res.send(`Logged in as ${user.name} (${user.email})`); } ); } } ); </code></pre><p>} catch (error) { res.status(500).send(\"OAuth or JWT verification error\"); } });</p><p>app.listen(3000, () =&gt; console.log(\"Server running on port 3000\")); ```</p>","contentLength":6512,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1k4hg9j/how_does_oauth_work_eli5/"},{"title":"Pipelining might be my favorite programming language feature","url":"https://herecomesthemoon.net/2025/04/pipelining/","date":1745238056,"author":"/u/SophisticatedAdults","guid":419,"unread":true,"content":"<p><em> Don’t take it too seriously. Or do. idk, I can’t stop you.</em></p><p>Pipelining might be my favorite programming language feature.</p><p>\n              What is pipelining? Pipelining is the feature that allows you to omit a single argument from your\n              parameter list, by instead passing the previous value.\n            </p><p>When I say pipelining, I’m talking about the ability to write code like this:</p><div><pre tabindex=\"0\"><code data-lang=\"rust\"></code></pre></div><p>\n              As opposed to code like this. (This is not real Rust code. Quick challenge for the curious Rustacean, can\n              you explain why we cannot rewrite the above code like this, even if we import all of the symbols?)\n            </p><div><pre tabindex=\"0\"><code data-lang=\"rust\"></code></pre></div><p>\n              I honestly feel like this should be so obvious that it shouldn’t even be up for debate. The first code\n              example—with its nice ‘pipelining’ or ‘method chaining’ or whatever you want to call it—it\n              . It can be read line-by-line. It’s easy to annotate it with comments. It doesn’t\n              require introduction of new variables to become more readable since it’s already readable as is.\n            </p><p>\n              As opposed to, y’know,\n              <em>the first word in the line describing the final action our function performs</em>.\n            </p><p>\n              Let me make it very clear: This is an  about syntax. In practice,\n              <em>semantics beat syntax every day of the week</em>. In other words, don’t take it too seriously.\n            </p><p>\n              Second, this is not about imperative vs. functional programming. This article takes for granted that\n              you’re already on board with concepts such as ‘map’ and ‘filter’. It’s possible to overuse that style, but\n              I won’t talk about it here.\n            </p><h2>You already agree with me</h2><p>\n              Here is a feature that’s so bog-standard in modern programming languages that it barely feels like a\n              feature at all. Member access for structs or classes with our beloved friend the -operator.\n            </p><p>\n              This is a form of pipelining. It puts the data first, the operator in the middle, and concludes with the\n              action (restricting to a member field). That’s an instance of what I call pipelining.\n            </p><div><pre tabindex=\"0\"><code data-lang=\"go\"></code></pre></div><p>\n              You see what I am getting at, right? It’s the same principle. One of the reasons why\n              -style member access syntax (and -style method call syntax!) is popular\n              is since it’s easy to read and chains easily.\n            </p><p>\n              Let’s make the comparison slightly more fair, and pretend that we have to write .\n              Compare:\n            </p><div><pre tabindex=\"0\"><code data-lang=\"go\"></code></pre></div><p>\n              Which one of these is easier to read? The pipelined syntax, obviously. This example is easy to parse\n              either way, but imagine you’d like to blend out some information and purely focus on the final operation.\n            </p><div><pre tabindex=\"0\"><code data-lang=\"go\"></code></pre></div><p>\n              You see the problem, right? In the first example, we have ‘all of the previous stuff’ and then\n               to it. In the second example, the operation which we want to perform\n              () and the new operand () are spread out with ‘all of the previous stuff’\n              sitting between them.\n            </p><p>Looking back at our original example, the problem should be obvious:</p><div><pre tabindex=\"0\"><code data-lang=\"rust\"></code></pre></div><p>\n              I cannot deny the allegations: I just don’t think it makes sense to write code like that as long as a\n              clearly better option exists.\n            </p><p>\n              Why would I have to parse the whole line just to figure out where my input comes in, and why is the data\n              flow ‘from the inside to the outside’? It’s kind of silly, if you ask me.\n            </p><p>\n              Readability is nice, and I could add add a whole section complaining about the mess that’s Python’s\n              ‘functional’ features.\n            </p><p>\n              However, let’s take a step back and talk about ease of editing. Going back to the example above, imagine\n              you’d like to add another  (or any other function call) in the middle there. How easy is\n              this?\n            </p><div><pre tabindex=\"0\"><code data-lang=\"rust\"></code></pre></div><ol><li>\n                You’ll have to parse through the line, counting commas and parentheses to find the exact place to add\n                the closing parenthesis.\n              </li><li>\n                The  of this is going to be basically unreadable, everything is crammed onto one\n                line.\n              </li><li>This line is getting long and unreadable, and at that point you’ll want to refactor it anyway!</li></ol><div><pre tabindex=\"0\"><code data-lang=\"rust\"></code></pre></div><p>\n              This is adding a single line of code. No parentheses counting. It’s easy and obvious. It’s easy to write\n              and easy to review. Perhaps most importantly, it shows up  in the\n               layer of whatever editor or code exploration tool you’re using.\n            </p><p>\n              You might think that this issue is  about trying to cram everything onto a single line, but\n              frankly, trying to move away from that doesn’t help much. It will still mess up your git diffs and the\n              blame layer.\n            </p><p>\n              You can, of course, just assign the result of every  and  call to a\n              helper variable, and I will (begrudgingly) acknowledge that that works, and is\n               better than trying to do absurd levels of nesting.\n            </p><p>\n              When you press  in your IDE, it will show a neat little pop-up that tells you which methods\n              you can call or which fields you can access.\n            </p><p>\n              This is probably the single IDE feature with the biggest value add, and if not that, then at least the\n              single most frequently used one. Some people will tell you that static analysis for namespace or\n              module-level code discovery is useless in the age of AI autocompletion and vibe coding, but I very much\n              disagree.</p><blockquote><p>\n                “grug very like type systems make programming easier. for grug, type systems most value when grug hit\n                dot on keyboard and list of things grug can do pop up magic. this 90% of value of type system or more to\n                grug” — grug\n              </p></blockquote><p>\n              Words to live by. What he’s describing here is something that essentially  pipelining to\n              work at all. (And types or type annotation, but having those is the direction the industry is moving in\n              anyway.)\n            </p><p>\n              It doesn’t matter if it’s the trusty  operator, C++’s , or if it’s\n              something more bespoke such as Elm’s or Gleam’s  or Haskell’s . In the\n              end, it’s a pipeline operator—the same principle applies. If your\n              <a href=\"https://en.wikipedia.org/wiki/Language_Server_Protocol\" target=\"_blank\">LSP</a> knows the type of\n              what’s on the left, it  in principle be able to offer suggestions for what to do next.\n            </p><p>\n              If your favorite language’s LSP/IDE does a poor job at offering suggestions during pipelining, then it’s\n              probably one of the following reasons:\n            </p><ol><li>\n                You don’t know which type you’re even holding. This happens most often when the language is dynamically\n                typed, ’types’ are hard to deduce with static analysis, and you’re touching/writing code without type\n                annotations. (e.g. Python)\n              </li><li>\n                The ecosystem and LSP just didn’t have enough time put into them, or most active users don’t care\n                enough. (e.g. any sufficiently obscure language)\n              </li><li>\n                You are in a situation in which even looking up which methods are available is hard, often due to a\n                bespoke build process that confuses the editor. (e.g. basically any build or runtime generation of code,\n                or bespoke loading/selection of libraries).\n              </li></ol><p>\n              In either case, great editor/LSP support is more or less considered mandatory for modern programming\n              languages. And of course, this is where pipelining shines.\n            </p><p>\n              Ask any IDE, autocompleting <code>fizz.bu... -&gt; fizz.buzz()</code> is  than\n              autocompleting , for the obvious reason that you\n              <em>didn’t even write  in the second example yet</em>, so your editor has less\n              information to work with.\n            </p><p>\n              Pipelining is  at data processing, and allows you to transform code that’s commonly\n              written with ‘inside-out’ control flow into ’line-by-line’ transformations.\n            </p><p>\n              Where could this possibly be more clear than in SQL, the presumably single most significant language for\n              querying and aggregating complex large-scale datasets?\n            </p><p>\n              You’ll be pleased to hear that, yes, people are in fact working on bringing pipelining to SQL. (Whether\n              it’s actually going to happen in this specific form\n              <a href=\"https://sqlite.org/forum/forumpost/2d2720461b82f2fd\" target=\"_blank\">is a different question</a>,\n              let’s not get too carried away here.)\n            </p><p>\n              Unless you’re one of those people who spends so much time dealing with SQL that it’s become second nature,\n              and the thought that the control flow of nested queries is hard to follow for the average non-database\n              engineer is incomprehensible to you, I guess.\n            </p><p>I’ll put their example of how a standard nested query can be simplified here, for convenience:</p><div><pre tabindex=\"0\"><code data-lang=\"sql\"></code></pre></div><p>Versus the SQL Syntax she told you not to worry about:</p><div><pre tabindex=\"0\"><code data-lang=\"sql\"></code></pre></div><p>\n              Less nesting. More aligned with other languages and\n              <a href=\"https://learn.microsoft.com/en-us/dotnet/csharp/linq/\" target=\"_blank\">LINQ</a>. Can easily be\n              read line-by-line.\n            </p><p>\n              Here’s a more\n              <a href=\"https://www.linkedin.com/pulse/not-so-good-idea-pipe-syntax-sql-franck-pachot-dx6he\" target=\"_blank\">skeptical voice (warning, LinkedIn!)</a>. Franck Pachot raises the great point that the  statement at the top of a query is\n              (essentially) its function signature and specifies the return type. With pipe syntax, you lose some of\n              this readability.\n            </p><p>I agree, but that seems like a solvable problem to me.</p><p>\n              And—surprise, surprise—it fits pretty well into pipelining. Any situation where you need to construct a\n              complex, stateful object (e.g. a client or runtime), it’s a great way to feed complex, optional arguments\n              into an object.\n            </p><p>\n              Some people say they prefer optional/named arguments, but honestly, I don’t understand why: An optional\n              named  parameter is harder to track down in code (and harder to mark as deprecated!) than\n              all instances of a  builder function.\n            </p><p>\n              If you have no clue what I’m talking about, this here is the type of pattern I’m talking about. You have a\n              ‘builder’ object, call some methods on it to configure it, and finally  the object\n              you’re actually interested in.\n            </p><div><pre tabindex=\"0\"><code data-lang=\"rs\"></code></pre></div><h2>Making Haskell (slightly more) readable</h2><p>\n              It has these weird operators like , , , or\n               and when you ask Haskell programmers about what they mean, they say something like\n              “Oh, this is just a special case of the generalized\n              <a href=\"https://hackage.haskell.org/package/base-4.12.0.0/docs/Control-Monad.html#v:-62--61--62-\" target=\"_blank\">Kleisli Monad Operator</a> in the category of endo-pro-applicatives over a locally small poset.” and your eyes\n              have glazed over before they’ve even finished the sentence.\n            </p><p>(It also doesn’t help that Haskell allows you to define custom operators however you please, yes.)</p><p>\n              If you’re wondering “How could a language have so many bespoke operators?”, my understanding is that most\n              of them are just fancy ways of telling Haskell to compose some functions in a highly advanced way. Here’s\n              the second-most basic\n              example, the  operator.\n            </p><p>\n              Imagine you have functions , , and some value . In a\n              “““normal””” language you might write . In Haskell, this is written as\n              . This is since  will automatically ‘grab’ values to the right as its\n              arguments, so you don’t need the parentheses.\n            </p><p>\n              A consequence of this is that  is written as  in\n              Haskell. If you wrote , the compiler will interpret it as\n              , which would be wrong. This is what people mean when they say that Haskell’s\n              function call syntax is\n              <a href=\"https://en.wikipedia.org/wiki/Associative_property\" target=\"_blank\">left-associative</a>.\n            </p><p>\n              The  operator is <em>nothing but syntactic sugar</em> that allows you to write\n               instead of having to write . That’s it. People were\n              fed-up with having to put parens everywhere, I guess.\n            </p><p>If your eyes glazed over at this point, I can’t blame you.</p><p>\n              Talking about any of the fancier operators would be punching well above my weight-class, so I’ll just\n              stick to what I’ve been saying throughout this entire post already. Here’s a stilted Haskell toy example,\n              intentionally not written in\n              <a href=\"https://wiki.haskell.org/Pointfree\" target=\"_blank\">pointfree</a> style.\n            </p><div><pre tabindex=\"0\"><code data-lang=\"haskell\"></code></pre></div><p>\n              If you want to figure out the flow of data, this whole function body has to be read\n              .\n            </p><p>\n              To make things even funnier, you need to start with the  clause to figure out which\n              local “variables” are being defined. This happens (for whatever reason) at the end of the function instead\n              of at the start. (Calling  a variable is misleading, but that’s besides the\n              point.)\n            </p><p>\n              At this point you might wonder if Haskell has some sort of pipelining operator, and yes, it turns out that\n              one was\n              <a href=\"https://github.com/haskell/core-libraries-committee/issues/78#issuecomment-1183568372\" target=\"_blank\">added in 2014</a>! That’s pretty late considering that Haskell exists since 1990. This allows us to refactor the above\n              code as follows:\n            </p><div><pre tabindex=\"0\"><code data-lang=\"haskell\"></code></pre></div><p>Isn’t that way easier to read?</p><p> is code which you can show to an enterprise Java programmer, tell them that they’re looking\n              at\n              <a href=\"https://docs.oracle.com/javase/8/docs/api/java/util/stream/Stream.html\" target=\"_blank\">Java Streams</a>\n              with slightly weird syntax, and they’ll get the idea.\n            </p><p>\n              Of course, in reality nothing is as simple. The Haskell ecosystem seems to be split between users of\n              , users of , and users of the\n              <a href=\"https://hackage.haskell.org/package/flow-2.0.0.0/docs/Flow.html#g:1\" target=\"_blank\">Flow</a>-provided operators, which allow the same functionality, but allow you to write\n               instead of .</p><p>\n              I don’t know what to say about that, other than that—not entirely unlike C++—Haskell has its own share of\n              operator-related and cultural historical baggage, and a split ecosystem, and this makes the language\n              significantly less approachable than it has to be.\n            </p><h2>Rust’s pipelining is pretty neat</h2><p>\n              In the beginning I said that ‘Pipelining is the feature that allows you to omit a single argument from\n              your parameter list, by instead passing the previous value.’\n            </p><p>\n              I still think that this is true, but it doesn’t get across the whole picture. If you’ve paid attention in\n              the previous sections, you’ll have noticed that  and\n               share basically  in common outside of the order of\n              operations.\n            </p><p>\n              In the first case, we’re accessing a value that’s  to the object. In the second, we’re\n              ‘just’ passing an expression to a free-standing function.\n            </p><p>\n              Or in other words, pipelining is not the same as pipelining. Even from an IDE-perspective, they’re\n              different. In Java, your editor will look for methods associated with an object and walk up the\n              inheritance chain. In Haskell, your editor will put a so-called\n              <a href=\"https://downloads.haskell.org/~ghc/7.10.3-rc1/users_guide/typed-holes.html\" target=\"_blank\">’typed hole’</a>, and try to deduce which functions have a type that ‘fits’ into the hole using\n              <a href=\"https://herecomesthemoon.net/2025/01/type-inference-in-rust-and-cpp/\" target=\"_blank\">Hindley-Milner Type Inference</a>.\n            </p><p>\n              Personally, I like type inference (and\n              <a href=\"https://en.wikipedia.org/wiki/Type_class\" target=\"_blank\">type classes</a>), but I also like if\n              types have a namespace attached to them, with methods and associated functions. I am pragmatic like that.\n            </p><p>\n              What I like about Rust is that it gives me the best out of both worlds here: You get traits and type\n              inference without needing to wrap your head around a fully functional, immutable, lazy, monad-driven\n              programming paradigm, and you get methods and associated values without the absolute dumpster fire of\n              complex inheritance chains or AbstractBeanFactoryConstructors.\n            </p><p>\n              I’ve not seen any other language that even comes close to the convenience of Rust’s pipelines, and its\n              lack of higher-kinded types or inheritance did not stop it. Quite the opposite, if anything.\n            </p><p>\n              I like pipelining. That’s the one thing that definitely should be obvious if you’ve read all the way\n              through this article.\n            </p><p>I just think they’re neat, y’know?</p><p>I like reading my code top-to-bottom, left-to-right instead of from-the-inside-to-the-outside.</p><p>\n              I like when I don’t need to count arguments and parentheses to figure out which value is the first\n              argument of the second function, and which is the second argument of the first function.\n            </p><p>\n              I like when my editor can show me all fields of a struct, and all methods or functions associated with a\n              value, just when I press  on my keyboard. It’s great.\n            </p><p>\n              I like when  and the  layer of the code repository don’t look like\n              complete ass.\n            </p><p>\n              I like when adding a function call in the middle of a process doesn’t require me to parse the whole line\n              to add the closing parenthesis, and doesn’t require me to adjust the nesting of the whole block.\n            </p><p>\n              I like when my functions distinguish between ‘a main value which we are acting upon’ and ‘secondary\n              arguments’, as opposed to treating them all as the same.\n            </p><p>\n              I like when I don’t have to pollute my namespaces with a ton of helper variables or free-standing\n              functions that I had to pull in from somewhere.\n            </p><p>\n              If you’re writing pipelined code—and not trying overly hard to fit everything into a single, convoluted,\n              nested pipeline—then your functions will naturally split up into a few pipeline chunks.\n            </p><p>\n              Each chunk starts with a piece of ‘main data’ that travels on a conveyer belt, where every line performs\n              exactly one action to transform it. Finally, a single value comes out at the end and gets its own name, so\n              that it may be used later.\n            </p><p>\n              And that is—in my humble opinion—exactly how it should be. Neat, convenient, separated ‘chunks’, each of\n              which can easily be understood in its own right.\n            </p><p><em>Thanks to kreest for proofreading this article.</em></p>","contentLength":19143,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1k4c5g5/pipelining_might_be_my_favorite_programming/"},{"title":"Getting Forked by Microsoft","url":"https://philiplaine.com/posts/getting-forked-by-microsoft/","date":1745237999,"author":"/u/starlevel01","guid":422,"unread":true,"content":"<p>Three years ago, I was part of a team responsible for developing and maintaining Kubernetes clusters for end user customers. A main source for downtime in customer environments occurred when image registries went down. The traditional way to solve this problem is to set up a stateful mirror, however we had to work within customer budget and time constraints which did not allow it. During a Black Friday, we started getting hit with a ton of traffic while GitHub container registries were down. This limited our ability to scale up the cluster as we depended on critical images from that registry. After this incident, I started thinking about a better way to avoid these scalability issues. A solution that did not need a stateful component and required minimal operational oversight. This is where the idea for <a href=\"https://github.com/spegel-org/spegel\">Spegel</a> came from.</p><p>As a sole maintainer of an open source project, I was enthused when Microsoft reached out to set up a meeting to talk about Spegel. The meeting went well, and I felt there was going to be a path forward ripe with cooperation and hopefully a place where I could onboard new maintainers. I continued discussions with one of the Microsoft engineers, helping them get Spegel running and answering any architecture questions they had. At the time I was positive as I saw it as a possibility for Micorosft to contribute back changes based on their learnings. As time went on, silence ensued, and I assumed work priorities had changed.</p><p>It was not until KubeCon Paris where I attended a talk that piqued my interest. The talk was about strategies to speed up image distribution where one strategy discussed was P2P sharing. The topics in the abstract sounded similar to Spegel so I was excited to hear other’s ideas about the problem. During the talk, I was enthralled seeing Spegel, my own project, be discussed as a P2P image sharing solution. When <a href=\"https://github.com/Azure/peerd\">Peerd</a>, a peer to peer distributor of container content in Kubernetes clusters made by Microsoft, was mentioned I quickly researched it. At the bottom of the README there was a thank you to myself and Spegel. This acknowledgement made it look like they had taken some inspiration from my project and gone ahead and developed a version of their own.</p><p>While looking into Peerd, my enthusiasm for understanding different approaches in this problem space quickly diminished. I saw function signatures and comments that looked very familiar, as if I had written them myself. Digging deeper I found test cases referencing Spegel and my previous employer, test cases that have been taken directly from my project. References that are still present to this day. The project is a forked version of Spegel, maintained by Microsoft, but under Microsoft’s MIT license.</p><p>Spegel was published with an MIT license. Software released under an MIT license allows for forking and modifications, without any requirement to contribute these changes back. I default to using the MIT license as it is simple and permissive. The license does not allow removing the original license and purport that the code was created by someone else. It looks as if large parts of the project were copied directly from Spegel without any mention of the original source. I have included a short snippet comparing the code which adds the mirror configuration where even the function comments are the same.</p><p>A negative impact from the creation of Peerd is that it has created confusion among new users. I am frequently asked about the differences between Spegel and Peerd. As a maintainer, it is my duty to come across as unbiased and factual as possible, but this tumultuous history makes it challenging. Microsoft carries a large brand recognition, so it has been difficult for Spegel to try and take up space next to such a behemoth.</p><p>As an open source maintainer I have dedicated ample time to community requests, bug fixes, and security fixes. In my conversation with Microsoft I was open to collaboration to continue building out a tool to benefit the open source community. Over the years I have contributed to a multitude of open source projects and created a few of my own. Spegel was the first project I created from the ground up that got some traction and seemed to be appreciated by the community. Seeing my project being forked by Microsoft made me feel like I was no longer useful. For a while I questioned if it was even worth continuing working on Spegel.</p><p>Luckily, I persisted. Spegel still continues strong with over 1.7k stars and 14.4 million pulls since its first release over two years ago. However, I am not the first and unfortunately not the last person to come across this David versus Goliath-esque experience. How can sole maintainers work with multi-billion corporations without being taken advantage of? With the changes of Hashicorp licensing having a rippling effect through the open source community, along with the strong decline in investment in open source as a whole, how does the community prevail? As an effort to fund the work on Spegel I have enabled GitHub sponsors. This experience has also made me consider changing the license of Spegel, as it seems to be the only stone I can throw.</p>","contentLength":5156,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1k4c4t0/getting_forked_by_microsoft/"},{"title":"PostgreSQL JSONB - Powerful Storage for Semi-Structured Data","url":"https://www.architecture-weekly.com/p/postgresql-jsonb-powerful-storage","date":1745224541,"author":"/u/Adventurous-Salt8514","guid":420,"unread":true,"content":"<p><strong>Object-oriented or relational? You’ve seen this battle in your projects.</strong><a href=\"https://event-driven.io/en/in_the_defence_of_orms/\" rel=\"\">Object-Relational Mappers</a></p><p>Still, those databases didn’t give us the consistency guarantees we were comfortable with, like transactions, isolation levels, etc. </p><p><strong>PostgreSQL added the JSONB column type to answer the question: </strong></p><p>Then other databases like MySQL and SQLite followed.</p><p>Let’s discuss today how it works, and if it’s the actual answer.</p><p><strong>First, terminology: the B in JSON stands for Binary</strong></p><ul><li><p><strong>JSON stores data as text,</strong></p></li><li><p><strong>JSONB uses a binary representation</strong></p></li></ul><p><strong>The binary format of JSONB enables efficient indexing and querying and eliminates the need for repeated parsing. </strong></p><p><strong>You can think of a JSONB column as a table inside the column. Each path to property can be seen logically as a different column.</strong></p><p>To visualise this conceptual model, imagine if our database could access any path directly without scanning the entire document:</p><pre><code><code>+----+---------------+--------------------------+--------------------+\n| id | customer.name | customer.contact.address | email              |\n+----+---------------+--------------------------+--------------------+\n| 1  | \"John Smith\"  | \"Portland\"               | \"john@example.com\" | \n+----+---------------+--------------------------+--------------------+</code></code></pre><p> Of course, that’s a bit of an oversimplification. </p><p>PostgreSQL doesn't create these virtual columns, but the binary representation effectively allows it to access paths directly without scanning the whole document. This approach provides several benefits:</p><ol><li><p>Path traversal is much faster than parsing text</p></li><li><p>Fields can be indexed individually</p></li><li><p>Query performance is more predictable</p></li></ol><p>When Postgresql stores a JSONB document, it doesn't simply dump JSON text into a field. It transforms the document into a binary representation.</p><p>Have a look on the following JSON:</p><pre><code></code></pre><p>This hierarchical structure could be flattened into a set of paths for each nested structure:</p><pre><code><code>Path: customer.name = \"John Smith\"\nPath: customer.contact.email = \"john@example.com\"\nPath: customer.contact.phone = \"555-1234\"\nPath: customer.contact.address.city = \"Portland\"\nPath: customer.contact.address.country = \"USA\"\n</code></code></pre><p><strong>The goal is: to performantly read a specific field without parsing the entire document.</strong></p><p><strong>JSONB's document is stored as a hierarchical, tree-like structure of key-value pairs, each containing metadata about its type and actual data.</strong></p><p><strong>When new JSON is stored, it has to be parsed and converted from text to key-value pairs. This conversion happens through a process called \"tokenisation</strong></p><p>For example, using our customer data example:</p><pre><code><code>{\n  \"id\": \"cust_web_123\",\n  \"email\": \"web_user@example.com\",\n  \"source\": \"website\",\n  \"web_data\": {\n    \"first_visit\": \"2023-03-12T15:22:47Z\",\n    \"utm_source\": \"google_search\"\n  },\n  \"contact\": {\n    \"phone\": \"+1234567890\"\n  }\n}\n</code></code></pre><p>Would be tokenised and stored with internal tree-like structures tracking:</p><pre><code><strong>Value: Object {\n      Count: 1,\n      Children: [</strong></code></pre><p><strong>Of course, it’s a bit simplified form, but you can think that each array element is actually a different node, and the index in the array is just a nested path.</strong></p><p><strong>PostgreSQL's JSONB tokenisation is sneaky. It not only parses data but also preserves the actual data types of values.</strong></p><ul><li><p>ensure that data maintains its semantic meaning,</p></li><li><p>enable type-specific operations (like numeric comparisons),</p></li><li><p>avoid type conversion when not needed.</p></li></ul><p><strong>What’s more, it’s not limited to JSON types.</strong></p><pre><code><code>customer_data-&gt;'customer'-&gt;'contact'-&gt;'address'-&gt;&gt;'city'</code></code></pre><p>PostgreSQL can navigate directly to that specific token  without scanning the entire document. It uses the described hierarchical structure, and gets the exact value with the type.</p><p><strong>When PostgreSQL encounters such query, it:</strong></p><ol><li><p>Parses the path into individual segments.</p></li><li><p>Locates the root object or array in the binary structure.</p></li><li><p>Each path segment computes a hash of the key.</p></li><li><p>Uses the hash to look up the corresponding entry in the structure.</p></li><li><p>Navigates to the next level if needed.</p></li><li><p>Extracts and returns the value in the requested format.</p></li></ol><p>This algorithm is heavily optimised for typical access patterns. For simple path expressions, PostgreSQL can retrieve values with near-constant time complexity, regardless of document size. However, the performance characteristics become more variable for more complex expressions, especially those involving arrays or filtering.</p><p><strong>Understanding this internal path representation explains why some JSONB queries perform better than others.</strong></p><p>To see how JSONB actually works, let's discuss a scenario that manages customer data.</p><p>Here's how a typical customer model might look in TypeScript:</p><pre><code><strong>// Different structure depending on acquisition channel\n  // At least one of these will be present per customer</strong></code></pre><p>If you tried to model it with relational tables, you'd face a classic data modelling challenge. You'd end up with either:</p><ul><li><p><strong>A complex set of tables with joins:</strong></p></li><li><p><strong>A wide table with mostly NULL values:</strong></p></li></ul><p>With JSONB, you can implement this in PostgreSQL without forcing every customer profile into the same rigid structure:</p><pre><code></code></pre><p>This allows storing profiles with wildly different shapes while keeping common fields queryable. The SQL implementation might look like:</p><pre><code><strong>INSERT INTO customers (id, email, customer_data) VALUES</strong><strong>-- Mobile customer with different structure</strong></code></pre><p>Despite the varying structures, you can still query across all customer types:</p><pre><code><strong>customer_data-&gt;&gt;'source' AS source,\n\n    COALESCE(\n        customer_data-&gt;'web_data'-&gt;&gt;'first_visit',\n        customer_data-&gt;'mobile_data'-&gt;&gt;'app_version',\n        customer_data-&gt;'crm_data'-&gt;&gt;'account_manager'\n    ) AS source_specific_data,\n</strong><strong>customer_data-&gt;'contact'-&gt;'address'-&gt;&gt;'city' AS city,\n</strong><strong>jsonb_array_length(customer_data-&gt;'purchases') AS purchase_count\n</strong><strong>FROM customers\nWHERE (customer_data-&gt;'contact'-&gt;'address'-&gt;&gt;'country') = 'USA';</strong></code></pre><p><em>WHERE (customer_data-&gt;'contact'-&gt;'address'-&gt;&gt;'country') = 'USA'</em></p><p>When getting such WHERE query, PostgreSQL does following steps:</p><ol><li><p>Extracts the string value at that path</p></li></ol><p>With a traditional JSON text column, this would require parsing the entire document. With JSONB, PostgreSQL can navigate directly to the relevant portion of the binary structure. </p><p>The data in the SELECT statement is processed similarly.</p><p><a href=\"https://www.postgresql.org/docs/current/functions-json.html#FUNCTIONS-SQLJSON-TABLE\" rel=\"\">documentation</a></p><p><strong>Thanks to the binary, hierarchical structure we just described, PostgreSQL can index JSONB data.</strong></p><p>Creating a GIN index is straightforward:</p><pre><code><code>CREATE INDEX idx_customer_data ON customers USING GIN (customer_data);\n</code></code></pre><p><strong>When PostgreSQL builds this index, it:</strong></p><ol><li><p>Iterates through every JSONB document in the table</p></li><li><p>Extracts each key and value pair at every level of nesting</p></li><li><p>Creates index entries for each key and for each scalar value</p></li><li><p>Organises these entries in a tree structure optimised for lookups</p></li></ol><p>The resulting index is usefull for certain operations like:</p><pre><code><strong>customer_data @&gt; '{\"tags\": [\"premium\"]}'</strong><strong>customer_data ? 'promotion_code'</strong><strong>WHERE customer_data ?| array['discount', 'coupon', 'promotion_code']</strong></code></pre><p>These queries can use the GIN index directly without extracting paths, making them highly efficient even with large datasets. The execution plan would show something like:</p><pre><code><code>Bitmap Heap Scan on customers  (cost=4.26..8.27 rows=1 width=32)\n  Recheck Cond: (customer_data @&gt; '{\"tags\": [\"premium\"]}'::jsonb)\n  -&gt;  Bitmap Index Scan on idx_customer_data  (cost=0.00..4.26 rows=1 width=0)\n        Index Cond: (customer_data @&gt; '{\"tags\": [\"premium\"]}'::jsonb)\n</code></code></pre><p>However, GIN indexes have significant drawbacks:</p><ol><li><p>They can be large, sometimes larger than the table itself.</p></li><li><p>They can slow down write operations.</p></li><li><p>They require more maintenance (vacuum).</p></li><li><p>They don't help with range queries or sorting.</p></li></ol><p>For each key or scalar value, the GIN index maintains a posting list of row IDs where that element appears. These posting lists are compressed and optimised for fast intersection operations, which is why containment checks are so efficient.</p><p>When PostgreSQL executes a containment query like:</p><pre><code>customer_data @&gt; '{\"status\": \"active\", \"type\": \"business\"}</code></pre><ol><li><p>Looks up the posting list for the key \"status\" with value \"active\"</p></li><li><p>Looks up the posting list for the key \"type\" with value \"business\"</p></li><li><p>Computes the intersection of these posting lists</p></li><li><p>Returns the matching row IDs</p></li></ol><p>This set-based operation is highly efficient compared to extracting and comparing values from each document.</p><p><strong><a href=\"https://www.postgresql.org/docs/current/btree.html\" rel=\"\">B-Tree index</a></strong></p><pre><code><code>-- Index for a specific extracted field\nCREATE INDEX idx_customer_source ON customers ((customer_data-&gt;&gt;'source'));\n</code></code></pre><p><em>(customer_data-&gt;&gt;'source')</em></p><p>The execution plan for a query using this index would show:</p><pre><code><code>Index Scan using idx_customer_source on customers  (cost=0.28..8.29 rows=1 width=32)\n  Index Cond: ((customer_data-&gt;&gt;'source'::text) = 'website'::text)\n</code></code></pre><ul><li><p>Are much smaller than GIN indexes,</p></li><li><p>Have less impact on write performance,</p></li><li><p>Support range queries and sorting,</p></li><li><p>Only help with queries that exactly match the indexed expression.</p></li></ul><p>What’s more, you can also use it to set up unique constraints and get the same strong checks as for regular tables!</p><p>Understanding the internal mechanisms helps explain why you might want different index types for different query patterns.</p><p>For more specific query patterns, you can combine these approaches:</p><pre><code><code>-- Location-based index for frequently filtered fields\nCREATE INDEX idx_customer_country ON customers ((customer_data-&gt;'contact'-&gt;'address'-&gt;&gt;'country'));\n\n-- Specialized conditional index for specific customer types\nCREATE INDEX idx_web_utm ON customers ((customer_data-&gt;'web_data'-&gt;&gt;'utm_source'))\nWHERE customer_data-&gt;&gt;'source' = 'website';\n</code></code></pre><p><em>customer_data-&gt;&gt;'source' = 'website'</em></p><pre><code><code>-- Query that can use the partial index\nSELECT id FROM customers \nWHERE customer_data-&gt;&gt;'source' = 'website' \nAND customer_data-&gt;'web_data'-&gt;&gt;'utm_source' = 'google';\n</code></code></pre><p>Internally, PostgreSQL maintains separate metadata about which index portions apply to which queries. The query planner uses statistics about data distribution to decide whether to use an index or not, which explains why sometimes PostgreSQL might choose a sequential scan even when an index exists.</p><p>Suppose we have a table with 1 million customer records, and we frequently run queries to find customers from specific countries:</p><pre><code><code>-- Without an index, this requires scanning all records\nSELECT id FROM customers \nWHERE customer_data-&gt;'contact'-&gt;'address'-&gt;&gt;'country' = 'Germany';\n</code></code></pre><p>The execution plan would show:</p><pre><code><code>Seq Scan on customers  (cost=0.00..24053.00 rows=10000 width=32)\n  Filter: ((customer_data-&gt;'contact'-&gt;'address'-&gt;&gt;'country'::text) = 'Germany'::text)\n</code></code></pre><p>After adding an appropriate index:</p><pre><code><code>CREATE INDEX idx_country ON customers ((customer_data-&gt;'contact'-&gt;'address'-&gt;&gt;'country'));\n</code></code></pre><p>The execution plan becomes:</p><pre><code><code>Index Scan using idx_country on customers  (cost=0.42..341.50 rows=10000 width=32)\n  Index Cond: ((customer_data-&gt;'contact'-&gt;'address'-&gt;&gt;'country'::text) = 'Germany'::text)\n</code></code></pre><p>This can reduce query time from seconds to milliseconds. But the choice between GIN and B-Tree indexes isn't always obvious. If we frequently need to find customers with specific combinations of attributes, a GIN index might be better:</p><pre><code><code>-- Query that benefits from a GIN index\nSELECT id FROM customers \nWHERE customer_data @&gt; '{\"contact\": {\"address\": {\"country\": \"Germany\"}}, \"status\": \"active\"}';\n</code></code></pre><p>Performance is good enough for most typical cases, and if you don’t have a huge amount of data, the performance of JSONB with indexing is similar to traditional columns. </p><p>Still, if it’s worth knowing that JSONB:</p><ul><li><p>Can perform worse for simple single-value lookups: Traditional columns with B-Tree indexes typically outperform JSONB path extraction,</p></li><li><p>can perform better for retrieving complete complex objects. Avoiding joins can lead to significant performance advantages,</p></li><li><p>Has variable performance for analytical queries: Depending on indexing strategy, can be either much faster or much slower than normalised data</p></li></ul><p><strong>While JSONB offers flexibility, it has performance characteristics you should understand, especially for large documents</strong><a href=\"https://www.postgresql.org/docs/current/storage-toast.html\" rel=\"\">TOAST (The Oversized-Attribute Storage Technique)</a></p><p><strong>A common anti-pattern is storing large arrays or deeply nested structures in a single JSONB document.</strong></p><p>You don't have to abandon all data validation when using JSONB. PostgreSQL allows you to enforce constraints on JSONB documents:</p><pre><code><strong>ALTER TABLE customers ADD CONSTRAINT valid_customer\nCHECK (\n    customer_data ? 'id' AND \n    customer_data ? 'email' AND \n    customer_data ? 'source' AND\n    customer_data ? 'contact'\n)</strong><strong>ALTER TABLE customers ADD CONSTRAINT valid_web_customer\nCHECK (\n    customer_data-&gt;&gt;'source' != 'website' OR \n    (customer_data ? 'web_data' AND customer_data-&gt;'web_data' ? 'first_visit')\n)</strong></code></pre><p>These constraints leverage PostgreSQL's ability to check path existence and values within the binary representation. Internally, PostgreSQL evaluates these expressions during insert and update operations, using the same path traversal algorithms that power queries.</p><p>PostgreSQL 12+ introduces a powerful new way to work with JSONB through the SQL/JSON path language. This provides a more expressive syntax for complex data extraction:</p><pre><code><code>SELECT jsonb_path_query(\n    customer_data, \n    '$.purchases[*] ? (@.amount &gt; 10 &amp;&amp; @.subscription == true)'\n) \nFROM customers;\n</code></code></pre><p>This query finds all purchases that are subscriptions with an amount greater than 10. The path expression is evaluated against each document, and matching elements are returned.</p><p>Internally, PostgreSQL parses the path expression into an execution plan specific to JSON path traversal.</p><p>JSONB works well with PostgreSQL's aggregation functions:</p><pre><code><code>-- Find average purchase amount across all customers\nSELECT AVG(\n    (jsonb_array_elements(customer_data-&gt;'purchases')-&gt;&gt;'amount')::numeric\n) \nFROM customers;\n\n-- Group purchases by product type\nSELECT \n    COALESCE(p-&gt;&gt;'product_id', p-&gt;&gt;'item_id') AS product,\n    COUNT(*) \nFROM \n    customers,\n    jsonb_array_elements(customer_data-&gt;'purchases') AS p\nGROUP BY \n    product;\n</code></code></pre><p>Of course, you can mix traditional columns with JSONB ones. </p><p><strong>For those columns that you know will always exist, or are the same, you can set up regular tables, and those with weaker schema, you can use JSONB.</strong></p><pre><code><code>CREATE TABLE customers (\n    id TEXT PRIMARY KEY,\n    email TEXT UNIQUE NOT NULL,\n    created_at TIMESTAMP NOT NULL DEFAULT NOW(),\n    source TEXT NOT NULL,\n    -- Common fields as columns\n    -- Variable parts as JSONB\n    source_data JSONB NOT NULL,\n    contact_info JSONB NOT NULL,\n    purchases JSONB NOT NULL\n);\n</code></code></pre><p>This hybrid approach gives you the best of both worlds: schema enforcement where it matters and flexibility where needed. It's often the most practical approach in real-world systems. </p><p>The hybrid aligns with PostgreSQL's internal implementation. Traditional columns can use the highly optimised columnar storage and indexing mechanisms PostgreSQL has refined over decades, while JSONB columns leverage the specialised binary representation for flexible data.</p><p><strong>JSONB represents a practical compromise between rigid relational schemas and completely schemaless document databases.</strong></p><p>Of course, in scenarios requiring absolute top performance, like business analytics with complex filtering, data warehousing, or very high-throughput OLTP systems, traditional columns with specialised indexes often perform better. In practice, a hybrid approach works best: use traditional columns for fixed, frequently queried attributes and JSONB for variable parts of your data.</p><p><strong>Big strength of JSONB is handling schema evolution.</strong></p><p>With JSONB, you can evolve your schema more gracefully. When business requirements change and you need to store additional data, you don't need to modify the database schema. Your application can simply start including new fields in the JSON documents it writes, and older documents without those fields continue to work fine.</p><p>I’m biased, but I extremely like JSONB and have used it in the past in my system. It can speed up development for line-of-business applications. Syntax can be a bit tricky, but once you learn it, or provide some wrapper it works like a charm.</p><p><a href=\"https://github.com/event-driven-io/pongo\" rel=\"\">https://github.com/event-driven-io/pongo</a></p><p>Pongo treats PostgreSQL as a Document Database, benefiting from JSONB support. Unlike the plain text storage of the traditional JSON type, JSONB stores JSON data in a binary format. This simple change brings significant advantages in terms of performance and storage efficiency.</p><p><strong>See also my video where I explained internals of both Pongo and JSONB:</strong></p><p><strong>p.s. Ukraine is still under brutal Russian invasion. A lot of Ukrainian people are hurt, without shelter and need help.</strong><a href=\"https://savelife.in.ua/en/donate/\" rel=\"\">Ukraine humanitarian </a><a href=\"https://www.gofundme.com/f/help-to-save-the-lives-of-civilians-in-a-war-zone\" rel=\"\">Ambulances for Ukraine</a><a href=\"https://redcross.org.ua/en/\" rel=\"\">Red Cross</a></p>","contentLength":16328,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1k48mey/postgresql_jsonb_powerful_storage_for/"}],"tags":["dev","reddit"]}