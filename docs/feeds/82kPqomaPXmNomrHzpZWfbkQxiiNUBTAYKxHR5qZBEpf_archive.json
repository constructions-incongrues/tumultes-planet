{"id":"82kPqomaPXmNomrHzpZWfbkQxiiNUBTAYKxHR5qZBEpf","title":"Hacker News: Show HN","displayTitle":"HN Show","url":"https://hnrss.org/show?points=60","feedLink":"https://news.ycombinator.com/shownew","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":17,"items":[{"title":"Show HN: Open Codex – OpenAI Codex CLI with open-source LLMs","url":"https://github.com/codingmoh/open-codex","date":1745258223,"author":"codingmoh","guid":198,"unread":true,"content":"<p>I’ve built Open Codex, a fully local, open-source alternative to OpenAI’s Codex CLI.</p><p>My initial plan was to fork their project and extend it. I even started doing that. But it turned out their code has several leaky abstractions, which made it hard to override core behavior cleanly. Shortly after, OpenAI introduced breaking changes. Maintaining my customizations on top became increasingly difficult.</p><p>So I rewrote the whole thing from scratch using Python. My version is designed to support local LLMs.</p><p>Right now, it only works with phi-4-mini (GGUF) via lmstudio-community/Phi-4-mini-instruct-GGUF, but I plan to support more models. Everything is structured to be extendable.</p><p>At the moment I only support single-shot mode, but I intend to add interactive (chat mode), function calling, and more.</p><p>You can install it using Homebrew:</p><pre><code>   brew tap codingmoh/open-codex\n   brew install open-codex\n\n</code></pre>\nIt's also published on PyPI:\nSource: <a href=\"https://github.com/codingmoh/open-codex\">https://github.com/codingmoh/open-codex</a>","contentLength":971,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43754620"},{"title":"Show HN: Nerdlog – Fast, multi-host TUI log viewer with timeline histogram","url":"https://github.com/dimonomid/nerdlog","date":1745235490,"author":"dimonomid","guid":196,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43750765"},{"title":"Show HN: Keep your PyTorch model in VRAM by hot swapping code","url":"https://github.com/valine/training-hot-swap/","date":1745194887,"author":"valine","guid":195,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43747560"},{"title":"Show HN: JuryNow – Get an anonymous instant verdict from 12 real people","url":"https://jurynow.app/","date":1745173949,"author":"sarah-brussels","guid":194,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43745554"},{"title":"Show HN: Undercutf1 – F1 Live Timing TUI with Driver Tracker, Variable Delay","url":"https://github.com/JustAman62/undercut-f1","date":1745049036,"author":"deltaknight","guid":192,"unread":true,"content":"<p>undercutf1 is a F1 live timing app, built as a TUI. It contains traditional timing pages like a Driver Tracker, Timing Tower, Race Control, along with some more detailed analysis like lap and gap history, so that you can see strategies unfolding.</p><p>I started to build undercutf1 almost two years ago, after becoming increasingly frustrated with the TV direction and lack of detailed information coming out of the live feed. Overtakes were often missed and strategies were often ill-explained or missed. I discovered that F1 live timing data is available over a simple SignalR stream, so I set out building an app that would let me see all the information I could dream of. Now undercutf1 serves as the perfect companion (like a second Martin Brundle) when I'm watching the sessions live.</p><p>If you want to test it out, you replay the Suzuka race easily by downloading the timing data, then starting a simulated session:</p><p>1. Download undercutf1 using the installation instructions in the README.</p><p>2. Import the Suzuka race session data using `undercutf1 import 2025 -m 1256 -s 10006`.</p><p>3. Start the app (`undercutf1`) then press S (Session) then F (Simulated Session), then select Suzuka then Race using the arrow keys, then press Enter.</p><p>4. Use arrow keys to navigate between the timing pages, and use N / Shift+N to fast-forward through the session.</p><p>If you want to test it out during this weekends Jeddah GP, simply install as in the README then start a live session by pressing S (Session) then L (Live Session).</p><p>The app is built for a terminal of roughly 110x30 cells, which probably seems an odd size but just so happens to be the size of a fullscreen terminal on a MBP zoomed in far enough that the text is easily glanceable when the laptop is placed on a coffee table some distance away from me :) Other terminal sizes will work fine, but information density/scaling may not be ideal.</p><p>If you're using the TUI during a live session, you'll want to synchronise the delay of the timing feed to your TV feed. Use the N/M keys to increase/decrease the delay. During non-race session, I find it fairly easy to sync the session clock on TV with the session clock on the bottom left of the timing screen. For race sessions, synchronisation is a little harder. I usually aim to sync the start of the race time (e.g. 13:00 on the timing screen clock) with the start of the formation lap, where the live feed helpfully shows the clock tick over to 0 minutes. I usually delay the feed by 30 to 60 seconds.</p>","contentLength":2481,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43734910"},{"title":"Show HN: Goldbach Conjecture up to 4*10^18+7*10^13","url":"https://medium.com/@jay_gridbach/grid-computing-shatters-world-record-for-goldbach-conjecture-verification-1ef3dc58a38d","date":1745043097,"author":"jay_gridbach","guid":191,"unread":true,"content":"<div><h2>A new challenge to a 280-year-old unsolved mathematical problem</h2></div><p>I’ve achieved a new world record in verifying the Goldbach Conjecture, a famous unsolved problem in mathematics, by extending the verification up to 4 quintillion (4×10¹⁸) + 70 trillion (7×10¹³). This article introduces Gridbach, the grid computing system I developed for this computation.</p><p>Check out my live grid computing system here:<a href=\"https://gridbach.com\" rel=\"noopener ugc nofollow\" target=\"_blank\">https://gridbach.com</a></p><p>No login is required. You can immediately see the computation results on both PC and mobile.</p><p>I am @jay_gridbach, a freelance engineer and consultant based in Kanagawa, Japan. Currently, I work as a PreSales engineer at a company in Tokyo, while also developing web applications and my own services. Gridbach is a project I’ve been nurturing since my corporate days, and I’m thrilled to release it in April 2025.</p><p>The <a href=\"https://en.wikipedia.org/wiki/Goldbach%27s_conjecture\" rel=\"noopener ugc nofollow\" target=\"_blank\">Goldbach Conjecture</a>, proposed by Prussian mathematician Christian Goldbach in 1742, remains an unsolved problem in mathematics. Despite the efforts of numerous modern mathematicians, no one has proven it mathematically. The conjecture itself is simple enough for a middle school student to understand:</p><blockquote><p><em>“Every even natural number greater than 2 is the sum of two prime numbers”</em></p><p><em>4 = 2 + 26 = 3 + 3………<p>1000000000001092576 = 1913 + 1000000000001090663</p></em>…</p></blockquote><p>While it’s believed to be true, a mathematical proof for all even numbers is still elusive.</p><p>In 2013, T. Oliveira e Silva from Portugal verified the conjecture up to 4×10¹⁸ using computers. This was the previous world record.<a href=\"https://sweet.ua.pt/tos/goldbach.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">https://sweet.ua.pt/tos/goldbach.html</a></p><p>My newly developed Gridbach system has slightly surpassed this record by adding 70 trillion to the verified range. I aim to push this further to 5 quintillion by increasing the number of participating machines and improving my algorithms. I’m not sure how to get this recognized as an official record, but I’m willing to write a paper if necessary. If anyone has insights on this, please let me know.</p><ul><li>Gridbach is a cloud-based distributed computing system accessible from any PC or smartphone.</li><li>It requires no login or app installation. The high-performance WASM (WebAssembly) binary code is downloaded as browser content, enabling computation on the user’s browser.</li><li>Each computation job covers a range of 100 million (50 million even numbers), taking about 5–10 seconds on a PC and 10–20 seconds on a smartphone.</li><li>Inspired by <a href=\"https://en.wikipedia.org/wiki/SETI@home\" rel=\"noopener ugc nofollow\" target=\"_blank\">SETI@home</a>, I’ve aimed to create a system that anyone can easily join.</li></ul><p>My system uses a combination of high-performance WASM for efficient computation and a highly scalable JAMStack architecture.</p><p>The challenges and lessons learned in choosing and setting up this stack are worth a separate post.</p><p>The app is simple, offering two main features: running computations on your machine and viewing the collective results from all Gridbach users. It’s also mobile-friendly.</p><p>For details on the dashboard data, please visit - <a href=\"https://app.gridbach.com/\" rel=\"noopener ugc nofollow\" target=\"_blank\">https://app.gridbach.com/</a></p><p>I define “Goldbach Ridge” as the maximum value of the smaller prime in the prime pairs satisfying the Goldbach Conjecture within a given range.</p><p>While T. Oliveira e Silva refers to these as the smaller prime  in the Goldbach partition, I named them as “ridge” as graphing these looks like mountain peaks and cols.</p><p>Oliveira e Silva al. discovered a large Goldbach ridge of 9781:</p><p>Feel free to join in and see if you can find some bigger peaks! So far, 6421 is the largest one found on my system. Your top 30 Goldbach Peaks are displayed on the My Calculation screen.</p><p>The core computation logic of mine is open-sourced as a Go command-line tool under the MIT license.<a href=\"https://github.com/nakatahr/gridbach-core\" rel=\"noopener ugc nofollow\" target=\"_blank\">https://github.com/nakatahr/gridbach-core</a></p><p>Here’s a snippet of the code. I’ve upgraded the Sieve of Eratosthenes to use bitwise operations on byte arrays for faster prime number generation, and optimized it for given ranges.</p><pre></pre><p>I also plan to write a separate article about the computation algorithm.</p><p>Thank you for reading! Please take 5 minutes to try the computation on <a href=\"https://gridbach.com\" rel=\"noopener ugc nofollow\" target=\"_blank\">gridbach.com</a>. I aim to continue breaking records and improving the system, hoping to spark interest in mathematics and IT.</p>","contentLength":4097,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43734583"},{"title":"Show HN: (bits) of a Libc, Optimized for Wasm","url":"https://github.com/ncruces/go-sqlite3/tree/main/sqlite3/libc","date":1744999585,"author":"ncruces","guid":190,"unread":true,"content":"<p>I make a no-CGO Go SQLite driver, by compiling the amalgamation to Wasm, then loading the result with wazero (a CGO-free Wasm runtime).</p><p>To compile SQLite, I use wasi-sdk, which uses wasi-libc, which is based on musl. It's been said that musl is slow(er than glibc), which is true, to a point.</p><p>musl uses SWAR on a size_t to implement various functions in string.h. This is fine, except size_t is just 32-bit on Wasm.</p><p>I found that implementing a few of those functions with Wasm SIMD128 can make them go around 4x faster.</p><p>Other functions don't even use SWAR; redoing  can make them 16x faster.</p><p>Smooth sort also has trouble pulling its own weight; a Shell sort seems both simpler and faster, while similarly avoiding recursion, allocations and the addressable stack.</p><p>I found that using SIMD intrinsics (rather than SWAR) makes it easier to avoid UB, but the code would definitely benefit from more eyeballs.</p>","contentLength":898,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43730458"},{"title":"Show HN: Attune - Build and publish APT repositories in seconds","url":"https://github.com/attunehq/attune","date":1744992858,"author":"ilikebits","guid":188,"unread":true,"content":"<p>Hey HN, we're Eliza and Xin, and we’ve been working on Attune. Attune is a tool for publishing Linux packages.</p><p>Previously, we worked at other startups building open source developer tools that ran on our customers’ CI and development machines. For many of them, being able to `apt-get install` our tools was a requirement.</p><p>When we went to actually set up APT repositories, we were really surprised by the state of tooling around package publishing. The open source tools we found were old, slow, and difficult to figure out how to run in CI. The commercial tools we found were not much better. The cloud-hosted vendors required us to provide our signing keys to a cloud vendor (which was a non-starter), while the self-hosted vendors required us to operate our own specialized hosting servers.</p><p>We just wanted something simple: sign locally, run quickly, be easy to use, and deploy to managed object storage.</p><p>We couldn’t find it, so we built it. If you want to try it out, you can create a repository with three commands:</p><pre><code>    attune repo create --uri https://apt.releases.example.com\n    attune repo pkg add --repo-id 123 package.deb\n    attune repo sync --repo-id 123\n</code></pre>\nYou can get the tool at <a href=\"https://github.com/attunehq/attune\">https://github.com/attunehq/attune</a>. There are a lot of rough edges right now since it's so new - sorry in advance, we're working on sanding those down.<p>It’s fully open source under Apache 2. We’re also working with some early customers to build enterprise features like audit logging, RBAC, and HSM integrations, and we’re thinking about building a managed cloud hosting service as well.</p><p>We’d love your feedback on whether this is useful for you, and what you’d like to see next. We’re well aware that publishing is a small piece of CI/CD, but we think a lot of the tooling in this area (publishing, artifact registries, package repositories) could really use some love.</p><p>What do you think? Comment here, or email us at founders@attunehq.com.</p>","contentLength":1944,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43729427"},{"title":"Show HN: AgentAPI – HTTP API for Claude Code, Goose, Aider, and Codex","url":"https://github.com/coder/agentapi","date":1744908898,"author":"hugodutka","guid":187,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43719447"},{"title":"Show HN: Plandex v2 – open source AI coding agent for large projects and tasks","url":"https://github.com/plandex-ai/plandex","date":1744838802,"author":"danenania","guid":186,"unread":true,"content":"<p>Hey HN! I’m Dane, the creator of Plandex (<a href=\"https://github.com/plandex-ai/plandex\">https://github.com/plandex-ai/plandex</a>), an open source AI coding agent focused especially on tackling large tasks in real world software projects.</p><p>Now I’m launching a major update, Plandex v2, which is the result of 8 months of heads down work, and is in effect a whole new project/product.</p><p>In short, Plandex is now a top-tier coding agent with fully autonomous capabilities. It combines models from Anthropic, OpenAI, and Google to achieve better results, more reliable agent behavior, better cost efficiency, and better performance than is possible by using only a single provider’s models.</p><p>I believe it is now one of the best tools available for working on large tasks in real world codebases with AI. It has an effective context window of 2M tokens, and can index projects of 20M tokens and beyond using tree-sitter project maps (30+ languages are supported). It can effectively find relevant context in massive million-line projects like SQLite, Redis, and Git.</p><p>A bit more on some of Plandex’s key features:</p><p>- Plandex has a built-in diff review sandbox that helps you get the benefits of AI without leaving behind a mess in your project. By default, all changes accumulate in the sandbox until you approve them. The sandbox is version-controlled. You can rewind it to any previous point, and you can also create branches to try out alternative approaches.</p><p>- It offers a ‘full auto mode’ that can complete large tasks autonomously end-to-end, including high level planning, context loading, detailed planning, implementation, command execution (for dependencies, builds, tests, etc.), and debugging.</p><p>- The autonomy level is highly configurable. You can move up and down the ladder of autonomy depending on the task, your comfort level, and how you weigh cost optimization vs. effort and results.</p><p>- Models and model settings are also very configurable. There are built-in models and model packs for different use cases. You can also add custom models and model packs, and customize model settings like temperature or top-p. All model changes are version controlled, so you can use branches to try out the same task with different models. The newly released OpenAI models and the paid Gemini 2.5 Pro model will be integrated in the default model pack soon.</p><p>- It can be easily self-hosted, including a ‘local mode’ for a very fast local single-user setup with Docker.</p><p>- Cloud hosting is also available for added convenience with a couple of subscription tiers: an ‘Integrated Models’ mode that requires no other accounts or API keys and allows you to manage billing/budgeting/spending alerts and track usage centrally, and a ‘BYO API Key’ mode that allows you to use your own OpenAI/OpenRouter accounts.</p><p>And of course I’d love to hear your feedback, whether positive or negative. Thanks so much!</p>","contentLength":2853,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43710576"},{"title":"Show HN: We Put Chromium on a Unikernel (OSS Apache 2.0)","url":"https://github.com/onkernel/kernel-images","date":1744809586,"author":"juecd","guid":185,"unread":true,"content":"<p>We’ve been building infrastructure to spin up browsers for AI agents. Originally, we built[0] it as a pool of warm Docker containers running Chromium, exposing:</p><p>- Chrome DevTools Protocol (for Playwright/Puppeteer)</p><p>We’ve been following the unikernel space for a while, so we decided to see if we could get our image on one. We went with Unikraft Cloud[1]. Here’s how it did:</p><p>- Boot-up time: 10–20ms (vs. ~5s for Docker containers)</p><p>- Near 0 CPU/memory consumption when idle</p><p>- Still ~8GB RAM when active (headful Chromium)</p><p>- Standby mode during long-running jobs: unikernels can sleep after X sec of inactivity, reducing clock time costs</p><p>- Session reuse: auth/session cookies persist for hours/days. Basically as long as the cookies are valid</p><p>- Cold start speed: good for low-latency, event-based handling</p><p>We open sourced it with Apache 2.0! Feel free to fork or submit an issue / PR. Open to feedback or suggestions. www.github.com/onkernel/kernel-images</p><p>[2] Thanks to the Unikraft Cloud team @fhuici @nderjung @razvandeax for helping us figure this out (we're not affiliated)</p><p>[3] (OPs) @rgarcia @juecd</p>","contentLength":1098,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43705144"},{"title":"Show HN: Torque – A lightweight meta-assembler for any processor","url":"https://benbridle.com/projects/torque.html","date":1744753605,"author":"benbridle","guid":184,"unread":true,"content":"<p>Torque is a lightweight meta-assembler that provides the tools necessary to write programs for any processor architecture.</p><p>For brief examples of more advanced programming techniques in Torque, see the <a href=\"https://benbridle.com/projects/torque/examples.html\">Examples</a> page.</p><p>Existing assemblers for proprietary platforms suffer from a number of issues. These assemblers tend to be poorly documented, provide languages that are clunky and verbose, be bloated and difficult to operate, or work only on one operating system. Development of C compilers is often a higher priority than the development of good assemblers.</p><p>Instead of learning a new assembler for every platform, it would be preferrable to instead use a single general-purpose assembler for every project. Torque was created to fill this niche.</p><p>Torque is designed around the idea that any assembly language can be emulated with just integers, bit sequences, labels, and sufficiently powerful macros. With Torque, the instruction encoding for a target processor can be defined as a set of macros in the program itself, using templates to specify how values are packed into ranges of bits. A program can be written for any processor using only Torque and the datasheet for that processor.</p><p>The latest release is available at <a href=\"https://benbridle.com/releases/tq-2.3.0\">tq-2.3.0</a> as a pre-compiled Linux executable.</p><p>To build the Torque assembler from source, first install the Rust compiler from <a href=\"https://www.rust-lang.org/tools/install\">www.rust-lang.org/tools/install</a>, then install the nightly toolchain with <code>rustup toolchain install nightly</code>, and then run <code>cargo +nightly build --release</code> inside the  source code directory.</p><p>The compiled binary will be created at .</p><p>The following command will assemble the Torque source file  and save the assembled output to the file  in the chosen format.</p><pre>tq [source] [destination] --format=&lt;format&gt;\n</pre><p>Torque is licensed under the <a href=\"https://en.wikipedia.org/wiki/MIT_License\">MIT License</a>. Feel free to use it, change it, and share it however you want.</p>","contentLength":1839,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43698801"},{"title":"Show HN: Resonate – real-time high temporal resolution spectral analysis","url":"https://alexandrefrancois.org/Resonate/","date":1744730773,"author":"arjf","guid":183,"unread":true,"content":"<p>Resonate builds on a resonator model that accumulates the signal contribution around its resonant frequency in the time domain using the Exponentially Weighted Moving Average (EWMA), also known as a low-pass filter in signal processing. Consistently with on-line perceptual signal analysis, the EWMA gives more weight to recent input values, whereas the contributions of older values decay exponentially.\nA compact, iterative formulation of the model affords computing an update at each signal input sample, requiring no buffering and involving only a handful of arithmetic operations.</p><p>Each resonator, characterized by its resonant frequency \\(f = \\frac{\\omega}{2\\pi}\\), is described by a complex number \\(R\\) whose amplitude captures the contribution of the input signal component around frequency \\(f\\).\nThe formulas below capture the recursive update for \\(R\\) by way of a phasor \\(P\\), applied for each sample \\(x\\) of a real-valued input signal \\(x(t) \\in [-1,1]\\), regularly sampled at sampling rate \\(sr\\). \\(\\Delta t=1/sr\\) is the sample duration, and \\(\\alpha \\in [0,1]\\) is a constant parameter that dictates how much each new measurement affects the accumulated value.</p>\n\n\\[P \\leftarrow P e^{-i \\omega \\Delta t}\\]\n\n\\[R \\leftarrow (1-\\alpha) R + \\alpha x P\\]\n\n<p>The two complex numbers \\(P\\) and \\(R\\) capture the full state of the resonator. Updating the state at each input signal sample only requires a handful of arithmetic operations. Calculating the power and/or magnitude is not necessary for the update, and can be carried out only when required by the application, relatively efficiently as well.\nThe single parameter \\(\\alpha\\), which can be related to a time constant, governs the dynamics of the system. For the frequency range of interest in audio applications (20-20000 Hz), the function \\(\\alpha_f = 1-e^{-\\Delta t\\frac{f}{log(1+f)} }\\) is a reasonable heuristic.\nThe smoothed state \\(\\tilde{R}\\) is produced by applying the EMWA to \\(R\\) with the same \\(\\alpha\\) to dampen power and phase oscillations.\nFinally, the output of each resonator is optionally normalized by the total response across the bank to a step signal of the resonator’s frequency (equalization).</p><p>Banks of resonators, independently tuned to perceptually relevant frequency scales, compute an instantaneous, perceptually relevant estimate of the spectral content of an input signal in real-time.\nBoth memory and per-sample computational complexity of such a bank are linear in the number of resonators, and independent of the number of input samples processed, or duration of processed signal.\nFurthermore, since the resonators are independent, there is no constraint on the tuning of their resonant frequencies or time constants, and all  computations can be parallelized across resonators.\nIn an offline processing context, the cumulative computational cost for a given duration increases linearly with the number of input samples processed.</p><p>Spectral information as a function of time is typically presented graphically for human consumption in the form of a spectrogram, in which the horizontal axis represents time and the vertical axis represents frequency. The value at each point represents the power of the frequency in the input signal at the given time slice. These values are usually normalized by the maximum value over the signal, and mapped to a logarithmic color scale to produce plots like those shown below.\nA Resonate oscillator bank with adequately tuned resonators computes an arbitrary frequency scale spectrogram directly and efficiently, with more relevant frequency resolution and much higher temporal resolutiont than FFT-based methods.</p><p>Alexandre R.J. François,\n“Resonate: Efficient Low Latency Spectral Analysis of Audio Signals,”\nto appear in Proceedings of the 50th Anniversary of the International Computer Music Conference 2025,\nBoston, MA, USA, 8-14 June 2025.</p><ul><li><p>The open source python module <a href=\"https://github.com/alexandrefrancois/noFFT\">noFFT</a> provides python and C++ implementations of Resonate functions and Jupyter notebooks illustrating their use in offline settings.</p></li></ul>","contentLength":4047,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43694157"},{"title":"Show HN: Unsure Calculator – back-of-a-napkin probabilistic calculator","url":"https://filiph.github.io/unsure/","date":1744705379,"author":"filiph","guid":182,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43690289"},{"title":"Show HN: MCP-Shield – Detect security issues in MCP servers","url":"https://github.com/riseandignite/mcp-shield","date":1744694101,"author":"nick_wolf","guid":181,"unread":true,"content":"<p>I noticed the growing security concerns around MCP (<a href=\"https://news.ycombinator.com/item?id=43600192\">https://news.ycombinator.com/item?id=43600192</a>) and built an open source tool that can detect several patterns of tool poisoning attacks, exfiltration channels and cross-origin manipulations.</p><p>MCP-Shield scans your installed servers (Cursor, Claude Desktop, etc.) and shows what each tool is trying to do at the instruction level, beyond just the API surface. It catches hidden instructions that try to read sensitive files, shadow other tools' behavior, or exfiltrate data.</p><p>Example of what it detects:</p><p>- Hidden instructions attempting to access ~/.ssh/id_rsa</p><p>- Cross-origin manipulations between server that can redirect WhatsApp messages</p><p>- Tool shadowing that overrides behavior of other MCP tools</p><p>- Potential exfiltration channels through optional parameters</p><p>I've included clear examples of detection outputs in the README and multiple example vulnerabilities in the repo so you can see the kinds of things it catches.</p><p>This is an early version, but I'd appreciate feedback from the community, especially around detection patterns and false positives.</p>","contentLength":1095,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43689178"},{"title":"Show HN: Zero-codegen, no-compile TypeScript type inference from Protobufs","url":"https://github.com/nathanhleung/protobuf-ts-types","date":1744645263,"author":"18nleung","guid":180,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43682547"},{"title":"Show HN: ActorCore – Stateful serverless framework that runs anywhere","url":"https://github.com/rivet-gg/actor-core","date":1744642645,"author":"NathanFlurry","guid":179,"unread":true,"content":"<p>Hey HN! Today we're launching ActorCore (<a href=\"https://actorcore.org/\" rel=\"nofollow\">https://actorcore.org/</a>), a stateful serverless framework that enables deploying Cloudflare Durable Object-like workloads to any cloud.</p><p>If you're unfamiliar with stateful serverless: it's like the actor model, where each actor maintains its own isolated, persistent state. (Think Lambda functions with local storage &amp; runs indefinitely.) It enables easily building long-running, realtime, durable, or local-first backends with the flexibility of serverless infrastructure. The most widely used implementation is Cloudflare Durable Objects, powering products like Clerk, Tldraw, Liveblocks, and Playroom.</p><p>- Vendor lock-in: Developers are hesitant to adopt a new programming model if there's no clear off-ramp. While it's straightforward to migrate a Postgres database, stateful serverless platforms can feel locked-in due to lack of viable alternatives.</p><p>- Ecosystem: Choosing a well-known database like Postgres comes with a mature ecosystem. Adopting a new model means rebuilding tooling and patterns from scratch.</p><p>- Conceptual gap: Many developers have spent their entire careers designing systems with intentionally separated state and compute. A model that merges the two can feel backwards at first.</p><p>We realized the best solution was to build a stateful serverless framework that can (a) be portable across clouds and (b) be easily extended – similar to how Hono created a unified API for traditional serverless functions across different providers. Thus, ActorCore was born.</p><p>Today, ActorCore supports running stateful serverless on Rivet Actors, Cloudflare Durable Objects, Redis, and standalone Node.js/Bun. The most common use cases include applications using collaborative, AI agent, local-first, and per-tenant database features.</p><p>State in ActorCore is an in-memory JavaScript object, with SQLite support coming later this month. State is stored in memory near your users on the edge, removing database round trips and minimizing latency due to physical distance. Additionally, state writes have tunable consistency, which enables both fully durable compute workloads and high-frequency state updates.</p><p>You may have noticed that this still leaves concern #3: the conceptual gap. While this isn't something we can solve with a framework, I've been spending ~40% of my time working on docs, content, and examples to help resolve this. ActorCore is also turning out to be community-driven as hoped, which enables more people to try and share their experience with stateful serverless.</p><p>We'd love it if you'd give ActorCore a try, read the roadmap, and/or let us know where we can improve anything. If you're hesitant about trying stateful serverless, I'd love to learn more in the comments. Looking forward to feedback!</p>","contentLength":2755,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43682030"}],"tags":["dev","hn"]}