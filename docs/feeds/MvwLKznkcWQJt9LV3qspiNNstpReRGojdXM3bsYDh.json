{"id":"MvwLKznkcWQJt9LV3qspiNNstpReRGojdXM3bsYDh","title":"Kubernetes Blog","displayTitle":"Dev - Kubernetes Blog","url":"https://kubernetes.io/feed.xml","feedLink":"https://kubernetes.io/","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":5,"items":[{"title":"Kubernetes Multicontainer Pods: An Overview","url":"https://kubernetes.io/blog/2025/04/22/multi-container-pods-overview/","date":1745280000,"author":"","guid":735,"unread":true,"content":"<p>As cloud-native architectures continue to evolve, Kubernetes has become the go-to platform for deploying complex, distributed systems. One of the most powerful yet nuanced design patterns in this ecosystem is the sidecar pattern—a technique that allows developers to extend application functionality without diving deep into source code.</p><h2>The origins of the sidecar pattern</h2><p>Think of a sidecar like a trusty companion motorcycle attachment. Historically, IT infrastructures have always used auxiliary services to handle critical tasks. Before containers, we relied on background processes and helper daemons to manage logging, monitoring, and networking. The microservices revolution transformed this approach, making sidecars a structured and intentional architectural choice.\nWith the rise of microservices, the sidecar pattern became more clearly defined, allowing developers to offload specific responsibilities from the main service without altering its code. Service meshes like Istio and Linkerd have popularized sidecar proxies, demonstrating how these companion containers can elegantly handle observability, security, and traffic management in distributed systems.</p><h2>Kubernetes implementation</h2><p>In Kubernetes, <a href=\"https://kubernetes.io/docs/concepts/workloads/pods/sidecar-containers/\">sidecar containers</a> operate within\nthe same Pod as the main application, enabling communication and resource sharing.\nDoes this sound just like defining multiple containers along each other inside the Pod? It actually does, and\nthis is how sidecar containers had to be implemented before Kubernetes v1.29.0, which introduced\nnative support for sidecars.\nSidecar containers can now be defined within a Pod manifest using the  field. What makes\nit a sidecar container is that you specify it with . You can see an example of this below, which is a partial snippet of the full Kubernetes manifest:</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>That field name,  may sound confusing. How come when you want to define a sidecar container, you have to put an entry in the  array?  are run to completion just before main application starts, so they’re one-off, whereas sidecars often run in parallel to the main app container. It’s the  with  which differs classic <a href=\"https://kubernetes.io/docs/concepts/workloads/pods/init-containers/\">init containers</a> from Kubernetes-native sidecar containers and ensures they are always up.</p><h2>When to embrace (or avoid) sidecars</h2><p>While the sidecar pattern can be useful in many cases, it is generally not the preferred approach unless the use case justifies it. Adding a sidecar increases complexity, resource consumption, and potential network latency. Instead, simpler alternatives such as built-in libraries or shared infrastructure should be considered first.</p><ol><li>You need to extend application functionality without touching the original code</li><li>Implementing cross-cutting concerns like logging, monitoring or security</li><li>Working with legacy applications requiring modern networking capabilities</li><li>Designing microservices that demand independent scaling and updates</li></ol><ol><li>Resource efficiency is your primary concern</li><li>Minimal network latency is critical</li><li>Simpler alternatives exist</li><li>You want to minimize troubleshooting complexity</li></ol><h2>Four essential multi-container patterns</h2><p>The  pattern is used to execute (often critical) setup tasks before the main application container starts. Unlike regular containers, init containers run to completion and then terminate, ensuring that preconditions for the main application are met.</p><ol><li>Verifying dependency availability</li><li>Running database migrations</li></ol><p>The init container ensures your application starts in a predictable, controlled environment without code modifications.</p><p>An ambassador container provides Pod-local helper services that expose a simple way to access a network service. Commonly, ambassador containers send network requests on behalf of a an application container and\ntake care of challenges such as service discovery, peer identity verification, or encryption in transit.</p><p><strong>Perfect when you need to:</strong></p><ol><li>Offload client connectivity concerns</li><li>Implement language-agnostic networking features</li><li>Add security layers like TLS</li><li>Create robust circuit breakers and retry mechanisms</li></ol><p>A  sidecar provides configuration updates to an application dynamically, ensuring it always has access to the latest settings without disrupting the service. Often the helper needs to provide an initial\nconfiguration before the application would be able to start successfully.</p><ol><li>Fetching environment variables and secrets</li><li>Polling configuration changes</li><li>Decoupling configuration management from application logic</li></ol><p>An  (or sometimes ) container enables interoperability between the main application container and external services. It does this by translating data formats, protocols, or APIs.</p><ol><li>Transforming legacy data formats</li><li>Bridging communication protocols</li><li>Facilitating integration between mismatched services</li></ol><p>While sidecar patterns offer tremendous flexibility, they're not a silver bullet. Each added sidecar introduces complexity, consumes resources, and potentially increases operational overhead. Always evaluate simpler alternatives first.\nThe key is strategic implementation: use sidecars as precision tools to solve specific architectural challenges, not as a default approach. When used correctly, they can improve security, networking, and configuration management in containerized environments.\nChoose wisely, implement carefully, and let your sidecars elevate your container ecosystem.</p>","contentLength":5279,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Introducing kube-scheduler-simulator","url":"https://kubernetes.io/blog/2025/04/07/introducing-kube-scheduler-simulator/","date":1743984000,"author":"","guid":734,"unread":true,"content":"<p>The Kubernetes Scheduler is a crucial control plane component that determines which node a Pod will run on.\nThus, anyone utilizing Kubernetes relies on a scheduler.</p><p><a href=\"https://github.com/kubernetes-sigs/kube-scheduler-simulator\">kube-scheduler-simulator</a> is a  for the Kubernetes scheduler, that started as a <a href=\"https://summerofcode.withgoogle.com/\">Google Summer of Code 2021</a> project developed by me (Kensei Nakada) and later received a lot of contributions.\nThis tool allows users to closely examine the scheduler’s behavior and decisions.</p><p>It is useful for casual users who employ scheduling constraints (for example, <a href=\"https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity/#affinity-and-anti-affinity\">inter-Pod affinity</a>)\nand experts who extend the scheduler with custom plugins.</p><p>The scheduler often appears as a black box,\ncomposed of many plugins that each contribute to the scheduling decision-making process from their unique perspectives.\nUnderstanding its behavior can be challenging due to the multitude of factors it considers.</p><p>Even if a Pod appears to be scheduled correctly in a simple test cluster, it might have been scheduled based on different calculations than expected. This discrepancy could lead to unexpected scheduling outcomes when deployed in a large production environment.</p><p>Also, testing a scheduler is a complex challenge.\nThere are countless patterns of operations executed within a real cluster, making it unfeasible to anticipate every scenario with a finite number of tests.\nMore often than not, bugs are discovered only when the scheduler is deployed in an actual cluster.\nActually, many bugs are found by users after shipping the release,\neven in the upstream kube-scheduler.</p><p>Having a development or sandbox environment for testing the scheduler — or, indeed, any Kubernetes controllers — is a common practice.\nHowever, this approach falls short of capturing all the potential scenarios that might arise in a production cluster\nbecause a development cluster is often much smaller with notable differences in workload sizes and scaling dynamics.\nIt never sees the exact same use or exhibits the same behavior as its production counterpart.</p><p>The kube-scheduler-simulator aims to solve those problems.\nIt enables users to test their scheduling constraints, scheduler configurations,\nand custom plugins while checking every detailed part of scheduling decisions.\nIt also allows users to create a simulated cluster environment, where they can test their scheduler\nwith the same resources as their production cluster without affecting actual workloads.</p><h2>Features of the kube-scheduler-simulator</h2><p>The kube-scheduler-simulator’s core feature is its ability to expose the scheduler's internal decisions.\nThe scheduler operates based on the <a href=\"https://kubernetes.io/docs/concepts/scheduling-eviction/scheduling-framework/\">scheduling framework</a>,\nusing various plugins at different extension points,\nfilter nodes (Filter phase), score nodes (Score phase), and ultimately determine the best node for the Pod.</p><p>The simulator allows users to create Kubernetes resources and observe how each plugin influences the scheduling decisions for Pods.\nThis visibility helps users understand the scheduler’s workings and define appropriate scheduling constraints.</p><p>Inside the simulator, a debuggable scheduler runs instead of the vanilla scheduler.\nThis debuggable scheduler outputs the results of each scheduler plugin at every extension point to the Pod’s annotations like the following manifest shows\nand the web front end formats/visualizes the scheduling results based on these annotations.</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>This debuggable scheduler can also run standalone, for example, on any Kubernetes cluster or in integration tests.\nThis would be useful to custom plugin developers who want to test their plugins or examine their custom scheduler in a real cluster with better debuggability.</p><h2>The simulator as a better dev cluster</h2><p>As mentioned earlier, with a limited set of tests, it is impossible to predict every possible scenario in a real-world cluster.\nTypically, users will test the scheduler in a small, development cluster before deploying it to production, hoping that no issues arise.</p><p><a href=\"https://github.com/kubernetes-sigs/kube-scheduler-simulator/blob/master/simulator/docs/import-cluster-resources.md\">The simulator’s importing feature</a>\nprovides a solution by allowing users to simulate deploying a new scheduler version in a production-like environment without impacting their live workloads.</p><p>By continuously syncing between a production cluster and the simulator, users can safely test a new scheduler version with the same resources their production cluster handles.\nOnce confident in its performance, they can proceed with the production deployment, reducing the risk of unexpected issues.</p><ol><li>: Examine if scheduling constraints (for example, PodAffinity, PodTopologySpread) work as intended.</li><li>: Assess how a cluster would behave with changes to the scheduler configuration.</li><li><strong>Scheduler plugin developers</strong>: Test a custom scheduler plugins or extenders, use the debuggable scheduler in integration tests or development clusters, or use the <a href=\"https://github.com/kubernetes-sigs/kube-scheduler-simulator/blob/simulator/v0.3.0/simulator/docs/import-cluster-resources.md\">syncing</a> feature for testing within a production-like environment.</li></ol><p>The simulator only requires Docker to be installed on a machine; a Kubernetes cluster is not necessary.</p><pre tabindex=\"0\"><code>git clone git@github.com:kubernetes-sigs/kube-scheduler-simulator.git\ncd kube-scheduler-simulator\nmake docker_up\n</code></pre><p>You can then access the simulator's web UI at .</p><p>The simulator has been maintained by dedicated volunteer engineers, overcoming many challenges to reach its current form.</p>","contentLength":5181,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Kubernetes v1.33 sneak peek","url":"https://kubernetes.io/blog/2025/03/26/kubernetes-v1-33-upcoming-changes/","date":1743013800,"author":"","guid":733,"unread":true,"content":"<p>As the release of Kubernetes v1.33 approaches, the Kubernetes project continues to evolve. Features may be deprecated, removed, or replaced to improve the overall health of the project. This blog post outlines some planned changes for the v1.33 release, which the release team believes you should be aware of to ensure the continued smooth operation of your Kubernetes environment and to keep you up-to-date with the latest developments. The information below is based on the current status of the v1.33 release and is subject to change before the final release date.</p><h2>The Kubernetes API removal and deprecation process</h2><p>The Kubernetes project has a well-documented <a href=\"https://kubernetes.io/docs/reference/using-api/deprecation-policy/\">deprecation policy</a> for features. This policy states that stable APIs may only be deprecated when a newer, stable version of that same API is available and that APIs have a minimum lifetime for each stability level. A deprecated API has been marked for removal in a future Kubernetes release. It will continue to function until removal (at least one year from the deprecation), but usage will result in a warning being displayed. Removed APIs are no longer available in the current version, at which point you must migrate to using the replacement.</p><ul><li><p>Generally available (GA) or stable API versions may be marked as deprecated but must not be removed within a major version of Kubernetes.</p></li><li><p>Beta or pre-release API versions must be supported for 3 releases after the deprecation.</p></li><li><p>Alpha or experimental API versions may be removed in any release without prior deprecation notice; this process can become a withdrawal in cases where a different implementation for the same feature is already in place.</p></li></ul><p>Whether an API is removed as a result of a feature graduating from beta to stable, or because that API simply did not succeed, all removals comply with this deprecation policy. Whenever an API is removed, migration options are communicated in the <a href=\"https://kubernetes.io/docs/reference/using-api/deprecation-guide/\">deprecation guide</a>.</p><h2>Deprecations and removals for Kubernetes v1.33</h2><h3>Deprecation of the stable Endpoints API</h3><p>The <a href=\"https://kubernetes.io/docs/concepts/services-networking/endpoint-slices/\">EndpointSlices</a> API has been stable since v1.21, which effectively replaced the original Endpoints API. While the original Endpoints API was simple and straightforward, it also posed some challenges when scaling to large numbers of network endpoints. The EndpointSlices API has introduced new features such as dual-stack networking, making the original Endpoints API ready for deprecation.</p><p>This deprecation only impacts those who use the Endpoints API directly from workloads or scripts; these users should migrate to use EndpointSlices instead. There will be a dedicated blog post with more details on the deprecation implications and migration plans in the coming weeks.</p><p>Following its deprecation in v1.31, as highlighted in the <a href=\"https://kubernetes.io/blog/2024/07/19/kubernetes-1-31-upcoming-changes/#deprecation-of-status-nodeinfo-kubeproxyversion-field-for-nodes-kep-4004-https-github-com-kubernetes-enhancements-issues-4004\">release announcement</a>, the <code>status.nodeInfo.kubeProxyVersion</code> field will be removed in v1.33. This field was set by kubelet, but its value was not consistently accurate. As it has been disabled by default since v1.31, the v1.33 release will remove this field entirely.</p><h3>Removal of host network support for Windows pods</h3><p>Windows Pod networking aimed to achieve feature parity with Linux and provide better cluster density by allowing containers to use the Node’s networking namespace.\nThe original implementation landed as alpha with v1.26, but as it faced unexpected containerd behaviours,\nand alternative solutions were available, the Kubernetes project has decided to withdraw the associated\nKEP. We're expecting to see support fully removed in v1.33.</p><h2>Featured improvement of Kubernetes v1.33</h2><p>As authors of this article, we picked one improvement as the most significant change to call out!</p><h3>Support for user namespaces within Linux Pods</h3><p>One of the oldest open KEPs today is <a href=\"https://kep.k8s.io/127\">KEP-127</a>, Pod security improvement by using Linux <a href=\"https://kubernetes.io/docs/concepts/workloads/pods/user-namespaces/\">User namespaces</a> for Pods. This KEP was first opened in late 2016, and after multiple iterations, had its alpha release in v1.25, initial beta in v1.30 (where it was disabled by default), and now is set to be a part of v1.33, where the feature is available by default.</p><p>This support will not impact existing Pods unless you manually specify  to opt in. As highlighted in the <a href=\"https://kubernetes.io/blog/2024/03/12/kubernetes-1-30-upcoming-changes/\">v1.30 sneak peek blog</a>, this is an important milestone for mitigating vulnerabilities.</p><h2>Selected other Kubernetes v1.33 improvements</h2><p>The following list of enhancements is likely to be included in the upcoming v1.33 release. This is not a commitment and the release content is subject to change.</p><h3>In-place resource resize for vertical scaling of Pods</h3><p>When provisioning a Pod, you can use various resources such as Deployment, StatefulSet, etc. Scalability requirements may need horizontal scaling by updating the Pod replica count, or vertical scaling by updating resources allocated to Pod’s container(s). Before this enhancement, container resources defined in a Pod's  were immutable, and updating any of these details within a Pod template would trigger Pod replacement.</p><p>But what if you could dynamically update the resource configuration for your existing Pods without restarting them?</p><p>The <a href=\"https://kep.k8s.io/1287\">KEP-1287</a> is precisely to allow such in-place Pod updates. It opens up various possibilities of vertical scale-up for stateful processes without any downtime, seamless scale-down when the traffic is low, and even allocating larger resources during startup that is eventually reduced once the initial setup is complete. This was released as alpha in v1.27, and is expected to land as beta in v1.33.</p><h3>DRA’s ResourceClaim Device Status graduates to beta</h3><p>The  field in ResourceClaim , originally introduced in the v1.32 release, is likely to graduate to beta in v1.33. This field allows drivers to report device status data, improving both observability and troubleshooting capabilities.</p><p>For example, reporting the interface name, MAC address, and IP addresses of network interfaces in the status of a ResourceClaim can significantly help in configuring and managing network services, as well as in debugging network related issues. You can read more about ResourceClaim Device Status in <a href=\"https://kubernetes.io/docs/concepts/scheduling-eviction/dynamic-resource-allocation/#resourceclaim-device-status\">Dynamic Resource Allocation: ResourceClaim Device Status</a> document.</p><h3>Ordered namespace deletion</h3><p>This KEP introduces a more structured deletion process for Kubernetes namespaces to ensure secure and deterministic resource removal. The current semi-random deletion order can create security gaps or unintended behaviour, such as Pods persisting after their associated NetworkPolicies are deleted. By enforcing a structured deletion sequence that respects logical and security dependencies, this approach ensures Pods are removed before other resources. The design improves Kubernetes’s security and reliability by mitigating risks associated with non-deterministic deletions.</p><h3>Enhancements for indexed job management</h3><p>These two KEPs are both set to graduate to GA to provide better reliability for job handling, specifically for indexed jobs. <a href=\"https://kep.k8s.io/3850\">KEP-3850</a> provides per-index backoff limits for indexed jobs, which allows each index to be fully independent of other indexes. Also, <a href=\"https://kep.k8s.io/3998\">KEP-3998</a> extends Job API to define conditions for making an indexed job as successfully completed when not all indexes are succeeded.</p><p>New features and deprecations are also announced in the Kubernetes release notes. We will formally announce what's new in <a href=\"https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.33.md\">Kubernetes v1.33</a> as part of the CHANGELOG for that release.</p><p>Kubernetes v1.33 release is planned for <strong>Wednesday, 23rd April, 2025</strong>. Stay tuned for updates!</p><p>You can also see the announcements of changes in the release notes for:</p><p>The simplest way to get involved with Kubernetes is by joining one of the many <a href=\"https://github.com/kubernetes/community/blob/master/sig-list.md\">Special Interest Groups</a> (SIGs) that align with your interests. Have something you’d like to broadcast to the Kubernetes community? Share your voice at our weekly <a href=\"https://github.com/kubernetes/community/tree/master/communication\">community meeting</a>, and through the channels below. Thank you for your continued feedback and support.</p>","contentLength":7780,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Fresh Swap Features for Linux Users in Kubernetes 1.32","url":"https://kubernetes.io/blog/2025/03/25/swap-linux-improvements/","date":1742925600,"author":"","guid":732,"unread":true,"content":"<p>Swap is a fundamental and an invaluable Linux feature.\nIt offers numerous benefits, such as effectively increasing a node’s memory by\nswapping out unused data,\nshielding nodes from system-level memory spikes,\npreventing Pods from crashing when they hit their memory limits,\nand <a href=\"https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/2400-node-swap/README.md#user-stories\">much more</a>.\nAs a result, the node special interest group within the Kubernetes project\nhas invested significant effort into supporting swap on Linux nodes.</p><p>The 1.22 release <a href=\"https://kubernetes.io/blog/2021/08/09/run-nodes-with-swap-alpha/\">introduced</a> Alpha support\nfor configuring swap memory usage for Kubernetes workloads running on Linux on a per-node basis.\nLater, in release 1.28, support for swap on Linux nodes has graduated to Beta, along with many\nnew improvements.\nIn the following Kubernetes releases more improvements were made, paving the way\nto GA in the near future.</p><p>Prior to version 1.22, Kubernetes did not provide support for swap memory on Linux systems.\nThis was due to the inherent difficulty in guaranteeing and accounting for pod memory utilization\nwhen swap memory was involved. As a result, swap support was deemed out of scope in the initial\ndesign of Kubernetes, and the default behavior of a kubelet was to fail to start if swap memory\nwas detected on a node.</p><p>In version 1.22, the swap feature for Linux was initially introduced in its Alpha stage.\nThis provided Linux users the opportunity to experiment with the swap feature for the first time.\nHowever, as an Alpha version, it was not fully developed and only partially worked on limited environments.</p><p>In version 1.28 swap support on Linux nodes was promoted to Beta.\nThe Beta version was a drastic leap forward.\nNot only did it fix a large amount of bugs and made swap work in a stable way,\nbut it also brought cgroup v2 support, introduced a wide variety of tests\nwhich include complex scenarios such as node-level pressure, and more.\nIt also brought many exciting new capabilities such as the  behavior\nwhich sets an auto-calculated swap limit to containers, OpenMetrics instrumentation\nsupport (through the  endpoint) and Summary API for\nVerticalPodAutoscalers (through the  endpoint), and more.</p><p>Today we are working on more improvements, paving the way for GA.\nCurrently, the focus is especially towards ensuring node stability,\nenhanced debug abilities, addressing user feedback,\npolishing the feature and making it stable.\nFor example, in order to increase stability, containers in high-priority pods\ncannot access swap which ensures the memory they need is ready to use.\nIn addition, the  behavior was removed since it might compromise\nthe node's health.\nSecret content protection against swapping has also been introduced\n(see relevant <a href=\"https://kubernetes.io/blog/2025/03/25/swap-linux-improvements/#memory-backed-volumes\">security-risk section</a> for more info).</p><p>To conclude, compared to previous releases, the kubelet's support for running with swap enabled\nis more stable and robust, more user-friendly, and addresses many known shortcomings.\nThat said, the NodeSwap feature introduces basic swap support, and this is just the beginning.\nIn the near future, additional features are planned to enhance swap functionality in various ways,\nsuch as improving evictions, extending the API, increasing customizability, and more!</p><p>In order for the kubelet to initialize on a swap-enabled node, the \nfield must be set to  on kubelet's configuration setting, or the deprecated\n command line flag must be deactivated.</p><p>It is possible to configure the  option to define the\nmanner in which a node utilizes swap memory.\nFor instance,</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>The currently available configuration options for  are:</p><ul><li> (default): Kubernetes workloads cannot use swap. However, processes\noutside of Kubernetes' scope, like system daemons (such as kubelet itself!) can utilize swap.\nThis behavior is beneficial for protecting the node from system-level memory spikes,\nbut it does not safeguard the workloads themselves from such spikes.</li><li>: Kubernetes workloads can utilize swap memory, but with certain limitations.\nThe amount of swap available to a Pod is determined automatically,\nbased on the proportion of the memory requested relative to the node's total memory.\nOnly non-high-priority Pods under the <a href=\"https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/#burstable\">Burstable</a>\nQuality of Service (QoS) tier are permitted to use swap.\nFor more details, see the <a href=\"https://kubernetes.io/blog/2025/03/25/swap-linux-improvements/#how-is-the-swap-limit-being-determined-with-limitedswap\">section below</a>.</li></ul><p>If configuration for  is not specified,\nby default the kubelet will apply the same behaviour as the  setting.</p><p>On Linux nodes, Kubernetes only supports running with swap enabled for hosts that use cgroup v2.\nOn cgroup v1 systems, all Kubernetes workloads are not allowed to use swap memory.</p><h2>Install a swap-enabled cluster with kubeadm</h2><p>It is required for this demo that the kubeadm tool be installed, following the steps outlined in the\n<a href=\"https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/\">kubeadm installation guide</a>.\nIf swap is already enabled on the node, cluster creation may proceed.\nIf swap is not enabled, please refer to the provided instructions for enabling swap.</p><h3>Create a swap file and turn swap on</h3><p>I'll demonstrate creating 4GiB of swap, both in the encrypted and unencrypted case.</p><h4>Setting up unencrypted swap</h4><p>An unencrypted swap file can be set up as follows.</p><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div><h4>Setting up encrypted swap</h4><p>An encrypted swap file can be set up as follows.\nBear in mind that this example uses the  binary (which is available\non most Linux distributions).</p><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div><h4>Verify that swap is enabled</h4><p>Swap can be verified to be enabled with both  command or the  command</p><pre tabindex=\"0\"><code>&gt; swapon -s\nFilename Type Size Used Priority\n/dev/dm-0 partition 4194300 0 -2\n</code></pre><pre tabindex=\"0\"><code>&gt; free -h\ntotal used free shared buff/cache available\nMem: 3.8Gi 1.3Gi 249Mi 25Mi 2.5Gi 2.5Gi\nSwap: 4.0Gi 0B 4.0Gi\n</code></pre><p>After setting up swap, to start the swap file at boot time,\nyou either set up a systemd unit to activate (encrypted) swap, or you\nadd a line similar to <code>/swapfile swap swap defaults 0 0</code> into .</p><h3>Set up a Kubernetes cluster that uses swap-enabled nodes</h3><p>To make things clearer, here is an example kubeadm configuration file  for the swap enabled cluster.</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>Then create a single-node cluster using <code>kubeadm init --config kubeadm-config.yaml</code>.\nDuring init, there is a warning that swap is enabled on the node and in case the kubelet\n is set to true. We plan to remove this warning in a future release.</p><h2>How is the swap limit being determined with LimitedSwap?</h2><p>The configuration of swap memory, including its limitations, presents a significant\nchallenge. Not only is it prone to misconfiguration, but as a system-level property, any\nmisconfiguration could potentially compromise the entire node rather than just a specific\nworkload. To mitigate this risk and ensure the health of the node, we have implemented\nSwap with automatic configuration of limitations.</p><p>With , Pods that do not fall under the Burstable QoS classification (i.e.\n/ QoS Pods) are prohibited from utilizing swap memory.\n QoS Pods exhibit unpredictable memory consumption patterns and lack\ninformation regarding their memory usage, making it difficult to determine a safe\nallocation of swap memory.\nConversely,  QoS Pods are typically employed for applications that rely on the\nprecise allocation of resources specified by the workload, with memory being immediately available.\nTo maintain the aforementioned security and node health guarantees,\nthese Pods are not permitted to use swap memory when  is in effect.\nIn addition, high-priority pods are not permitted to use swap in order to ensure the memory\nthey consume always residents on disk, hence ready to use.</p><p>Prior to detailing the calculation of the swap limit, it is necessary to define the following terms:</p><ul><li>: The total amount of physical memory available on the node.</li><li>: The total amount of swap memory on the node that is available for use by Pods (some swap memory may be reserved for system use).</li><li>: The container's memory request.</li></ul><p>Swap limitation is configured as:\n<code>(containerMemoryRequest / nodeTotalMemory) × totalPodsSwapAvailable</code></p><p>In other words, the amount of swap that a container is able to use is proportionate to its\nmemory request, the node's total physical memory and the total amount of swap memory on\nthe node that is available for use by Pods.</p><p>It is important to note that, for containers within Burstable QoS Pods, it is possible to\nopt-out of swap usage by specifying memory requests that are equal to memory limits.\nContainers configured in this manner will not have access to swap memory.</p><p>There are a number of possible ways that one could envision swap use on a node.\nWhen swap is already provisioned and available on a node,\nthe kubelet is able to be configured so that:</p><ul><li>It can start with swap on.</li><li>It will direct the Container Runtime Interface to allocate zero swap memory\nto Kubernetes workloads by default.</li></ul><p>Swap configuration on a node is exposed to a cluster admin via the\n<a href=\"https://kubernetes.io/docs/reference/config-api/kubelet-config.v1/\"> in the KubeletConfiguration</a>.\nAs a cluster administrator, you can specify the node's behaviour in the\npresence of swap memory by setting .</p><p>The kubelet employs the <a href=\"https://kubernetes.io/docs/concepts/architecture/cri/\">CRI</a>\n(container runtime interface) API, and directs the container runtime to\nconfigure specific cgroup v2 parameters (such as ) in a manner that will\nenable the desired swap configuration for a container. For runtimes that use control groups,\nthe container runtime is then responsible for writing these settings to the container-level cgroup.</p><h3>Node and container level metric statistics</h3><p>Kubelet now collects node and container level metric statistics,\nwhich can be accessed at the  (which is used mainly by monitoring\ntools like Prometheus) and  (which is used mainly by Autoscalers) kubelet HTTP endpoints.\nThis allows clients who can directly interrogate the kubelet to\nmonitor swap usage and remaining swap memory when using .\nAdditionally, a  metric has been added to cadvisor to show\nthe total physical swap capacity of the machine.\nSee <a href=\"https://kubernetes.io/docs/reference/instrumentation/node-metrics/\">this page</a> for more info.</p><h3>Node Feature Discovery (NFD)</h3><p><a href=\"https://github.com/kubernetes-sigs/node-feature-discovery\">Node Feature Discovery</a>\nis a Kubernetes addon for detecting hardware features and configuration.\nIt can be utilized to discover which nodes are provisioned with swap.</p><p>As an example, to figure out which nodes are provisioned with swap,\nuse the following command:</p><div><pre tabindex=\"0\"><code data-lang=\"shell\"></code></pre></div><p>This will result in an output similar to:</p><pre tabindex=\"0\"><code>k8s-worker1: true\nk8s-worker2: true\nk8s-worker3: false\n</code></pre><p>In this example, swap is provisioned on nodes  and , but not on .</p><p>Having swap available on a system reduces predictability.\nWhile swap can enhance performance by making more RAM available, swapping data\nback to memory is a heavy operation, sometimes slower by many orders of magnitude,\nwhich can cause unexpected performance regressions.\nFurthermore, swap changes a system's behaviour under memory pressure.\nEnabling swap increases the risk of noisy neighbors,\nwhere Pods that frequently use their RAM may cause other Pods to swap.\nIn addition, since swap allows for greater memory usage for workloads in Kubernetes that cannot be predictably accounted for,\nand due to unexpected packing configurations,\nthe scheduler currently does not account for swap memory usage.\nThis heightens the risk of noisy neighbors.</p><p>The performance of a node with swap memory enabled depends on the underlying physical storage.\nWhen swap memory is in use, performance will be significantly worse in an I/O\noperations per second (IOPS) constrained environment, such as a cloud VM with\nI/O throttling, when compared to faster storage mediums like solid-state drives\nor NVMe.\nAs swap might cause IO pressure, it is recommended to give a higher IO latency\npriority to system critical daemons. See the relevant section in the\n<a href=\"https://kubernetes.io/blog/2025/03/25/swap-linux-improvements/#good-practice-for-using-swap-in-a-kubernetes-cluster\">recommended practices</a> section below.</p><p>On Linux nodes, memory-backed volumes (such as <a href=\"https://kubernetes.io/docs/concepts/configuration/secret/\"></a>\nvolume mounts, or <a href=\"https://kubernetes.io/docs/concepts/storage/volumes/#emptydir\"></a> with )\nare implemented with a  filesystem.\nThe contents of such volumes should remain in memory at all times, hence should\nnot be swapped to disk.\nTo ensure the contents of such volumes remain in memory, the  tmpfs option\nis being used.</p><p>The Linux kernel officially supports the  option from version 6.3 (more info\ncan be found in <a href=\"https://kubernetes.io/docs/reference/node/kernel-version-requirements/#requirements-other\">Linux Kernel Version Requirements</a>).\nHowever, the different distributions often choose to backport this mount option to older\nLinux versions as well.</p><p>In order to verify whether the node supports the  option, the kubelet will do the following:</p><ul><li>If the kernel's version is above 6.3 then the  option will be assumed to be supported.</li><li>Otherwise, kubelet would try to mount a dummy tmpfs with the  option at startup.\nIf kubelet fails with an error indicating of an unknown option,  will be assumed\nto not be supported, hence will not be used.\nA kubelet log entry will be emitted to warn the user about memory-backed volumes might swap to disk.\nIf kubelet succeeds, the dummy tmpfs will be deleted and the  option will be used.\n<ul><li>If the  option is not supported, kubelet will emit a warning log entry,\nthen continue its execution.</li></ul></li></ul><p>It is deeply encouraged to encrypt the swap space.\nSee the <a href=\"https://kubernetes.io/blog/2025/03/25/swap-linux-improvements/#setting-up-encrypted-swap\">section above</a> with an example for setting unencrypted swap.\nHowever, handling encrypted swap is not within the scope of kubelet;\nrather, it is a general OS configuration concern and should be addressed at that level.\nIt is the administrator's responsibility to provision encrypted swap to mitigate this risk.</p><h2>Good practice for using swap in a Kubernetes cluster</h2><h3>Disable swap for system-critical daemons</h3><p>During the testing phase and based on user feedback, it was observed that the performance\nof system-critical daemons and services might degrade.\nThis implies that system daemons, including the kubelet, could operate slower than usual.\nIf this issue is encountered, it is advisable to configure the cgroup of the system slice\nto prevent swapping (i.e., set ).</p><h3>Protect system-critical daemons for I/O latency</h3><p>Swap can increase the I/O load on a node.\nWhen memory pressure causes the kernel to rapidly swap pages in and out,\nsystem-critical daemons and services that rely on I/O operations may\nexperience performance degradation.</p><p>To mitigate this, it is recommended for systemd users to prioritize the system slice in terms of I/O latency.\nFor non-systemd users,\nsetting up a dedicated cgroup for system daemons and processes and prioritizing I/O latency in the same way is advised.\nThis can be achieved by setting  for the system slice,\nthereby granting it higher I/O priority.\nSee <a href=\"https://www.kernel.org/doc/Documentation/admin-guide/cgroup-v2.rst\">cgroup's documentation</a> for more info.</p><h3>Swap and control plane nodes</h3><p>The Kubernetes project recommends running control plane nodes without any swap space configured.\nThe control plane primarily hosts Guaranteed QoS Pods, so swap can generally be disabled.\nThe main concern is that swapping critical services on the control plane could negatively impact performance.</p><h3>Use of a dedicated disk for swap</h3><p>It is recommended to use a separate, encrypted disk for the swap partition.\nIf swap resides on a partition or the root filesystem, workloads may interfere\nwith system processes that need to write to disk.\nWhen they share the same disk, processes can overwhelm swap,\ndisrupting the I/O of kubelet, container runtime, and systemd, which would impact other workloads.\nSince swap space is located on a disk, it is crucial to ensure the disk is fast enough for the intended use cases.\nAlternatively, one can configure I/O priorities between different mapped areas of a single backing device.</p><p>As you can see, the swap feature was dramatically improved lately,\npaving the way for a feature GA.\nHowever, this is just the beginning.\nIt's a foundational implementation marking the beginning of enhanced swap functionality.</p><p>In the near future, additional features are planned to further improve swap capabilities,\nincluding better eviction mechanisms, extended API support, increased customizability,\nbetter debug abilities and more!</p><p>You can review the current <a href=\"https://kubernetes.io/docs/concepts/architecture/nodes/#swap-memory\">documentation</a>\nfor using swap with Kubernetes.</p><p>Feel free to reach out to me, Itamar Holder ( on Slack and GitHub)\nif you'd like to help or ask further questions.</p>","contentLength":15526,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Ingress-nginx CVE-2025-1974: What You Need to Know","url":"https://kubernetes.io/blog/2025/03/24/ingress-nginx-cve-2025-1974/","date":1742846400,"author":"","guid":731,"unread":true,"content":"<p><a href=\"https://kubernetes.io/docs/concepts/services-networking/ingress/\">Ingress</a> is the traditional Kubernetes feature for exposing your workload Pods to the world so that they can be useful. In an implementation-agnostic way, Kubernetes users can define how their applications should be made available on the network. Then, an <a href=\"https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/\">ingress controller</a> uses that definition to set up local or cloud resources as required for the user’s particular situation and needs.</p><p>Many different ingress controllers are available, to suit users of different cloud providers or brands of load balancers. Ingress-nginx is a software-only ingress controller provided by the Kubernetes project. Because of its versatility and ease of use, ingress-nginx is quite popular: it is deployed in over 40% of Kubernetes clusters!</p><p>Ingress-nginx translates the requirements from Ingress objects into configuration for nginx, a powerful open source webserver daemon. Then, nginx uses that configuration to accept and route requests to the various applications running within a Kubernetes cluster. Proper handling of these nginx configuration parameters is crucial, because ingress-nginx needs to allow users significant flexibility while preventing them from accidentally or intentionally tricking nginx into doing things it shouldn’t.</p><h2>Vulnerabilities Patched Today</h2><p>Four of today’s ingress-nginx vulnerabilities are improvements to how ingress-nginx handles particular bits of nginx config. Without these fixes, a specially-crafted Ingress object can cause nginx to misbehave in various ways, including revealing the values of <a href=\"https://kubernetes.io/docs/concepts/configuration/secret/\">Secrets</a> that are accessible to ingress-nginx. By default, ingress-nginx has access to all Secrets cluster-wide, so this can often lead to complete cluster takeover by any user or entity that has permission to create an Ingress.</p><p>The most serious of today’s vulnerabilities, <a href=\"https://github.com/kubernetes/kubernetes/issues/131009\">CVE-2025-1974</a>, rated <a href=\"https://www.first.org/cvss/calculator/3-1#CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H\">9.8 CVSS</a>, allows anything on the Pod network to exploit configuration injection vulnerabilities via the Validating Admission Controller feature of ingress-nginx. This makes such vulnerabilities far more dangerous: ordinarily one would need to be able to create an Ingress object in the cluster, which is a fairly privileged action. When combined with today’s other vulnerabilities, <strong>CVE-2025-1974 means that anything on the Pod network has a good chance of taking over your Kubernetes cluster, with no credentials or administrative access required</strong>. In many common scenarios, the Pod network is accessible to all workloads in your cloud VPC, or even anyone connected to your corporate network! This is a very serious situation.</p><p>First, determine if your clusters are using ingress-nginx. In most cases, you can check this by running <code>kubectl get pods --all-namespaces --selector app.kubernetes.io/name=ingress-nginx</code> with cluster administrator permissions.</p><p><strong>If you are using ingress-nginx, make a plan to remediate these vulnerabilities immediately.</strong></p><p>If you can’t upgrade right away, you can significantly reduce your risk by turning off the Validating Admission Controller feature of ingress-nginx.</p><ul><li>If you have installed ingress-nginx using Helm\n<ul><li>Reinstall, setting the Helm value <code>controller.admissionWebhooks.enabled=false</code></li></ul></li><li>If you have installed ingress-nginx manually\n<ul><li>delete the ValidatingWebhookconfiguration called </li><li>edit the  Deployment or Daemonset, removing  from the controller container’s argument list</li></ul></li></ul><p>If you turn off the Validating Admission Controller feature as a mitigation for CVE-2025-1974, remember to turn it back on after you upgrade. This feature provides important quality of life improvements for your users, warning them about incorrect Ingress configurations before they can take effect.</p><h2>Conclusion, thanks, and further reading</h2><p>The ingress-nginx vulnerabilities announced today, including CVE-2025-1974, present a serious risk to many Kubernetes users and their data. If you use ingress-nginx, you should take action immediately to keep yourself safe.</p><p>Thanks go out to Nir Ohfeld, Sagi Tzadik, Ronen Shustin, and Hillai Ben-Sasson from Wiz for responsibly disclosing these vulnerabilities, and for working with the Kubernetes SRC members and ingress-nginx maintainers (Marco Ebert and James Strong) to ensure we fixed them effectively.</p>","contentLength":4163,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null}],"tags":["dev","k8s"]}