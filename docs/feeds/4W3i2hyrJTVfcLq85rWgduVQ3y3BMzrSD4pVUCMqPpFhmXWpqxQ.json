{"id":"4W3i2hyrJTVfcLq85rWgduVQ3y3BMzrSD4pVUCMqPpFhmXWpqxQ","title":"Hacker News: Front Page","displayTitle":"HN Front","url":"https://hnrss.org/frontpage?points=75","feedLink":"https://news.ycombinator.com/","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":15,"items":[{"title":"Prolog Adventure Game","url":"https://github.com/stefanrodrigues2/Prolog-Adventure-game","date":1745281512,"author":"shakna","guid":213,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43757916"},{"title":"Evertop: E-ink IBM XT clone with 100+ hours of battery life","url":"https://github.com/ericjenott/Evertop","date":1745273269,"author":"harryvederci","guid":212,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43757037"},{"title":"Cheating the Reaper in Go","url":"https://mcyoung.xyz/2025/04/21/go-arenas/","date":1745271992,"author":"ingve","guid":211,"unread":true,"content":"<p>Even though I am a C++ programmer at heart, Go fascinates me for none of the reasons you think. Go has made several interesting design decisions:</p><ol><li><p>It has virtually no Undefined Behavior.</p></li><li><p>It has very simple GC semantics that they’re mostly stuck with due to design decisions in the surface language.</p></li></ol><p>These things mean that despite Go having a GC, it’s possible to do manual memory management in pure Go and in cooperation with the GC (although without any help from the  package). To demonstrate this, we will be building an untyped, garbage-collected arena abstraction in Go which relies on several GC implementation details.</p><p>I would never play this kind of game in Rust or C++, because LLVM is extremely intelligent and able to find all kinds of ways to break you over the course of frequent compiler upgrades. On the other hand, although Go does not promise any compatibility across versions for code that imports , in practice, two forces work against Go doing this:</p><ol><li><p>Go prioritizes not breaking the ecosystem; this allows to assume that <a href=\"https://www.hyrumslaw.com/\">Hyrum’s Law</a> will protect certain observable behaviors of the runtime, from which we may infer what can or cannot break easily.</p></li></ol><p>This is in contrast to a high-performance native compiler like LLVM, which has a carefully defined boundary around all UB, allowing them to arbitrarily break programs that cross it (mostly) without fear of breaking the ecosystem.</p><p>So, let’s dive in and cheat death.</p><p>Our goal is to build an , which is a data structure for efficient allocation of memory that has the same lifetime. This reduces pressure on the general-purpose allocator by only requesting memory in large chunks and then freeing it all at once.</p><p>For a comparison in Go, consider the following program:</p><div><figure><pre><code data-lang=\"go\"></code></pre></figure></div><p>This program will print successive powers of 2: this is because  is implemented approximately like so:</p><div><figure><pre><code data-lang=\"go\"></code></pre></figure></div><p>For appending small pieces,  is only called  times, a big improvement over calling it for every call to . Virtually every programming language’s dynamic array abstraction makes this optimization.</p><p>An arena generalizes this concept, but instead of resizing exponentially, it allocates  blocks and vends pointers into them. The interface we want to conform to is as follows:</p><div><figure><pre><code data-lang=\"go\"></code></pre></figure></div><p>In go a size and and an alignment, out comes a pointer fresh memory with that layout. Go does not have user-visible uninitialized memory, so we additionally require that the returned region be zeroed. We also require that  be a power of two.</p><p>We can give this a type-safe interface by writing a generic  function:</p><div><figure><pre><code data-lang=\"go\"></code></pre></figure></div><p>This all feels very fine and dandy to anyone used to hurting themselves with  or  in C++, but there is a small problem. What happens when we allocate pointer-typed memory into this allocator?</p><div><figure><pre><code data-lang=\"go\"></code></pre></figure></div><p> takes a size and an alignment, which is sufficient to describe the  of any type. For example, on 64-bit systems,  and  have the same layout: 8 bytes of size, and 8 bytes of alignment.</p><p>However, the Go GC (and all garbage collectors, generally) require one additional piece of information, which is somewhere between the layout of a value (how it is placed in memory) and the type of a value (rich information on its structure). To understand this, we need a brief overview on what a GC does.</p><blockquote><p>For a complete overview on how to build a simple GC, take a look at a toy GC I designed some time ago: <a href=\"https://mcyoung.xyz/2022/06/07/alkyne-gc/\">The Alkyne GC</a>.</p></blockquote><p>A garbage collector’s responsibility is to maintain a memory allocator and an accounting of:</p><ol><li>What memory has been allocated.</li><li>Whether that memory is still in use.</li></ol><p>Memory that is not in use can be reclaimed and marked as unallocated, for re-use.</p><p>The most popular way to accomplish this is via a “mark and sweep” architecture. The GC will periodically walk the entire object graph of the program from certain pre-determined ; anything it finds is “marked” as alive. After a mark is complete, all other memory is “swept”, which means to mark it is unallocated for future re-use, or to return it to the OS, in the case of significant surplus.</p><p>The roots are typically entities that are actively being manipulated by the program. In the case of Go, this is anything currently on the stack of some G, or anything in a global (of which there is a compile-time-known set).</p><p>The marking phase begins with , which looks at the stack of each G and locates any pointers contained therein. The Go compiler generates metadata for each function that specifies which stack slots in a function’s frame contain pointers. All of these pointers are live by definition.</p><p>These pointers are placed into a queue, and each pointer is traced to its allocation on the heap. If the GC does not know anything about a particular address, it is discarded as foreign memory that does not need to be marked. If it does, each pointer in that allocation is pushed onto the queue if it has not already been marked as alive. The process continues until the queue is empty.</p><p>The critical step here is to take the address of some allocation, and convert it into all of the pointer values within. Go has precise garbage collection, which means that it only treats things declared as pointers in the surface language as pointers: an integer that happens to look like an address will not result in sweeping. This results in more efficient memory usage, but trades off some more complexity in the GC.</p><p>For example, the types , , ,  all contain at least one pointer, while , , <code>struct {X bool; F uintptr}</code> do not. The latter are called  types.</p><p>Go enhances the layout of a type into a  by adding a bitset that specifies which pointer-aligned, pointer-sized words of the type’s memory region contain a pointer. These are called the . For example, here are the shapes of a few Go types on a 64-bit system.</p><table><thead><tr></tr></thead><tbody><tr></tr></tbody></table><p>In the Go GC, each allocation is tagged with its shape (this is done in a variety of ways in the GC, either through an explicit header on the allocation, itself (a “malloc header”), a runtime type stored in the allocation’s , or another mechanism). When scanning a value, it uses this information to determine where the pointers to scan through are.</p><p>The most obvious problem with our  type is that it does not discriminate shapes, so it cannot allocate memory that contains pointers: the GC will not be able to find the pointers, and will free them prematurely!</p><p>In our example where we allocated an  in our custom allocator, we wind up with a  on the stack. You would think that Go would simply trace through the first  to find an  and mark it as being alive, but that is not what happens! Go instead finds a pointer into some chunk that the custom allocator grabbed from the heap, which is missing the pointer bits of its shape!</p><p>Why does go not look at the type of the pointer it steps through? Two reasons.</p><ol><li><p>All pointers in Go are untyped from the runtime’s perspective; every  gets erased into an . This allows much of the Go runtime to be “generic” without using actual generics.</p></li><li><p>Pointee metadata can be aggregated, so that each pointer to an object does not have to remember its type at runtime.</p></li></ol><p>The end result for us is that we can’t put pointers on the arena. This makes our  API unsafe, especially since Go does not provide a standard constraint for marking generic parameters as pointer-free: unsurprisingly, the don’t expect most users to care about such a detail.</p><p>It  possible to deduce the pointer bits of a type using reflection, but that’s very slow, and the whole point of using arenas is to go fast. As we design our arena, though, it will become clear that there is a safe way to have pointers on it.</p><p>Now that we have a pretty good understanding about what the Go GC is doing, we can go about designing a fast arena structure.</p><p>The ideal case is that a call to  is very fast: just offsetting a pointer in the common case. One assumption we can make off the bat is that all memory can be forced to have maximum alignment: most objects are a pointer or larger, and Go does have a maximum alignment for ordinary user types, so we can just ignore the  parameter and always align to say, 8 bytes. This means that the pointer to the next unallocated chunk will always be well-aligned. Thus, we might come up with a structure like this one:</p><div><figure><pre><code data-lang=\"go\"></code></pre></figure></div><p>How fast is this really? Here’s a simple benchmark for it.</p><div><figure><pre><code data-lang=\"go\"></code></pre></figure></div><p>The focus of this benchmark is to measure the cost of allocating many objects of the same size. The number of times the  loop will execute is unknown, and determined by the benchmarking framework to try to reduce statistical anomaly. This means that if we instead just benchmark a single allocation, the result will be  sensitive to the number of runs.</p><p>We also use  to get a throughput measurement on the benchmark. This is a bit easier to interpret than the gross , the benchmark would otherwise produce. It tells us how much memory each allocator can allocate per unit time.</p><p>We want to compare against , but just writing  will get optimized out, since the resulting pointer does not escape. Writing it to a global is sufficient to convince Go that it escapes.</p><p>Here’s the results, abbreviated to show only the bytes per second. All benchmarks were performed on my AMD Ryzen Threadripper 3960X. Larger is better.</p><div><figure><pre><code data-lang=\"console\"></code></pre></figure></div><p>This is quite nice, and certainly worth pursuing! The performance increase seems to scale up with the amount of memory allocated, for a 2x-4x improvement across different cases.</p><p>Now we need to contend with the fact that our implementation is completely broken if we want to have pointers in it.</p><p>In , when we assign a freshly-allocated chunk, we overwrite , which means the GC can reclaim it. But this is fine: as long as pointers into that arena chunk are alive, the GC will not free it, independent of the arena. So it seems like we don’t need to worry about it?</p><p>However, the whole point of an arena is to allocate lots of memory that has the same lifetime. This is common for graph data structures, such as an AST or a compiler IR, which performs a lot of work that allocates a lot and then throws the result away.</p><p>We are not allowed to put pointers in the arena, because they would disappear from the view of the GC and become freed too soon. But, if a pointer wants to go on an arena, it necessarily outlive the whole arena, since it outlives part of the arena, and the arena is meant to have the same lifetime.</p><p>In particular, if we could make it so that holding any pointer returned by  prevents the  from being swept by the GC, the arena can safely contain pointers into itself! Consider this:</p><ol><li><p>We have a pointer . It is allocated on some arena .</p></li><li><p>The GC sees our pointer (as a type-erased ) and marks its allocation as live.</p></li><li><p>Somehow, the GC also marks  as alive as a consequence.</p></li><li><p>Somehow, the GC then marks every chunk  has allocated as alive.</p></li><li><p>Therefore he chunk that  points to is also alive, so  does not need to be marked directly, and will not be freed early.</p></li></ol><p>The step (3) is crucial. By forcing the whole arena to be marked, any pointers stored in the arena into itself will be kept alive automatically, without the GC needing to know how to scan for them.</p><p>So, even though  is still going to result in a use-after-free, <code>*New[*int](a) = New[int](a)</code> would not! This small improvement does not make arenas themselves safe, but a data structure with an internal arena can be completely safe, so long as the only pointers that go into the arena are from the arena itself.</p><p>How can we make this work? The easy part is (4), which we can implement by adding a  to the arena, and sticking every pointer we allocate into it.</p><div><figure><pre><code data-lang=\"go\"></code></pre></figure></div><p>The cost of the  is amortized: to allocate  bytes, we wind up allocating an additional  times. But what does this do to our benchmarks?</p><div><figure><pre><code data-lang=\"console\"></code></pre></figure></div><p>Seems pretty much the same, which is a good sign.</p><p>Now that the arena does not discard any allocated memory, we can focus on condition (3): making it so that if any pointer returned by  is alive, then so is the whole arena.</p><p>Here we can make use of an important property of how Go’s GC works: any pointer into an allocation will keep it alive, as well as <em>anything reachable from that pointer</em>. But the chunks we’re allocating are s, which will not be scanned. If there could  be a single pointer in this slice that was scanned, we would be able to stick the pointer  there, and so when anything that  returns is scanned, it would cause  to be marked as alive.</p><p>So far, we have been allocating  using , but we would actually like to allocate <code>struct { A [N]uintptr; P unsafe.Pointer }</code>, where  is some dynamic value.</p><p>In its infintie wisdom, the Go standard library actually gives us a dedicated mechanism to do this: . This can be used to construct arbitrary anonymous  types at runtime, which we can then allocate on the heap.</p><p>So, instead of calling , we might call this function:</p><div><figure><pre><code data-lang=\"go\"></code></pre></figure></div><p>This appears to have a minor but noticeable effect on performance.</p><div><figure><pre><code data-lang=\"console\"></code></pre></figure></div><p>Looking back at , the end of this function has a branch:</p><div><figure><pre><code data-lang=\"go\"></code></pre></figure></div><p>This is the absolute hottest part of allocation, since it is executed every time we call this function. The branch is a bit unfortunate, but it’s necessary, as noted by the comment.</p><p>In C++, if we have an array of  with  elements in it, and  is a pointer to the start of the array,  is a valid pointer, even though it can’t be dereferenced; it points “one past the end” of the array. This is a useful construction, since, for example, you can use it to eliminate a loop induction variable:</p><div><figure><pre><code data-lang=\"c++\"></code></pre></figure></div><p>Go, however, gets very upset if you do this, because it confuses the garbage collector. The GC can’t tell the difference between a one-past-the-end pointer for allocation A, and for the start of allocation B immediately after it. At best this causes memory to stay alive for longer, and at worst it triggers safety interlocks in the GC. The GC will panic if it happens to scan a pointer for an address that it knows has been freed.</p><p>But in our code above, every chunk now has an extra element at the very end that is not used for allocation, so we  have a pointer that  one-past-the-end of the  that we are vending memory from.</p><p>The updated allocation function would look like this:</p><div><figure><pre><code data-lang=\"go\"></code></pre></figure></div><p>Notably, we do not replace  with an end pointer, because of the  comparison. We can’t actually avoid the subtraction  because we would have to do it to make this comparison work if we got rid of .</p><p>So how much better is this?</p><div><figure><pre><code data-lang=\"console\"></code></pre></figure></div><p>Remarkably, not very! This is an improvement on the order of magnitude of one or two percentage points. This is because the branch we deleted is extremely predictable.</p><p>Turns out there’s a bigger improvement we can make.</p><p>Here’s the assembly Go generated for this function, heavily abridged, and annotated with the corresponding Go source code.</p><div><figure><pre><code data-lang=\"llvm\"></code></pre></figure></div><p>There’s a lot going on in this function, but most of it is a mix of Go not being great at register allocation, and lots of .</p><p>A write barrier is a mechanism for synchronizing ordinary user code with the GC. Go generates code for one any time a non-pointer-free type is stored. For example, writing to a , , or  requires a write barrier.</p><p>Write barriers are implemented as follows:</p><ol><li><p> is checked, which determines whether the write barrier is necessary, which is only when the GC is in the mark phase. Otherwise the branch is taken to skip the write barrier.</p></li><li><p>A call to one of the  functions happens.  is the number of pointers that the GC needs to be informed of.</p></li><li><p>This function calls , which returns a buffer onto which pointers the GC needs to now trace through should be written to.</p></li><li><p>The actual store happens.</p></li></ol><p>A write barrier is required for a case like the following. Consider the following code.</p><div><figure><pre><code data-lang=\"go\"></code></pre></figure></div><p>This function will call  to allocate eight bytes of memory. The resulting pointer will be returned in . This function then stores  into  and returns. If we Godbolt this function, we’ll find that it does, in fact, generate a write barrier:</p><div><figure><pre><code data-lang=\"llvm\"></code></pre></figure></div><p>Note that two pointers get written: the pointer returned by , and the old value of . This ensures that regardless of where in this function the GC happens to be scanning through , it sees both values during the mark phase.</p><p>Now, this isn’t necessary if the relevant pointers are already reachable in some other way… which is exactly the case in our arena (thanks to the  slice). So the write barrier in the fast path is redundant.</p><p>But, how do we get rid of it? There is a , but that’s not allowed outside of a list of packages allowlisted in the compiler. It also doens’t disable write barriers; it simply generates a diagnostic if any are emitted.</p><p>But remember, write barriers only occur when storing pointer-typed memory… so we can just replace  with .</p><div><figure><pre><code data-lang=\"go\"></code></pre></figure></div><p> hates this, because it doesn’t know that we’re smarter than it is. Does This make the code faster? To make it a little bit more realistic, I’ve written a separate variant of the benchmarks that hammers the GC really hard in a separate G:</p><div><figure><pre><code data-lang=\"go\"></code></pre></figure></div><p>The result indicates that this is a worthwhile optimization for churn-heavy contexts. Performance is much worse overall, but that’s because the GC is pre-empting everyone. The improvement seems to be on the order of 20% for very small allocations.</p><div><figure><pre><code data-lang=\"console\">Before\nAfter\n</code></pre></figure></div><p>Go does not offer an easy mechanism to “reallocate” an allocation, as with  in C. This is because it has no mechanism for freeing pointers explicitly, which is necessary for a reallocation abstraction.</p><p>But we already don’t care about safety, so we can offer reallocation on our arena. Now, the reallocation we can offer is quite primitive: if a chunk happens to be the most recent one allocated, we can grow it. Otherwise we just allocate a new chunk and don’t free the old one.</p><p>This makes it possible to implement “arena slices” that can be constructed by appending, which will not trigger reallocation on slice growth as long as nothing else gets put on the arena.</p><p> would look something like this:</p><div><figure><pre><code data-lang=\"go\"></code></pre></figure></div><p>Then, whenever we append to our arena slice, we can call  to grow it. However, this does not work if the slice’s base pointer is not the original address returned by  or . It is an exercise for the reader to:</p><ol><li><p>Implement a  type that uses an arena for allocation.</p></li><li><p>Make this work for any value of  within the most recent allocation, not just the base offset. This requires extra book-keeping.</p></li></ol><p>Here is the entirety of the code that we have developed, not including the reallocation function above.</p><div><figure><pre><code data-lang=\"go\"></code></pre></figure></div><p>There are other optimizations that we could make here that I haven’t discussed. For example, arenas could be re-used; once an arena is done, it could be “reset” and placed into a . This arena would not need to go into the GC to request new chunks, re-using the ones previously allocated (and potentially saving on the cost of zeroing memory over and over again).</p><p>I did say that this relies very heavily on Go’s internal implementation details. Whats the odds that they get broken in the future? Well, the requirement that allocations know their shape is forced by the existence of , and the requirement that a pointer into any part of an allocation keeps the whole thing alive essentially comes from slices being both sliceable and mutable; once a slice escapes to the heap (and thus multiple goroutines) coordinating copies for shrinking a slice would require much more complexity than the current write barrier implementation.</p><p>And in my opinion, it’s pretty safe to say that Hyrum’s Law has us covered here. ;)</p>","contentLength":19102,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43756871"},{"title":"Visiting Us","url":"https://www.epic.com/visiting/","date":1745268560,"author":"tobr","guid":210,"unread":true,"content":"<div>Sat, Apr 26—Fri, May 09, 2025</div><div>Fri, May 16—Sun, May 18, 2025</div><div>Sat, Aug 09—Fri, Aug 22, 2025</div><div>Wed, Dec 24—Thu, Dec 25, 2025</div>","contentLength":124,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43756309"},{"title":"A M.2 HDMI capture card","url":"https://interfacinglinux.com/2025/04/18/magewell-eco-m-2-hdmi-capture-card/","date":1745262115,"author":"Venn1","guid":209,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43755286"},{"title":"Astronomers confirm the existence of a lone black hole","url":"https://phys.org/news/2025-04-astronomers-lone-black-hole.html","date":1745260583,"author":"wglb","guid":208,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43755017"},{"title":"Blog hosted on a Nintendo Wii","url":"https://blog.infected.systems/posts/2025-04-21-this-blog-is-hosted-on-a-nintendo-wii/","date":1745260144,"author":"edent","guid":207,"unread":true,"content":"<div><p>If you are reading this message, the experiment below is still ongoing. This page was served to you by a real Nintendo Wii.</p></div><p>For a long time, I’ve enjoyed the idea of running general-purpose operating systems on decidedly not-general-purpose hardware.</p><p>There’s been a few good examples of this over the years, including a few which were officially sanctioned by the OEM. Back in the day, my PS3 ran <a href=\"https://www.destructoid.com/yellow-dog-linux-on-ps3-actually-works/\">Yellow Dog Linux</a>, and I’ve been searching for a (decently priced) copy of <a href=\"https://en.wikipedia.org/wiki/Linux_for_PlayStation_2\">PS2 Linux</a> for 10+ years at this point.</p><p>But what a lot of these systems have in common is that they’re now very outdated. Or they’re hobbyist ports that someone got running once and where longer-term support never made it upstream. The PSP Linux kernel image was last built in 2008, and Dreamcast Linux is even more retro, using a 2.4.5 kernel built in 2001.</p><p>I haven’t seen many of these projects where I’d be comfortable running one as part of an actual production workload. Until now.</p><p>While browsing <a href=\"https://netbsd.org/\">the NetBSD website</a> recently, I noticed the fact that there was a ‘Wii’ option listed right there on the front page in the ‘Install Media’ section, nestled right next to the other first-class targets like the Raspberry Pi, and generic x86 machines.</p><p>Unlike the other outdated and unmaintained examples above, clicking through to the NetBSD Wii port takes you to the latest stable NetBSD 10.1 release from Dec 2024. Even the <a href=\"https://nycdn.netbsd.org/pub/NetBSD-daily/HEAD/latest/evbppc/binary/gzimg/\">daily HEAD builds</a> are composed for the Wii.</p><p>As soon as I discovered this was fully supported and maintained, I knew I had to try deploying an actual production workload on it. That workload is the blog you’re reading now.</p><h3>Finding a sacrificial Wii</h3><p>Our story begins at the EMF Camp 2024 Swap Shop - your premier source for pre-loved game consoles, cardboard boxes full of 56k modems, and radioactive orphan sources.</p><p>I picked this up expecting to use it for homebrew games and emulation mostly, but I don’t think it expected this fate.</p><p>So we have a spare Wii. And an OS with mainline support. But is a Wii actually fast enough to handle this as a production workload?</p><p>The single-core ‘Broadway’ CPU in the Wii is part of IBM’s continued evolution of the PowerPC 750 lineup, dating all the way back to Apple’s iconic 1998 Bondi Blue fishtank iMac. Although Broadway is one of the later 750 revisions, the commercially-available equivalent chip - the PowerPC 750CL - has a maximum TDP of only 9.8 W, and clocks about 33% higher than the version in the Wii.</p><p>So with a single-core chip based on a late-90s architecture and a TDP well under 10 W, it’s clear that we’re probably fairly contstrained here in terms of compute performance.</p><p>With that said, one of the other PowerPC 750 deployments you might be familiar with is currently floating 1,500,000 km from Earth mapping the deepest reaches of the universe in more detail than humanity has ever seen before. So if I can’t get this thing serving a static website, then I think it’s probably time to execute on my long-term plan of retiring from tech and opening a cat cafe.</p><p>On a more serious note, you can read about the James Wii Space Telescope’s use of the PowerPC 750 <a href=\"https://ntrs.nasa.gov/api/citations/20150019915/downloads/20150019915.pdf\">in this NASA presentation</a>. The 750 actually gets a lot of use in spaceflight and satellite applications because there is a radiation-hardened version available, known as the <a href=\"https://en.wikipedia.org/wiki/RAD750\">RAD750</a>. Some other recent uses of the chip include both the Mars <a href=\"https://en.wikipedia.org/wiki/Curiosity_(rover)\">Curiosity</a> and <a href=\"https://en.wikipedia.org/wiki/Perseverance_(rover)\">Perseverance</a> rovers.</p><h3>Installing NetBSD on the Wii</h3><p>Okay, Nintendo lawyers avert your eyes.</p><p>It had been a long time since I softmodded a Wii. I remember the <a href=\"https://www.youtube.com/watch?v=4BlpONgj74A\">Twilight Hack</a>, which involved exploiting a buffer overflow in the  save game handler to run unsigned code.</p><p>Things are much easier these days. The <a href=\"https://wiibrew.org/wiki/Wilbrand\">Wilbrand exploit</a> seems to be what people generally recommend now. Like some other exploits, it takes advantage of the fact that an SD card can be used to store and retrieve messages from the Wii Message Board. Exploting this allows unsigned code execution, which allows us to boot the <a href=\"https://www.gamebrew.org/wiki/HackMii_Installer_Wii\">HackMii</a> tool that installs the Homebrew Channel.</p><p>It’s an easy mod which just requires knowing the MAC address of the console and generating a few files to load from an SD card. There’s a handy browser-based tool <a href=\"https://wilbrand.donut.eu.org/\">here</a> which does all of the hard work for you.</p><p>I did have some issues using a larger SDHC card to run the Wilbrand exploit, but had the best luck with a 1GB non-SDHC card. SD card compatibility <a href=\"https://wiibrew.org/wiki/SD/SDHC_card_compatibility_tests\">seems to be a known issue</a> for Wii homebrew, but overall I’d still call the process fairly painless.</p><p>Once we’ve hacked the console, we should have the Homebrew Channel available in our Wii Menu:</p><p>For this card, I opted to use a fairly speedy 32GB SDHC card. The Wii doesn’t support SDXC or newer cards, which means we’re limited to 32GB. Larger flash devices also  tend to be faster and more resilient than smaller ones. And NetBSD seems a lot less bothered by living on a larger card than the Wilbrand exploit was. So overall I’d recommend getting a decent quality fast 32GB card if you want to try this.</p><p>We can unpack and write this image however we please, but I’m a fan of using the Raspberry Pi Imager because it’ll do the work of extracting the image and verifying it post-write for us:</p><p>At this point, things are very easy. The NetBSD Wii image has all the necessary metadata &amp; structure needed to boot directly from the Homebrew Channel as if it were any other kind of homebrew app. I think there’s a lot of credit due here to <a href=\"https://mastodon.sdf.org/@jmcwhatever\">NetBSD developer Jared McNeill</a>, who seems to be the main author of the Wii port.</p><p>Placing our SD card in the console and launching the Homebrew Channel is all we need to do to prepare ourselves to launch NetBSD:</p><p>Once booted into NetBSD, we can use a USB keyboard just fine, but it will be easiest to get SSH working so we can manage the system remotely. The SSH daemon is already running out-of-the-box, so the only changes we need to make are to set a password for the  user and then enable logging in as root by adding  to the .</p><p>You could set up an unprivileged user or do anything else you fancy here but I was keen to get SSH going ASAP, as due to  laziness I was doing this bit using a capture card and Photo Booth on macOS which doesn’t actually support disabling the image-flip on the video feed:</p><p>If you thought it was hard to exit Vim, try doing it back to front.</p><p>After installation, I set a static network config by editing  and restarted the host.</p><p>On that note, I’m using the official RVL-015 Wii LAN Adapter. I went to great lengths to track down one of these for a decent price for the best chance of compatibility. On reflection, this probably wasn’t needed as by the time we’re booted into NetBSD we should have all of NetBSD’s drivers available to us, so I expect most generic USB adapters would probably work (in theory!).</p><p>If anyone is wondering though, here are the specifics of the adapter and chipset, taken from :</p><div><pre tabindex=\"0\"><code data-lang=\"text\"></code></pre></div><p>After restarting, I installed NetBSD’s  package manager by setting some env vars and then using :</p><div><pre tabindex=\"0\"><code data-lang=\"sh\"></code></pre></div><p>After this, I was able to use pkgin to install a bunch of useful packages - most importantly including our  web server, which I’ve picked due to it being slim and well-suited for resource constrained environments:</p><div><pre tabindex=\"0\"><code data-lang=\"sh\"></code></pre></div><p>After that, I copied the basic  sample config, and enabled and started it:</p><div><pre tabindex=\"0\"><code data-lang=\"sh\"></code></pre></div><p>By default,  is set up to serve static content from . Since my blog is a collection of static pages built with Hugo, I was able to simply  these files over and within seconds I had the system serving my site over standard HTTP.</p><h3>Is it fast enough? (addendum)</h3><p>Alright, you got me. It turns out that while a PPC 750 might be enough to map the futhest reaches of the universe, a bit of soak testing suggests it does struggle a bit when trying to concurrently serve a lot of pages encrypted with modern TLS.</p><p>I tried freeing up resources by disabling a bunch of services I don’t need that are running out of the box on NetBSD:</p><div><pre tabindex=\"0\"><code data-lang=\"sh\"></code></pre></div><p>I also disabled , which was using a staggering 15% of the whole system’s RAM:</p><div><pre tabindex=\"0\"><code data-lang=\"text\"></code></pre></div><p>Unfortunately it seems  is definitely necessary to keep the system clock in-check. I’m not sure whether the Wii just drifts a lot, whether it’s specific to NetBSD on the Wii, or whether the clock battery in what may be a nearly 20 year old console has given up but I got some interesting and quite indignant sounding error messages after disabling this:</p><p>To compensate, I cheated by adding  to the main , so the system would run it hourly at  min past the hour. We still get timesync, but we don’t need to sacrifice a sixth of our RAM keeping it resident:</p><div><pre tabindex=\"0\"><code data-lang=\"text\"></code></pre></div><p>Even after freeing up the resources above, it seems like serving multiple encrypted requests in parallel was a struggle for the 750, so I opted to move the TLS termination for the blog to a Caddy instance sitting in front of the Wii.</p><p>I have Caddy acting as a reverse proxy to the Wii, handling encryption and cert management with ACME. <strong>Importantly, there are no caching options enabled in Caddy. Every request the site serves is being serviced directly by the Wii</strong> - including the large number of images on this post which I’ll almost certainly regret adding. I optimised as much as I could, but this page is still almost exactly 1 MB when all of the content is loaded.</p><p>Through this method, I’ve also been able to sinkhole LLM slop-scrapers at Caddy’s level, by dropping requests from <a href=\"https://github.com/ai-robots-txt/ai.robots.txt/blob/main/robots.txt\">known scraper User Agents</a> before they’re forwarded to the Wii. Hopefully that might help to keep our single core chugging along without too much distress.</p><p>Moving the SSL termination to Caddy also gives me the advantage of enabling Caddy’s Prometheus exporter, so I can load it into my InfluxDB + Grafana stack and monitor site load without putting a bunch of additional stress on the Wii.</p><p>But I’m still interested (and I’m sure you are too) in monitoring the general state of the Wii resources as this post goes live.</p><p>Considering I had to disable the NTP client for using too much RAM, I think running something like a Prometheus exporter directly on the Wii is right out of the window to begin with. So I put together a simple shell script that runs from the  every 15 min, outputting some system stats to a basic HTML file in the webroot.</p><div><pre tabindex=\"0\"><code data-lang=\"text\"></code></pre></div><p>Honestly, this worked way better and was far easier than I was expecting. Naturally, there are some downsides to the setup here though.</p><p>Rebooting NetBSD reboots the whole console, and not just the NetBSD ‘app’, so you’ll find yourself back at the Wii Menu after any kernel patch or system upgrade. Yes, this does mean that the Wiimote and sensor bar in your server cupboard are now a vital component of the production infrastructure.</p><p>I was reasonably pleased with the power consumption too. Some testing based on stats from my UPS monitoring suggest that when idling, the Wii is adding a fairly consistent ~18 W to my overall homelab usage.</p><p>By my calculations, that means I can expect the Wii to use ~13.2 kWh/month, which on my fairly expensive UK power tariff comes out to around £3.47/month - which does actually make this cheaper than most of the VPSes I can find around the obvious cloud providers. So when you’re looking for your next VPS… you know what to consider.</p><p>This was a fun experiment for a rainy day over a long weekend. I’ll probably keep it going for a while if it actually continues to work as well as it started. I’m often interested in applying artificial constraints to the things I deploy, as I find that’s when I learn best.</p><p>Who knows, maybe I’ll have been forced to become an expert in NetBSD TCP kernel tunables by this time next week…</p>","contentLength":11515,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43754953"},{"title":"Show HN: Dia, an open-weights TTS model for generating realistic dialogue","url":"https://github.com/nari-labs/dia","date":1745255227,"author":"toebee","guid":197,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43754124"},{"title":"A new form of verification on Bluesky","url":"https://bsky.social/about/blog/04-21-2025-verification","date":1745252211,"author":"ink_13","guid":206,"unread":true,"content":"<p>Trust is everything. Social media has connected us in powerful ways, but it hasn’t always given us the tools to know who we’re interacting with or why we should trust them.</p><p>In 2023, we launched our first layer of verification: letting individuals and organizations <a href=\"https://bsky.social/about/blog/4-28-2023-domain-handle-tutorial\">set their domain as their username</a>. Since then, over 270,000 accounts have linked their Bluesky username to their website. Domain handles continue to be an important part of verification on Bluesky. At the same time, we've heard from users that a larger visual signal would be useful in knowing which accounts are authentic.</p><p><strong>Now, we’re introducing a new layer — a user-friendly, easily recognizable blue check</strong>. Bluesky will proactively verify authentic and notable accounts and display a blue check next to their names. Additionally, <strong>through our Trusted Verifiers feature, select independent organizations can verify accounts directly</strong>. Bluesky will review these verifications as well to ensure authenticity.</p><p>Blue checks issued by platforms are just one form of trust. But trust doesn’t come only from the top down; it emerges from relationships, communities, and shared context.</p><p>So, we’re also enabling : organizations that can directly issue blue checks. Trusted verifiers are marked by scalloped blue checks, as shown below.</p><p>For example, the New York Times can now issue blue checks to its journalists directly in the app. Bluesky’s moderation team reviews each verification to ensure authenticity.</p><p>When you tap on a verified account's blue check, you’ll see which organizations have granted verification.</p><p>You can also choose to hide verification within the app — navigate to Settings &gt; Moderation &gt; Verification Settings to toggle it off.</p><h2>How to Get Verified on Bluesky</h2><p>During this initial phase, Bluesky is not accepting direct applications for verification. As this feature stabilizes, we’ll launch a request form for notable and authentic accounts interested in becoming verified or becoming trusted verifiers.</p>","contentLength":1992,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43753651"},{"title":"LLM-powered tools amplify developer capabilities rather than replacing them","url":"https://matthewsinclair.com/blog/0178-why-llm-powered-programming-is-more-mech-suit-than-artificial-human","date":1745246174,"author":"matthewsinclair","guid":205,"unread":true,"content":"<p>\nLast month, I used Claude Code to build two apps: an MVP for a non-trivial backend agent processing platform and the early workings of a reasonably complex frontend for a B2C SaaS product. Together, these projects generated approximately 30k lines of code (and about the same amount again thrown away over the course of the exercise). The experience taught me something important about AI and software development that contradicts much of the current narrative.</p><p>\nRemember <a href=\"https://www.youtube.com/watch?v=grTBAvkC3ew\" target=\"_blank\">Ripley’s final showdown with the Xenomorph Queen in </a>? She straps into the Power Loader—an industrial exoskeleton that amplifies her strength and capabilities. The suit doesn’t replace Ripley; it transforms her into something far more powerful than either human or machine alone.</p><p>\nThis is exactly how tools like Claude Code work in practice. They’re not replacement programmers; they’re amplifiers of what I can already do. That backend project? It would’ve taken me months the old way. With Claude Code, I knocked out the core in weeks. But let’s be clear - I wasn’t just describing what I wanted and watching magic happen.</p><p>\nThink of Ripley controlling that Power Loader. Claude Code gave me tremendous lifting power while I maintained control of where we were going. I made all the architectural calls, set the quality bar, and kept us on vision. Most importantly, I had to watch - diligently - for it going off the rails. And when it did (which happened regularly), I had to bring it back into line. The AI cranked out implementation details at incredible speed, but the brain behind the operation? Still mine, and constantly engaged.</p><p>\nWith great power comes great responsibility. You must maintain constant awareness when working with AI coding tools—something I learned through several painful lessons.</p><p>\nClaude Code occasionally made bewildering decisions: changing framework code to make tests pass, commenting out whole sections of code and replacing them with hardcoded values to achieve a passing test rather than fixing the underlying problem, or introducing dependencies that weren’t necessary or appropriate. It has a massive bias for action, so you have to ruthlessly tell it to do less than it wants to do to keep it under control. I even found myself swearing at it from time to time in a weird form of anthropomorphising that I am still not entirely sure I am comfortable with or properly understand.</p><p>\nMuch like piloting an A380, the system handles tremendous complexity but requires a human to grab the yoke at critical moments. Modern aircraft can practically fly themselves, but we still need skilled pilots in the cockpit making key decisions. Taking your eyes off the process, even briefly, can lead to trouble. In my case, the backend required three complete rewrites because I looked away at crucial junctures, allowing the AI to go down problematic implementation paths that weren’t apparent until much later.</p><p>\nThis vigilance requirement creates a fascinating dynamic. While the AI dramatically accelerates certain aspects of development, it demands a different kind of attention from the developer—less focus on writing each line of code, more focus on reviewing, guiding, and maintaining architectural integrity.</p><p>\nWorking with Claude Code has fundamentally shifted how I think about the economics of programming time. Traditionally, coding involves three distinct “time buckets”:</p><ul><li> Understanding the business problem and value  </li><li> Designing the solution conceptually  </li><li> Actually  the code  </li></ul><p>\nFor decades, that last bucket consumed enormous amounts of our time. We’d spend hours, days or weeks writing, debugging, and refining. With Claude, that time cost has plummeted to nearly zero. I can generate thousands of lines of functional code in a sitting—something that is, frankly, mind-blowing.</p><p>\nBut here’s the critical insight: the first two buckets haven’t gone anywhere. In fact, they’ve become  important than ever. Understanding the intent behind what you’re building and clearly defining what needs to be done are now the primary constraints on development speed.</p><p>\nAnd there’s a new skill that emerges: wielding the knife. With code generation being essentially free, we need to become much more comfortable with throwing away entire solutions. The sunk cost fallacy hits programmers hard—we hate discarding code we’ve invested in, fearing we might break something important or never get back to a working state.</p><p>\nBut when your assistant can rewrite everything in minutes, that calculus changes completely. Three times during my backend project, I looked at substantial amounts of code—thousands of lines that —and decided to scrap it entirely because the approach wasn’t right. This wasn’t easy. My instinct was still to try to salvage and refactor. But the right move was to step back, rethink the approach, and direct the AI down a different path.</p><p>\nThis willingness to cut ruthlessly is a muscle most developers haven’t developed yet. It requires confidence in your architectural judgment and a radical shift in how you value implementation time.</p><p>\nFor me, using Claude Code has been a learning exercise in itself. Progress often felt like two steps forward and three back, particularly in the early stages. Generating 20k+ lines of code became relatively straightforward on a daily basis, but knowing when to throw everything away and rebuild from scratch—that took 30 years of experience.</p><p>\nWisdom and a bit of grey hair gave me the confidence to recognise when a particular approach wasn’t going to scale or maintain properly, even when the code appeared functional on the surface. Rather than just sit there and watch it generate code, I had to pay very close attention to spot anti-patterns or worse emerging that would either stop working soon after it was written, or lie dormant and bite later on. </p><p>\nThis highlights a critical danger: developers without substantial real-world experience might not recognise when the AI produces nonsense output. They might not realise when AI-generated code solves the immediate problem but creates three more in the process. The mech suit amplifies capability, but it also amplifies mistakes when operated without expertise. These tools are incredibly powerful, but they are also incredibly dangerous when pointed in the wrong direction. </p><p>\nChess provides a useful parallel here. <a href=\"https://en.wikipedia.org/wiki/Advanced_chess\" target=\"_blank\">“Centaur chess”</a> pairs humans with AI chess engines, creating teams that outperform both solo humans and solo AI systems playing on their own. What’s fascinating is that even when AI chess engines can easily defeat grandmasters, the human-AI combination still produces superior results to the AI alone. The human provides strategic direction and creative problem-solving; the machine offers computational power and tactical precision.</p><p>\nMy experience with Claude demonstrated this effect clearly. When I treated the AI as a partner rather than a replacement, development velocity increased dramatically. What I found most effective was when I spent time writing out a stream-of-consciousness “spec” and then iterating with Claude to turn it into an more formal design document. </p><p>\nBut the partnership still required my domain knowledge and architectural judgment to succeed. The AI excelled at pattern recognition and code generation but lacked the contextual understanding to make appropriate trade-offs and design decisions. It can’t tell when it’s done that seems ok, but is actually bonkers. It needed me to watch it constantly and keep it on track.</p><p>\nBuilding these applications required finding the right balance between delegation and control. Claude went down some insane rabbit holes on the backend when left unsupervised—places where the AI would implement increasingly complex solutions to problems that should have been solved differently or perhaps didn’t need solving at all. In one nightmare example, it ended up completely duplicating a whole section of code in one place rather than reuse an existing component. It worked (for some version of the word “work”) but it was . Way wrong. </p><p>\nIt was a similar story on the front end. I had to constantly stop it from trying to implement functionality in hacky JavaScript rather than use idiomatic Elixir and Phoenix LiveView patterns. </p><p>\nOver time, I developed a rhythm for collaboration. For straightforward implementations following established patterns, I could delegate broadly. For novel challenges or areas with significant trade-offs, I needed to provide more detailed specifications and review output more carefully.</p><p>\nWhat I’ve built could not have been completed so quickly without Claude Code, but it also would have failed completely without human oversight. The true value emerged from understanding when to leverage the AI’s capabilities and when to assert human judgment.</p><h2>\nThe Future is Augmentation</h2><p>\nThere is a view in many circles that LLMs will replace programmers. I am hesitant to say that this will  happen, becuase a lot of things with LLMs have surprised me recently, and I expect more surprises to come. For now, however, I don’t see LLMs  replacing programmers; but they are transforming how we work. Like Ripley in her Power Loader, we’re learning to operate powerful new tools that extend our capabilities far beyond what we could achieve alone.</p><p>\nThis transformation will change what we value in developers. Raw coding ability becomes less important; architectural thinking, pattern recognition, and technical judgment become more crucial. The ability to effectively direct and collaborate with AI tools emerges as a vital skill in itself.</p><p>\nThe developers who thrive in this new environment won’t be those who fear or resist AI tools, but those who master them—who understand both their extraordinary potential and their very real limitations. They’ll recognise that the goal isn’t to remove humans from the equation but to enhance what humans can accomplish.</p><p>\nIn my view, that’s something to embrace, not fear. The mech suit awaits, and with it comes the potential to build software at scales and speeds previously unimaginable—but only for those skilled enough to operate the machines in ways that don’t harm themselves or those around them.</p><p><em>[ED: If you’d like to sign up for this content as an email, <a href=\"https://eepurl.com/duaiov\" target=\"_blank\">click here</a> to join the mailing list.]</em></p>","contentLength":10319,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43752492"},{"title":"AI assisted search-based research works now","url":"https://simonwillison.net/2025/Apr/21/ai-assisted-search/","date":1745244917,"author":"simonw","guid":204,"unread":true,"content":"<h2>AI assisted search-based research actually works now</h2><p>For the past two and a half years the feature I’ve most wanted from LLMs is the ability to take on search-based research tasks on my behalf. We saw the first glimpses of this back in early 2023, with Perplexity (first launched <a href=\"https://en.wikipedia.org/wiki/Perplexity_AI\">December 2022</a>, first prompt leak <a href=\"https://simonwillison.net/2023/Jan/22/perplexityai/\">in January 2023</a>) and then the GPT-4 powered Microsoft Bing (which launched/cratered spectacularly <a href=\"https://simonwillison.net/2023/Feb/15/bing/\">in February 2023</a>). Since then a whole bunch of people have taken a swing at this problem, most notably <a href=\"https://gemini.google.com/\">Google Gemini</a> and <a href=\"https://openai.com/index/introducing-chatgpt-search/\">ChatGPT Search</a>.</p><p>Those 2023-era versions were promising but very disappointing. They had a strong tendency to hallucinate details that weren’t present in the search results, to the point that you couldn’t trust anything they told you.</p><p>In this first half of 2025 I think these systems have finally crossed the line into being genuinely useful.</p><h4>Deep Research, from three different vendors</h4><p>First came the  implementations—<a href=\"https://blog.google/products/gemini/google-gemini-deep-research/\">Google Gemini</a> and <a href=\"https://openai.com/index/introducing-deep-research/\">then OpenAI</a> and <a href=\"https://www.perplexity.ai/hub/blog/introducing-perplexity-deep-research\">then Perplexity</a> launched products with that name and they were all impressive: they could take a query, then churn away for several minutes assembling a lengthy report with dozens (sometimes hundreds) of citations. Gemini’s version had a  upgrade a few weeks ago when they <a href=\"https://blog.google/products/gemini/deep-research-gemini-2-5-pro-experimental/\">switched it to using Gemini 2.5 Pro</a>, and I’ve had some outstanding results from it since then.</p><p>Waiting a few minutes for a 10+ page report isn’t my ideal workflow for this kind of tool. I’m impatient, I want answers faster than that!</p><h4>o3 and o4-mini are really good at search</h4><p>Last week, OpenAI released <a href=\"https://openai.com/index/introducing-o3-and-o4-mini/\">search-enabled o3 and o4-mini</a> through <a href=\"https://chatgpt.com/\">ChatGPT</a>. On the surface these look like the same idea as we’ve seen already: LLMs that have the option to call a search tool as part of replying to a prompt.</p><p>But there’s one  difference: these models can run searches as part of the chain-of-thought reasoning process they use before producing their final answer.</p><p>This turns out to be a  deal. I’ve been throwing all kinds of questions at ChatGPT (in o3 or o4-mini mode) and getting back genuinely useful answers grounded in search results. I haven’t spotted a hallucination yet, and unlike prior systems I rarely find myself shouting \"no, don’t search for !\" at the screen when I see what they’re doing.</p><p>Here are four recent example transcripts:</p><p>Talking to o3 feels like talking to a Deep Research tool in real-time, without having to wait for several minutes for it to produce an overly-verbose report.</p><p>My hunch is that doing this well requires a very strong reasoning model. Evaluating search results is hard, due to the need to wade through huge amounts of spam and deceptive information. The disappointing results from previous implementations usually came down to the Web being full of junk.</p><p>Maybe o3, o4-mini and Gemini 2.5 Pro are the first models to cross the gullibility-resistance threshold to the point that they can do this effectively?</p><h4>Google and Anthropic need to catch up</h4><p>The user-facing <a href=\"https://gemini.google.com/\">Google Gemini app</a> can search too, but it doesn’t show me what it’s searching for. As a result, I just don’t trust it. This is a big missed opportunity since Google presumably have by far the best search index, so they really should be able to build a great version of this. And Google’s AI assisted search on their regular search interface hallucinates  to the point that it’s actively damaging their brand. I just checked and Google is still showing slop <a href=\"https://simonwillison.net/2024/Dec/29/encanto-2/\">for Encanto 2</a>!</p><p>Claude also finally <a href=\"https://simonwillison.net/2025/Mar/20/\">added web search</a> a month ago but it doesn’t feel nearly as good. It’s <a href=\"https://simonwillison.net/2025/Mar/21/anthropic-use-brave/\">using the Brave search index</a> which I don’t think is as comprehensive as Bing or Gemini, and searches don’t happen as part of that powerful reasoning flow.</p><h4>Lazily porting code to a new library version via search</h4><p>I did  feel like doing the work to upgrade. On a whim, I pasted <a href=\"https://github.com/simonw/tools/blob/aa310a4f9cde07d5e8e87572f70fceca532884dd/gemini-mask.html\">my full HTML code</a> (with inline JavaScript) into ChatGPT o4-mini-high and prompted:</p><blockquote><p><code>This code needs to be upgraded to the new recommended JavaScript library from Google. Figure out what that is and then look up enough documentation to port this code to it.</code></p></blockquote><p>(I couldn’t even be bothered to look up the name of the new library myself!)</p><p>... it did <a href=\"https://chatgpt.com/share/68028f7b-11ac-8006-8150-00c4205a2507\">exactly that</a>. It churned away thinking for 21 seconds, ran a bunch of searches, figured out the new library (which existed  outside of its training cut-off date), found the <a href=\"https://ai.google.dev/gemini-api/docs/migrate\">upgrade instructions</a> and produced <a href=\"https://github.com/simonw/tools/commit/d199de213dc3f866a3b8efbcdd2dde34204dc409\">a new version</a> of my code that worked perfectly.</p><p>I ran this prompt on my phone out of idle curiosity while I was doing something else. I was  impressed and surprised when it did exactly what I needed.</p><h4>How does the economic model for the Web work now?</h4><p>I’m writing about this today because it’s been one of my “can LLMs do this reliably yet?” questions for over two years now. I think they’ve just crossed the line into being useful as research assistants, without feeling the need to check  they say with a fine-tooth comb.</p><p>I still don’t trust them not to make mistakes, but I think I might trust them enough that I’ll skip my own fact-checking for lower-stakes tasks.</p><p>This also means that a bunch of the potential dark futures we’ve been predicting for the last couple of years are a whole lot more likely to become true. Why visit websites if you can get your answers directly from the chatbot instead?</p><p>The lawsuits over this <a href=\"https://simonwillison.net/2023/Dec/31/ai-in-2023/#ethics-diabolically-complex\">started flying</a> back when the LLMs were still mostly rubbish. The stakes are a lot higher now that they’re actually good at it!</p><p>I can feel my usage of Google search taking a nosedive already. I expect a bumpy ride as a new economic model for the Web lurches into view.</p>","contentLength":5567,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43752262"},{"title":"Launch HN: Magic Patterns (YC W23) – AI Design and Prototyping for Product Teams","url":"https://news.ycombinator.com/item?id=43752176","date":1745244423,"author":"alexdanilowicz","guid":203,"unread":true,"content":"Alex and Teddy here. We’re launching Magic Patterns (<a href=\"https://www.magicpatterns.com\">https://www.magicpatterns.com</a>), an AI prototyping tool that helps PMs and designers create functional, interactive designs and websites. There’s a demo video at <a href=\"https://www.youtube.com/watch?v=SK8C_tQBwIU\" rel=\"nofollow\">https://www.youtube.com/watch?v=SK8C_tQBwIU</a>, as well as video walkthroughs of specific examples at <a href=\"https://www.magicpatterns.com/docs/documentation/tutorials/video-tutorials\">https://www.magicpatterns.com/docs/documentation/tutorials/v...</a><p>While other tools help with “AI-assisted coding,” we have been quietly focused on “AI-assisted designing.” With Magic Patterns you can visually communicate your idea, get hands on feedback from customers, and test new features.</p><p>Teddy and I are best friends and former frontend engineers turned founders. We arrived at Magic Patterns after several pivots—always in the design tooling space, but different products that all struggled to get usage. We started working on Magic Patterns after an internal hackathon. Teddy built a UI library catalog and I messed around with GPT 3.5. We thought it’d be fun to combine the two: an AI component generator. Describe whatever you want, and get back a React component!</p><p>That started to take off and we gained users, but it wasn’t developers using the tool. Instead, it was PMs, designers, and leadership who could finally communicate their ideas. They use it to test new ideas quickly, get feedback from customers, and improve communication with internal teams. Also, hobbyists (and programmers who aren’t designers) use us to create designs and UIs that they wouldn’t be able to otherwise.</p><p>We use Sonnet 3.5 and 3.7, and leverage a fine-tuned model for fast-applying edits. The most challenging part is determining the most relevant context to feed to the LLM. We attempt to solve this with our click to update feature and by letting users define a brand preset, or default prompt.</p><p>Unlike other tools in this space, we’re specifically focused on (1) product teams—we're realtime and collaborative; and (2) frontend only—we don't spin up a database or backend because we aren't solving \"idea to fullstack app.\"</p><p>A common workflow is a product manager building an interactive prototype and then passing it off to a designer for more polish or directly to engineers. Many teams are even skipping Figma entirely now, telling us that it feels like an unnecessary middleman. Teams are instead generating clickable prototypes, collaborating directly with stakeholders, and using that as the mockup.</p><p>With Magic Patterns, you can: - Collaborate with your team on our infinite canvas; - Match your existing designs by creating reusable components directly; - Brainstorm features and flows. (The latter is what we use it for internally.)</p><p>We started as a way to build small, custom components, but now people are one-shotting entire\nwebsites and hosting them with us, or building dashboards that they share internally or in customer demos. People have sold $10k/mo contracts with Magic Patterns designs!</p><p>Small business owners—everyone from fishermen to driving instructors to hotel managers—are using us to build their websites and then hosting them with us. Example sites built by Magic Patterns include <a href=\"https://getdealflow.ai/\" rel=\"nofollow\">https://getdealflow.ai/</a> and <a href=\"https://joinringo.com/\" rel=\"nofollow\">https://joinringo.com/</a>. It’s amazing how people who couldn’t have done that before are now able to, and super gratifying to us to be empowering people in this way.</p><p>Today no login is required, just click “Coming from Hackernews?” and you’ll get 5 messages free to try. Once you hit the limit, you’ll then be prompted to login. Plans start at $19/mo for another 100 messages a month (<a href=\"https://www.magicpatterns.com/pricing\">https://www.magicpatterns.com/pricing</a>).</p><p>We’re stoked to be sharing with HN today and are open to all feedback!</p>","contentLength":3664,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43752176"},{"title":"Pipelining might be my favorite programming language feature","url":"https://herecomesthemoon.net/2025/04/pipelining/","date":1745237776,"author":"Mond_","guid":202,"unread":true,"content":"<p><em> Don’t take it too seriously. Or do. idk, I can’t stop you.</em></p><p>Pipelining might be my favorite programming language feature.</p><p>\n              What is pipelining? Pipelining is the feature that allows you to omit a single argument from your\n              parameter list, by instead passing the previous value.\n            </p><p>When I say pipelining, I’m talking about the ability to write code like this:</p><div><pre tabindex=\"0\"><code data-lang=\"rust\"></code></pre></div><p>\n              As opposed to code like this. (This is not real Rust code. Quick challenge for the curious Rustacean, can\n              you explain why we cannot rewrite the above code like this, even if we import all of the symbols?)\n            </p><div><pre tabindex=\"0\"><code data-lang=\"rust\"></code></pre></div><p>\n              I honestly feel like this should be so obvious that it shouldn’t even be up for debate. The first code\n              example—with its nice ‘pipelining’ or ‘method chaining’ or whatever you want to call it—it\n              . It can be read line-by-line. It’s easy to annotate it with comments. It doesn’t\n              require introduction of new variables to become more readable since it’s already readable as is.\n            </p><p>\n              As opposed to, y’know,\n              <em>the first word in the line describing the final action our function performs</em>.\n            </p><p>\n              Let me make it very clear: This is an  about syntax. In practice,\n              <em>semantics beat syntax every day of the week</em>. In other words, don’t take it too seriously.\n            </p><p>\n              Second, this is not about imperative vs. functional programming. This article takes for granted that\n              you’re already on board with concepts such as ‘map’ and ‘filter’. It’s possible to overuse that style, but\n              I won’t talk about it here.\n            </p><h2>You already agree with me</h2><p>\n              Here is a feature that’s so bog-standard in modern programming languages that it barely feels like a\n              feature at all. Member access for structs or classes with our beloved friend the -operator.\n            </p><p>\n              This is a form of pipelining. It puts the data first, the operator in the middle, and concludes with the\n              action (restricting to a member field). That’s an instance of what I call pipelining.\n            </p><div><pre tabindex=\"0\"><code data-lang=\"go\"></code></pre></div><p>\n              You see what I am getting at, right? It’s the same principle. One of the reasons why\n              -style member access syntax (and -style method call syntax!) is popular\n              is since it’s easy to read and chains easily.\n            </p><p>\n              Let’s make the comparison slightly more fair, and pretend that we have to write .\n              Compare:\n            </p><div><pre tabindex=\"0\"><code data-lang=\"go\"></code></pre></div><p>\n              Which one of these is easier to read? The pipelined syntax, obviously. This example is easy to parse\n              either way, but imagine you’d like to blend out some information and purely focus on the final operation.\n            </p><div><pre tabindex=\"0\"><code data-lang=\"go\"></code></pre></div><p>\n              You see the problem, right? In the first example, we have ‘all of the previous stuff’ and then\n               to it. In the second example, the operation which we want to perform\n              () and the new operand () are spread out with ‘all of the previous stuff’\n              sitting between them.\n            </p><p>Looking back at our original example, the problem should be obvious:</p><div><pre tabindex=\"0\"><code data-lang=\"rust\"></code></pre></div><p>\n              I cannot deny the allegations: I just don’t think it makes sense to write code like that as long as a\n              clearly better option exists.\n            </p><p>\n              Why would I have to parse the whole line just to figure out where my input comes in, and why is the data\n              flow ‘from the inside to the outside’? It’s kind of silly, if you ask me.\n            </p><p>\n              Readability is nice, and I could add add a whole section complaining about the mess that’s Python’s\n              ‘functional’ features.\n            </p><p>\n              However, let’s take a step back and talk about ease of editing. Going back to the example above, imagine\n              you’d like to add another  (or any other function call) in the middle there. How easy is\n              this?\n            </p><div><pre tabindex=\"0\"><code data-lang=\"rust\"></code></pre></div><ol><li>\n                You’ll have to parse through the line, counting commas and parentheses to find the exact place to add\n                the closing parenthesis.\n              </li><li>\n                The  of this is going to be basically unreadable, everything is crammed onto one\n                line.\n              </li><li>This line is getting long and unreadable, and at that point you’ll want to refactor it anyway!</li></ol><div><pre tabindex=\"0\"><code data-lang=\"rust\"></code></pre></div><p>\n              This is adding a single line of code. No parentheses counting. It’s easy and obvious. It’s easy to write\n              and easy to review. Perhaps most importantly, it shows up  in the\n               layer of whatever editor or code exploration tool you’re using.\n            </p><p>\n              You might think that this issue is  about trying to cram everything onto a single line, but\n              frankly, trying to move away from that doesn’t help much. It will still mess up your git diffs and the\n              blame layer.\n            </p><p>\n              You can, of course, just assign the result of every  and  call to a\n              helper variable, and I will (begrudgingly) acknowledge that that works, and is\n               better than trying to do absurd levels of nesting.\n            </p><p>\n              When you press  in your IDE, it will show a neat little pop-up that tells you which methods\n              you can call or which fields you can access.\n            </p><p>\n              This is probably the single IDE feature with the biggest value add, and if not that, then at least the\n              single most frequently used one. Some people will tell you that static analysis for namespace or\n              module-level code discovery is useless in the age of AI autocompletion and vibe coding, but I very much\n              disagree.</p><blockquote><p>\n                “grug very like type systems make programming easier. for grug, type systems most value when grug hit\n                dot on keyboard and list of things grug can do pop up magic. this 90% of value of type system or more to\n                grug” — grug\n              </p></blockquote><p>\n              Words to live by. What he’s describing here is something that essentially  pipelining to\n              work at all. (And types or type annotation, but having those is the direction the industry is moving in\n              anyway.)\n            </p><p>\n              It doesn’t matter if it’s the trusty  operator, C++’s , or if it’s\n              something more bespoke such as Elm’s or Gleam’s  or Haskell’s . In the\n              end, it’s a pipeline operator—the same principle applies. If your\n              <a href=\"https://en.wikipedia.org/wiki/Language_Server_Protocol\" target=\"_blank\">LSP</a> knows the type of\n              what’s on the left, it  in principle be able to offer suggestions for what to do next.\n            </p><p>\n              If your favorite language’s LSP/IDE does a poor job at offering suggestions during pipelining, then it’s\n              probably one of the following reasons:\n            </p><ol><li>\n                You don’t know which type you’re even holding. This happens most often when the language is dynamically\n                typed, ’types’ are hard to deduce with static analysis, and you’re touching/writing code without type\n                annotations. (e.g. Python)\n              </li><li>\n                The ecosystem and LSP just didn’t have enough time put into them, or most active users don’t care\n                enough. (e.g. any sufficiently obscure language)\n              </li><li>\n                You are in a situation in which even looking up which methods are available is hard, often due to a\n                bespoke build process that confuses the editor. (e.g. basically any build or runtime generation of code,\n                or bespoke loading/selection of libraries).\n              </li></ol><p>\n              In either case, great editor/LSP support is more or less considered mandatory for modern programming\n              languages. And of course, this is where pipelining shines.\n            </p><p>\n              Ask any IDE, autocompleting <code>fizz.bu... -&gt; fizz.buzz()</code> is  than\n              autocompleting , for the obvious reason that you\n              <em>didn’t even write  in the second example yet</em>, so your editor has less\n              information to work with.\n            </p><p>\n              Pipelining is  at data processing, and allows you to transform code that’s commonly\n              written with ‘inside-out’ control flow into ’line-by-line’ transformations.\n            </p><p>\n              Where could this possibly be more clear than in SQL, the presumably single most significant language for\n              querying and aggregating complex large-scale datasets?\n            </p><p>\n              You’ll be pleased to hear that, yes, people are in fact working on bringing pipelining to SQL. (Whether\n              it’s actually going to happen in this specific form\n              <a href=\"https://sqlite.org/forum/forumpost/2d2720461b82f2fd\" target=\"_blank\">is a different question</a>,\n              let’s not get too carried away here.)\n            </p><p>\n              Unless you’re one of those people who spends so much time dealing with SQL that it’s become second nature,\n              and the thought that the control flow of nested queries is hard to follow for the average non-database\n              engineer is incomprehensible to you, I guess.\n            </p><p>I’ll put their example of how a standard nested query can be simplified here, for convenience:</p><div><pre tabindex=\"0\"><code data-lang=\"sql\"></code></pre></div><p>Versus the SQL Syntax she told you not to worry about:</p><div><pre tabindex=\"0\"><code data-lang=\"sql\"></code></pre></div><p>\n              Less nesting. More aligned with other languages and\n              <a href=\"https://learn.microsoft.com/en-us/dotnet/csharp/linq/\" target=\"_blank\">LINQ</a>. Can easily be\n              read line-by-line.\n            </p><p>\n              Here’s a more\n              <a href=\"https://www.linkedin.com/pulse/not-so-good-idea-pipe-syntax-sql-franck-pachot-dx6he\" target=\"_blank\">skeptical voice (warning, LinkedIn!)</a>. Franck Pachot raises the great point that the  statement at the top of a query is\n              (essentially) its function signature and specifies the return type. With pipe syntax, you lose some of\n              this readability.\n            </p><p>I agree, but that seems like a solvable problem to me.</p><p>\n              And—surprise, surprise—it fits pretty well into pipelining. Any situation where you need to construct a\n              complex, stateful object (e.g. a client or runtime), it’s a great way to feed complex, optional arguments\n              into an object.\n            </p><p>\n              Some people say they prefer optional/named arguments, but honestly, I don’t understand why: An optional\n              named  parameter is harder to track down in code (and harder to mark as deprecated!) than\n              all instances of a  builder function.\n            </p><p>\n              If you have no clue what I’m talking about, this here is the type of pattern I’m talking about. You have a\n              ‘builder’ object, call some methods on it to configure it, and finally  the object\n              you’re actually interested in.\n            </p><div><pre tabindex=\"0\"><code data-lang=\"rs\"></code></pre></div><h2>Making Haskell (slightly more) readable</h2><p>\n              It has these weird operators like , , , or\n               and when you ask Haskell programmers about what they mean, they say something like\n              “Oh, this is just a special case of the generalized\n              <a href=\"https://hackage.haskell.org/package/base-4.12.0.0/docs/Control-Monad.html#v:-62--61--62-\" target=\"_blank\">Kleisli Monad Operator</a> in the category of endo-pro-applicatives over a locally small poset.” and your eyes\n              have glazed over before they’ve even finished the sentence.\n            </p><p>(It also doesn’t help that Haskell allows you to define custom operators however you please, yes.)</p><p>\n              If you’re wondering “How could a language have so many bespoke operators?”, my understanding is that most\n              of them are just fancy ways of telling Haskell to compose some functions in a highly advanced way. Here’s\n              the second-most basic\n              example, the  operator.\n            </p><p>\n              Imagine you have functions , , and some value . In a\n              “““normal””” language you might write . In Haskell, this is written as\n              . This is since  will automatically ‘grab’ values to the right as its\n              arguments, so you don’t need the parentheses.\n            </p><p>\n              A consequence of this is that  is written as  in\n              Haskell. If you wrote , the compiler will interpret it as\n              , which would be wrong. This is what people mean when they say that Haskell’s\n              function call syntax is\n              <a href=\"https://en.wikipedia.org/wiki/Associative_property\" target=\"_blank\">left-associative</a>.\n            </p><p>\n              The  operator is <em>nothing but syntactic sugar</em> that allows you to write\n               instead of having to write . That’s it. People were\n              fed-up with having to put parens everywhere, I guess.\n            </p><p>If your eyes glazed over at this point, I can’t blame you.</p><p>\n              Talking about any of the fancier operators would be punching well above my weight-class, so I’ll just\n              stick to what I’ve been saying throughout this entire post already. Here’s a stilted Haskell toy example,\n              intentionally not written in\n              <a href=\"https://wiki.haskell.org/Pointfree\" target=\"_blank\">pointfree</a> style.\n            </p><div><pre tabindex=\"0\"><code data-lang=\"haskell\"></code></pre></div><p>\n              If you want to figure out the flow of data, this whole function body has to be read\n              .\n            </p><p>\n              To make things even funnier, you need to start with the  clause to figure out which\n              local “variables” are being defined. This happens (for whatever reason) at the end of the function instead\n              of at the start. (Calling  a variable is misleading, but that’s besides the\n              point.)\n            </p><p>\n              At this point you might wonder if Haskell has some sort of pipelining operator, and yes, it turns out that\n              one was\n              <a href=\"https://github.com/haskell/core-libraries-committee/issues/78#issuecomment-1183568372\" target=\"_blank\">added in 2014</a>! That’s pretty late considering that Haskell exists since 1990. This allows us to refactor the above\n              code as follows:\n            </p><div><pre tabindex=\"0\"><code data-lang=\"haskell\"></code></pre></div><p>Isn’t that way easier to read?</p><p> is code which you can show to an enterprise Java programmer, tell them that they’re looking\n              at\n              <a href=\"https://docs.oracle.com/javase/8/docs/api/java/util/stream/Stream.html\" target=\"_blank\">Java Streams</a>\n              with slightly weird syntax, and they’ll get the idea.\n            </p><p>\n              Of course, in reality nothing is as simple. The Haskell ecosystem seems to be split between users of\n              , users of , and users of the\n              <a href=\"https://hackage.haskell.org/package/flow-2.0.0.0/docs/Flow.html#g:1\" target=\"_blank\">Flow</a>-provided operators, which allow the same functionality, but allow you to write\n               instead of .</p><p>\n              I don’t know what to say about that, other than that—not entirely unlike C++—Haskell has its own share of\n              operator-related and cultural historical baggage, and a split ecosystem, and this makes the language\n              significantly less approachable than it has to be.\n            </p><h2>Rust’s pipelining is pretty neat</h2><p>\n              In the beginning I said that ‘Pipelining is the feature that allows you to omit a single argument from\n              your parameter list, by instead passing the previous value.’\n            </p><p>\n              I still think that this is true, but it doesn’t get across the whole picture. If you’ve paid attention in\n              the previous sections, you’ll have noticed that  and\n               share basically  in common outside of the order of\n              operations.\n            </p><p>\n              In the first case, we’re accessing a value that’s  to the object. In the second, we’re\n              ‘just’ passing an expression to a free-standing function.\n            </p><p>\n              Or in other words, pipelining is not the same as pipelining. Even from an IDE-perspective, they’re\n              different. In Java, your editor will look for methods associated with an object and walk up the\n              inheritance chain. In Haskell, your editor will put a so-called\n              <a href=\"https://downloads.haskell.org/~ghc/7.10.3-rc1/users_guide/typed-holes.html\" target=\"_blank\">’typed hole’</a>, and try to deduce which functions have a type that ‘fits’ into the hole using\n              <a href=\"https://herecomesthemoon.net/2025/01/type-inference-in-rust-and-cpp/\" target=\"_blank\">Hindley-Milner Type Inference</a>.\n            </p><p>\n              Personally, I like type inference (and\n              <a href=\"https://en.wikipedia.org/wiki/Type_class\" target=\"_blank\">type classes</a>), but I also like if\n              types have a namespace attached to them, with methods and associated functions. I am pragmatic like that.\n            </p><p>\n              What I like about Rust is that it gives me the best out of both worlds here: You get traits and type\n              inference without needing to wrap your head around a fully functional, immutable, lazy, monad-driven\n              programming paradigm, and you get methods and associated values without the absolute dumpster fire of\n              complex inheritance chains or AbstractBeanFactoryConstructors.\n            </p><p>\n              I’ve not seen any other language that even comes close to the convenience of Rust’s pipelines, and its\n              lack of higher-kinded types or inheritance did not stop it. Quite the opposite, if anything.\n            </p><p>\n              I like pipelining. That’s the one thing that definitely should be obvious if you’ve read all the way\n              through this article.\n            </p><p>I just think they’re neat, y’know?</p><p>I like reading my code top-to-bottom, left-to-right instead of from-the-inside-to-the-outside.</p><p>\n              I like when I don’t need to count arguments and parentheses to figure out which value is the first\n              argument of the second function, and which is the second argument of the first function.\n            </p><p>\n              I like when my editor can show me all fields of a struct, and all methods or functions associated with a\n              value, just when I press  on my keyboard. It’s great.\n            </p><p>\n              I like when  and the  layer of the code repository don’t look like\n              complete ass.\n            </p><p>\n              I like when adding a function call in the middle of a process doesn’t require me to parse the whole line\n              to add the closing parenthesis, and doesn’t require me to adjust the nesting of the whole block.\n            </p><p>\n              I like when my functions distinguish between ‘a main value which we are acting upon’ and ‘secondary\n              arguments’, as opposed to treating them all as the same.\n            </p><p>\n              I like when I don’t have to pollute my namespaces with a ton of helper variables or free-standing\n              functions that I had to pull in from somewhere.\n            </p><p>\n              If you’re writing pipelined code—and not trying overly hard to fit everything into a single, convoluted,\n              nested pipeline—then your functions will naturally split up into a few pipeline chunks.\n            </p><p>\n              Each chunk starts with a piece of ‘main data’ that travels on a conveyer belt, where every line performs\n              exactly one action to transform it. Finally, a single value comes out at the end and gets its own name, so\n              that it may be used later.\n            </p><p>\n              And that is—in my humble opinion—exactly how it should be. Neat, convenient, separated ‘chunks’, each of\n              which can easily be understood in its own right.\n            </p><p><em>Thanks to kreest for proofreading this article.</em></p>","contentLength":19143,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43751076"},{"title":"Dumb statistical models, always making people look bad","url":"https://statmodeling.stat.columbia.edu/2025/04/18/dumb-statistical-models-always-making-people-look-bad/","date":1745068346,"author":"hackandthink","guid":200,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43736172"},{"title":"Synology Lost the Plot with Hard Drive Locking Move","url":"https://www.servethehome.com/synology-lost-the-plot-with-hard-drive-locking-move/","date":1745045301,"author":"motiejus","guid":199,"unread":true,"content":"<p>I dislike writing these articles, but here we are. According to <a href=\"https://www.hardwareluxx.de/index.php/news/hardware/festplatten/65949-synology-weitet-den-zwang-zur-eignen-oder-zertifizierten-festplatte-auf-die-plus-modelle-aus.html\">HardwareLuxx</a>, Synology is on a rough course with generations-old sub-par NAS hardware and now appears to be locking its NAS units to its own branded hard drives in its upcoming 2025 Plus models. This is a shame since a few years ago, Synology had neat hardware.</p><h2>Synology Lost the Plot with Hard Drive Locking Move</h2><p>Translating a relevant bit for our readers.</p><p><em>“…since an estimated hard drive health report is essential for both private and professional use, this creates a compulsion to use Synology’s own or equivalent drives. Additionally, certain features such as volume-wide deduplication, lifespan analysis, and automatic firmware updates for third-party devices will be disabled. There are also restrictions on the creation of storage pools, as well as, of course, support in the event of failures, which are not further specified.” (: Translated from <a href=\"https://www.hardwareluxx.de/index.php/news/hardware/festplatten/65949-synology-weitet-den-zwang-zur-eignen-oder-zertifizierten-festplatte-auf-die-plus-modelle-aus.html\">HardwareLuxx</a>)</em></p><p>Changing this to require Synology branded drives in the “Plus” line is silly. There are only a handful of hard drive manufacturers, and Synology does not have anywhere near the scale to compete at hard drive manufacturing. Consolidation in the hard drive industry was driven by huge economies of scale benefits. Without scale, and a library of patents, it is a very hard market to enter. As a result, Synology must be simply re-branding drives. Labeling drives as a “Dell”, “NetApp”, “HPE”, or other big vendor drive has been going on for years (decades?) on both the hard drive and SSD sides of storage. Realistically, there is usually very little that gets changed in vendor firmware from the drive manufacturer firmware. When we see some changes, it is usually in conjunction with a branded storage controller that also has corresponding firmware changes. Firmware tweaks very rarely target the built-in PCH or SoC SATA controllers on Intel, AMD, and Arm platforms in the mainstream server and storage market.</p><p>Let us just call this what it is. It is a grab for extra margin dollars. The challenge is that it is bad for Synology’s customers. For example, the Synology Plus series only scales to 16TB currently with the HAT3310-16T. Synology’s enterprise series scales to 20TB. WD Red Pro drives are already pushing 26TB. Or in other words, in an 8-bay desktop NAS segment to use Synology’s branded drives with Synology’s full feature set has a maximum raw capacity of 128TB. For a QNAP or TrueNAS 8-bay, that is 208TB of raw capacity.</p><p>We just looked at the drives on Amazon, to see when they would be available and their pricing.</p><ul><li>WD Red Pro 16TB has an estimated delivery date of April 25 to April 29 (<a href=\"https://amzn.to/4cHl4Fp\">Amazon Affiliate link</a>) while being $10 less expensive</li><li>Toshiba N300 Pro 16TB has an estimated delivery date of May 4 to May 9 (<a href=\"https://amzn.to/3Evjdqy\">Amazon Affiliate link</a>) while being $11 less expensive</li><li>Toshiba N300 16TB (CMR) is showing overnight availability so April 19 (<a href=\"https://amzn.to/3RUc0mW\">Amazon Affiliate Link</a>) and is $35 less expensive</li><li>Seagate IronWolf Pro 16TB is showing same day availability so April 18 (<a href=\"https://amzn.to/3EsKlqh\">Amazon Affiliate link</a>) but is $10 more expensive</li></ul><p>When a drive fails, one of the key factors in data security is how fast an array can be rebuilt into a healthy status. Of course, Amazon is just one vendor, but they have the distribution to do same-day and early morning overnight parts to a large portion of the US. Even overnighting a drive that arrives by noon from another vendor would be slower to arrive than two of the four other options at Amazon.</p><p>For Synology customers, the challenge of being vendor-locked into using Synology branded drives is not just about alleged firmware improvements. It is also about being able to keep data secure while finding easy replacements. A NAS in 2025 needs to work with any NAS rated hard drive because frankly there are not that many vendors out there. If a particular model was found to have a very high year 2 failure rate, you would naturally want to start replacing drives with a different model. Vendor locking features to specific Synology drives prevents this, but it also allows Synology to use different vendors behind the label without telling its customers.</p><p>Additionally, there can also be concerns about drive availability in the long-term. If your NAS is vendor-locked to only use Synology drives, then as owner of that NAS you are fully dependent upon Synology’s survival as a company and that they would continue manufacturing drives in the capacity points that you want. Sure, they make 2TB and 4TB model Synology drives today, but what about four years from now when you need a replacement drive? What if Synology sells or merges with some other technology vendor, or goes out of business? This move creates concerns within the Synology ecosystem that did not exist before, and that is not good for customers.</p><p>Many will notice that Synology devices have been largely absent from STH even though Synology is a very popular NAS solution. That is not by chance. While I actually like the company’s software, Synology’s NAS hardware feels extremely dated to the point that it feels like most of the solutions are running generations old hardware. The combination of neglecting hardware refresh cycles and now vendor locking features to only using Synology drives will ultimately hurt users. I cannot imagine recommending a NAS solution where I could not get a replacement drive in under 24 hours, if at all, and that makes Synology extremely hard to recommend in 2025. If the situation changes, then I am happy to have our team review more Synology gear. In the meantime, there are plenty of other options out there.</p>","contentLength":5608,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43734706"}],"tags":["dev","hn"]}